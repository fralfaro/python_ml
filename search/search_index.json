{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bienvenidos a MAT281!","text":""},{"location":"#contenidos-del-curso","title":"Contenidos del Curso","text":"<p>Introducci\u00f3n</p><p>Conceptos b\u00e1sicos del Machine Learning</p> <p>Machine Learning</p><p>Regresi\u00f3n, Clasificaci\u00f3n, Clustering, etc.</p>"},{"location":"__init__/","title":"init","text":"In\u00a0[\u00a0]: Copied! <pre>__version__ = \"0.1.0\"\n</pre> __version__ = \"0.1.0\""},{"location":"introduction/011_concepts/","title":"Conceptos B\u00e1sicos ML","text":"In\u00a0[18]: Copied! <pre>import seaborn as sns\niris = sns.load_dataset('iris')\niris.head()\n</pre> import seaborn as sns iris = sns.load_dataset('iris') iris.head() Out[18]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa <p>Aqu\u00ed, cada fila de los datos se refiere a una \u00fanica flor observada y el n\u00famero de filas es el n\u00famero total de flores en el conjunto de datos. En general, nos referiremos a las filas de la matriz como muestras (<code>samples</code>), y el n\u00famero de filas como <code>n_samples</code>.</p> <p>De manera similar, cada columna de los datos se refiere a una pieza particular de informaci\u00f3n que describe cada muestra. En general, nos referiremos a las columnas de la matriz como caracter\u00edsticas (features), y el n\u00famero de columnas como <code>n_features</code>.</p> In\u00a0[2]: Copied! <pre># features matrix\nX = iris.drop('species',axis=1)\nX\n</pre> # features matrix X = iris.drop('species',axis=1) X Out[2]: sepal_length sepal_width petal_length petal_width 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 ... ... ... ... ... 145 6.7 3.0 5.2 2.3 146 6.3 2.5 5.0 1.9 147 6.5 3.0 5.2 2.0 148 6.2 3.4 5.4 2.3 149 5.9 3.0 5.1 1.8 <p>150 rows \u00d7 4 columns</p> In\u00a0[3]: Copied! <pre># target\ny = iris['species']\ny\n</pre> # target y = iris['species'] y Out[3]: <pre>0         setosa\n1         setosa\n2         setosa\n3         setosa\n4         setosa\n         ...    \n145    virginica\n146    virginica\n147    virginica\n148    virginica\n149    virginica\nName: species, Length: 150, dtype: object</pre> <p>La funci\u00f3n <code>train_test_split</code> de Scikit-learn es una herramienta \u00fatil para dividir un conjunto de datos en conjuntos de entrenamiento y prueba.</p> <p>La funci\u00f3n <code>train_test_split</code> toma como entrada los datos que se van a dividir, junto con los valores correspondientes de la variable de respuesta. La funci\u00f3n tambi\u00e9n permite ajustar la proporci\u00f3n de datos que se asignar\u00e1n al conjunto de entrenamiento y al conjunto de prueba. Por defecto, la proporci\u00f3n es del 75% para el conjunto de entrenamiento y el 25% para el conjunto de prueba.</p> <p>Para esto, podemos utilizar la funci\u00f3n <code>train_test_split</code>:</p> In\u00a0[26]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# separar informacion\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42)\n</pre> from sklearn.model_selection import train_test_split  # separar informacion X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42) In\u00a0[27]: Copied! <pre>print(f\"dimensiones de X_train: {X_train.shape}\")\nprint(f\"dimensiones de y_train: {y_train.shape}\")\nprint(\"\")\nprint(f\"dimensiones de X_test:   {X_test.shape}\")\nprint(f\"dimensiones de y_test:   {y_test.shape}\")\n</pre> print(f\"dimensiones de X_train: {X_train.shape}\") print(f\"dimensiones de y_train: {y_train.shape}\") print(\"\") print(f\"dimensiones de X_test:   {X_test.shape}\") print(f\"dimensiones de y_test:   {y_test.shape}\") <pre>dimensiones de X_train: (90, 4)\ndimensiones de y_train: (90,)\n\ndimensiones de X_test:   (60, 4)\ndimensiones de y_test:   (60,)\n</pre> <p>Veamos un ejemplo pr\u00e1ctico:</p> In\u00a0[6]: Copied! <pre># Importar las bibliotecas necesarias\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# semilla\nnp.random.seed(42)\n\n# Crear un conjunto de datos de ejemplo\nrng = np.random.RandomState(42)\n\nn = 50\nx = np.arange(n)+1\ny1 = 2*x  \ny2 = 2*x + 2*rng.randn(n)\n\ndf = pd.DataFrame({\n    'x':x,\n    'y': y1,\n    'y_hat':y2\n})\n\ndf.head()\n</pre> # Importar las bibliotecas necesarias import pandas as pd import numpy as np  import matplotlib.pyplot as plt import seaborn as sns  # semilla np.random.seed(42)  # Crear un conjunto de datos de ejemplo rng = np.random.RandomState(42)  n = 50 x = np.arange(n)+1 y1 = 2*x   y2 = 2*x + 2*rng.randn(n)  df = pd.DataFrame({     'x':x,     'y': y1,     'y_hat':y2 })  df.head() Out[6]: x y y_hat 0 1 2 2.993428 1 2 4 3.723471 2 3 6 7.295377 3 4 8 11.046060 4 5 10 9.531693 In\u00a0[7]: Copied! <pre># visualizar resultados\nplt.rcParams[\"figure.figsize\"] = (12,4)\nplt.scatter(df['x'],df['y_hat'], color='red')\nplt.plot(df['x'],df['y']);\n</pre> # visualizar resultados plt.rcParams[\"figure.figsize\"] = (12,4) plt.scatter(df['x'],df['y_hat'], color='red') plt.plot(df['x'],df['y']); <p>Nuestra l\u00ednea azul corresponde a los valores originales, mientras que los puntos rojos corresponde a los valores estimados.</p> <p>Veamos el error de estimaci\u00f3n entre el valor original ($y$) y el valor estimado ($\\hat{y}$):</p> In\u00a0[8]: Copied! <pre># error\ny_original = np.array(df['y'])\ny_estimado = np.array(df['y_hat'])\n\nerror_estimacion = y_original-y_estimado\nerror_estimacion\n</pre> # error y_original = np.array(df['y']) y_estimado = np.array(df['y_hat'])  error_estimacion = y_original-y_estimado error_estimacion Out[8]: <pre>array([-0.99342831,  0.2765286 , -1.29537708, -3.04605971,  0.46830675,\n        0.46827391, -3.15842563, -1.53486946,  0.93894877, -1.08512009,\n        0.92683539,  0.93145951, -0.48392454,  3.82656049,  3.44983567,\n        1.12457506,  2.02566224, -0.62849467,  1.81604815,  2.8246074 ,\n       -2.93129754,  0.4515526 , -0.13505641,  2.84949637,  1.08876545,\n       -0.22184518,  2.30198715, -0.75139604,  1.20127738,  0.5833875 ,\n        1.20341322, -3.70455637,  0.02699445,  2.11542186, -1.64508982,\n        2.4416873 , -0.41772719,  3.91934025,  2.6563721 , -0.39372247,\n       -1.47693316, -0.34273656,  0.23129656,  0.60220739,  2.95704398,\n        1.43968842,  0.92127754, -2.11424445, -0.68723658,  3.52608031])</pre> <p>Este resultado nos da el error de estimaci\u00f3n por cada muestra (o fila). Para encontrar un resultado que resuma el error de estimaci\u00f3n utilizaremos funciones de distancias o m\u00e9tricas de estimaci\u00f3n.</p> <ol> <li><p>M\u00e9tricas absolutas: Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>$$\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |$$</p> <ul> <li>Root Mean Squared Error (RMSE):</li> </ul> <p>$$\\textrm{RMSE}(y,\\hat{y}) =(\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2)^{1/2}$$</p> </li> <li><p>M\u00e9tricas Porcentuales: Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son:</p> <ul> <li>Mean absolute percentage error (MAPE):</li> </ul> <p>$$\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |$$</p> </li> </ol> <p>Veamos un ejemplo pr\u00e1ctico, las m\u00e9tricas de estimaci\u00f3n  las podemos obtener desde <code>sklearn.metrics</code>:</p> In\u00a0[9]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n\nmae = mean_absolute_error(y_original,y_estimado)\nrmse = mean_squared_error(y_original,y_estimado)\nmape = mean_absolute_percentage_error(y_original,y_estimado)\n\nprint(f\"mae:  {round(mae,4)}\")\nprint(f\"rmse: {round(rmse,4)}\")\nprint(f\"mape: {round(mape,4)}\")\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error  mae = mean_absolute_error(y_original,y_estimado) rmse = mean_squared_error(y_original,y_estimado) mape = mean_absolute_percentage_error(y_original,y_estimado)  print(f\"mae:  {round(mae,4)}\") print(f\"rmse: {round(rmse,4)}\") print(f\"mape: {round(mape,4)}\") <pre>mae:  1.5328\nrmse: 3.6206\nmape: 0.0586\n</pre> <p>\u00bf C\u00f3mo saber cu\u00e1l es la mejor estimaci\u00f3n entre dos estimaciones?</p> <p>Supongamos que tenemos dos estimaciones $\\hat{y}_{1}$ y $\\hat{y}_{2}$, aquel que tenga un error de estimaci\u00f3n m\u00e1s bajo en las m\u00e9tricas de estimaci\u00f3n corresponder\u00e1 al mejor modelo.</p> <p>Definamos una segunda estimaci\u00f3n ($\\hat{y}_{2}$) y analicemos las m\u00e9tricas de estimaci\u00f3n:</p> In\u00a0[10]: Copied! <pre># definir otra estimacion\ny_estimado_2 = y_estimado+rng.randn(n)\n\n# error estimacion segunda estimacion\nmae_2 = mean_absolute_error(y_original,y_estimado_2)\nrmse_2 = mean_squared_error(y_original,y_estimado_2)\nmape_2 = mean_absolute_percentage_error(y_original,y_estimado_2)\n\nprint(f\"mae_2:  {round(mae_2,4)}\")\nprint(f\"rmse_2: {round(rmse_2,4)}\")\nprint(f\"mape_2: {round(mape_2,4)}\")\n</pre> # definir otra estimacion y_estimado_2 = y_estimado+rng.randn(n)  # error estimacion segunda estimacion mae_2 = mean_absolute_error(y_original,y_estimado_2) rmse_2 = mean_squared_error(y_original,y_estimado_2) mape_2 = mean_absolute_percentage_error(y_original,y_estimado_2)  print(f\"mae_2:  {round(mae_2,4)}\") print(f\"rmse_2: {round(rmse_2,4)}\") print(f\"mape_2: {round(mape_2,4)}\") <pre>mae_2:  1.8174\nrmse_2: 4.7062\nmape_2: 0.0671\n</pre> <p>Si tomamos en cuanta todas las m\u00e9tricas entre la primera estimaci\u00f3n ($\\hat{y}_{1}$) y la segunda ($\\hat{y}_{2}$) observamos que:</p> <ul> <li>$MAE_{1}&lt;MAE_{2}$</li> <li>$RMSE_{1}&lt;RMSE_{2}$</li> <li>$MAPE_{1}&lt;MAPE_{2}$</li> </ul> <p>En este caso, todas las m\u00e9tricas de estimaci\u00f3n para el primer caso son menores las m\u00e9tricas de estimaci\u00f3n para el segundo caso, por lo tanto, la primera estimaci\u00f3n ($\\hat{y}_{1}$) es mejor que la segunda ($\\hat{y}_{2}$).</p> In\u00a0[11]: Copied! <pre>import numpy as np \n\n# semilla\nnp.random.seed(1)\n\n\ny = np.random.randint(2, size=150)\nyhat = np.random.randint(2, size=150)\n\ndf = pd.DataFrame({\n    'y':y,\n    'y_estimado':yhat\n})\n\ndf['coincidencia'] = df['y']==df['y_estimado']\n\ndf.head()\n</pre> import numpy as np   # semilla np.random.seed(1)   y = np.random.randint(2, size=150) yhat = np.random.randint(2, size=150)  df = pd.DataFrame({     'y':y,     'y_estimado':yhat })  df['coincidencia'] = df['y']==df['y_estimado']  df.head() Out[11]: y y_estimado coincidencia 0 1 1 True 1 1 1 True 2 0 0 True 3 0 1 False 4 1 0 False <p>En este caso, la columna coincidencia corresponde los casos donde $y_{i}$ e $\\hat{y}_{i}$ son iguales:</p> In\u00a0[12]: Copied! <pre># total de coincidencias\ndf['coincidencia'].value_counts()\n</pre> # total de coincidencias df['coincidencia'].value_counts() Out[12]: <pre>True     88\nFalse    62\nName: coincidencia, dtype: int64</pre> <p>En este caso, sabemos que acertamos en 88 oportunidades, sin embargo, esto no nos da m\u00e1s detalles en que clase estamos acertando de mejor manera.</p> <p>Matriz de confusi\u00f3n</p> <p>La matriz de confusi\u00f3n es una herramienta que permite la visualizaci\u00f3n del desempe\u00f1o de un algoritmo Para la clasificaci\u00f3n binaria (por ejemplo, 0 y 1), se tiene la siguiente matriz de confusi\u00f3n:</p> <p></p> <p>Ac\u00e1 se define:</p> <ul> <li>TP = Verdadero positivo: el modelo predijo la clase positiva correctamente, para ser una clase positiva.</li> <li>FP = Falso positivo: el modelo predijo la clase negativa incorrectamente, para ser una clase positiva.</li> <li>FN = Falso negativo: el modelo predijo incorrectamente que la clase positiva ser\u00eda la clase negativa.</li> <li>TN = Verdadero negativo: el modelo predijo la clase negativa correctamente, para ser la clase negativa.</li> </ul> <p>En este contexto, los valores TP Y TN muestran los valores correctos que tuve al momento de realizar la predicci\u00f3n, mientras que los valores de de FN Y FP denotan los valores que me equivoque de clase.</p> <p>Los conceptos de FN y FP se pueden interpretar con la siguiente imagen:</p> <p></p> <p>Calculemos la matriz de confusi\u00f3n utilizando el comando <code>confusion_matrix</code>:</p> In\u00a0[13]: Copied! <pre>from sklearn.metrics import confusion_matrix\n\ncm =  confusion_matrix(df['y'],df['y_estimado'])\n\nprint('\\nMatriz de confusion:\\n ')\nprint(cm)\n</pre> from sklearn.metrics import confusion_matrix  cm =  confusion_matrix(df['y'],df['y_estimado'])  print('\\nMatriz de confusion:\\n ') print(cm) <pre>\nMatriz de confusion:\n \n[[42 26]\n [36 46]]\n</pre> <p>M\u00e9tricas de estimaci\u00f3n</p> <p>En este contexto, se busca maximizar el n\u00famero al m\u00e1ximo la suma de los elementos TP Y TN, mientras que se busca disminuir la suma de los elementos de FN y FP. Para esto se definen las siguientes m\u00e9tricas:</p> <ol> <li>Accuracy</li> </ol> <p>$$accuracy(y,\\hat{y}) = \\dfrac{TP+TN}{TP+TN+FP+FN}$$</p> <ol> <li>Recall:</li> </ol> <p>$$recall(y,\\hat{y}) = \\dfrac{TP}{TP+FN}$$</p> <ol> <li>Precision:</li> </ol> <p>$$precision(y,\\hat{y}) = \\dfrac{TP}{TP+FP} $$</p> <ol> <li>F-score:</li> </ol> <p>$$fscore(y,\\hat{y}) = 2\\times \\dfrac{precision(y,\\hat{y})\\times recall(y,\\hat{y})}{precision(y,\\hat{y})+recall(y,\\hat{y})} $$</p> <p>Veamos un ejemplo pr\u00e1ctico, las m\u00e9tricas de estimaci\u00f3n  las podemos obtener desde <code>sklearn.metrics</code>:</p> In\u00a0[14]: Copied! <pre>from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\naccuracy = accuracy_score(df['y'],df['y_estimado'])\nrecall = recall_score(df['y'],df['y_estimado'])\nprecision = precision_score(df['y'],df['y_estimado'])\nfscore = f1_score(df['y'],df['y_estimado'])\n\nprint(f\"accuracy:  {round(accuracy,4)}\")\nprint(f\"recall:    {round(recall,4)}\")\nprint(f\"precision: {round(precision,4)}\")\nprint(f\"fscore:    {round(fscore,4)}\")\n</pre> from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score  accuracy = accuracy_score(df['y'],df['y_estimado']) recall = recall_score(df['y'],df['y_estimado']) precision = precision_score(df['y'],df['y_estimado']) fscore = f1_score(df['y'],df['y_estimado'])  print(f\"accuracy:  {round(accuracy,4)}\") print(f\"recall:    {round(recall,4)}\") print(f\"precision: {round(precision,4)}\") print(f\"fscore:    {round(fscore,4)}\") <pre>accuracy:  0.5867\nrecall:    0.561\nprecision: 0.6389\nfscore:    0.5974\n</pre> <p>En este caso, todas l\u00e1s m\u00e9tricas estan cercanos a un valor de 0.6 (bien lejos del valor 1 esperado/so\u00f1ado), esto nos dice que la estimaci\u00f3n para este caso no es la mejor.</p> <p>Curva ROC y AUC</p> <p></p> <p>La curva ROC (Receiver Operating Characteristic) se utiliza para evaluar el rendimiento de los algoritmos de clasificaci\u00f3n binaria, es decir, entre dos clases o categor\u00edas (1 o 0, Verdadero o Falso, etc.). La curva ROC proporciona una representaci\u00f3n gr\u00e1fica, en lugar de un valor \u00fanico como la mayor\u00eda de las otras m\u00e9tricas.</p> <p>Antes de todo, aclaremos que en la clasificaci\u00f3n binaria, hay cuatro resultados posibles para una predicci\u00f3n: Verdadero positivo (TP), Falso positivo (FP), Verdadero negativo (TN) y Falso negativo (FN).</p> <p></p> <p>La curva ROC se genera calculando y trazando la tasa de verdaderos positivos (TPR) contra la tasa de falsos positivos (FPR) para un solo clasificador en una variedad de umbrales (en el siguiente p\u00e1rrafo se definen matem\u00e1ticamente ambas tasas). Por ejemplo, en un modelo de regresi\u00f3n log\u00edstica, el umbral ser\u00eda sobre la probabilidad predicha de una observaci\u00f3n perteneciente a la clase positiva. Normalmente, en la regresi\u00f3n log\u00edstica, si se predice que una observaci\u00f3n ser\u00e1 positiva con una probabilidad mayor que 0.5, se etiqueta como positiva. Sin embargo, realmente podr\u00edamos elegir cualquier umbral entre 0 y 1 (0.1, 0.3, 0.6, 0.99, etc.), y, de este modo, la curva ROC nos ayudar\u00eda a visualizar c\u00f3mo estas elecciones afectan al rendimiento del clasificador.</p> <p>La tasa de verdaderos positivos (TPR), o Sensibilidad, se define como:</p> <p>$$TPR= \\dfrac{TP}{TP+FN}$$</p> <p>donde TP es el n\u00famero de Verdaderos positivos y FN es el n\u00famero de Falsos negativos. La tasa de verdaderos positivos es una medida de la probabilidad de que una instancia positiva real se clasifique como positiva.</p> <p>La tasa de falsos positivos (FPR), o 1 \u2013 Especificidad, se define como:</p> <p>$$FPR= \\dfrac{FP}{FP+TN}$$</p> <p>donde FP es el n\u00famero de Falsos positivos y TN es el n\u00famero de Verdaderos negativos. La tasa de falsos positivos es esencialmente una medida de la frecuencia con la que se producir\u00e1 una \u00abfalsa alarma\u00bb, o la frecuencia con la que una instancia negativa real se clasificar\u00e1 como positiva.</p> <p>En la siguiente figura se muestra como ser\u00eda la curva ROC para tres hipot\u00e9ticos clasificadores distintos.</p> <p></p> <p>La l\u00ednea de puntos gris representa un clasificador aleatorio (equivalente a \u00abadivinar al azar\u00bb); su curva ROC consiste en una l\u00ednea recta diagonal de pendiente 1. Opuestamente, la l\u00ednea violeta representa un clasificador perfecto, es decir, que no comete ning\u00fan error (uno con una tasa de verdaderos positivos del 100% y una tasa de falsos positivos del 0%).</p> <p>Casi todos los ejemplos del mundo real se ubicar\u00e1n en alg\u00fan lugar entre estas dos l\u00edneas; no es perfecto, pero proporciona m\u00e1s poder de predicci\u00f3n que la suposici\u00f3n aleatoria. Por lo general, lo que buscamos es un clasificador que mantenga una alta tasa de verdaderos positivos y al mismo tiempo tenga una baja tasa de falsos positivos; este clasificador ideal tender\u00eda a pasar muy pr\u00f3ximo a la esquina superior izquierda de la figura anterior, casi rozando a la l\u00ednea p\u00farpura.</p> <p>El AUC</p> <p>Si bien es \u00fatil visualizar la curva ROC de un clasificador binario, en muchos casos podemos resumir esta informaci\u00f3n en una sola m\u00e9trica: el AUC. El AUC significa \u00e1rea bajo la curva (ROC) en ingl\u00e9s. Generalmente, cuanto mayor es la puntuaci\u00f3n AUC, mejor es el rendimiento de un clasificador binario para una tarea de clasificaci\u00f3n dada.</p> <p>La siguiente figura muestra que para un clasificador sin poder predictivo (es decir, de predicci\u00f3n aleatoria), el AUC es 0.5, y para un clasificador perfecto, el AUC es 1.0; esto es el \u00e1rea bajo sus correspondientes curvas ROC.</p> <p></p> <p>La mayor\u00eda de los clasificadores estar\u00e1n entre 0.5 y 1.0, con la rara excepci\u00f3n de que el clasificador funcione peor que la predicci\u00f3n aleatoria (AUC&lt;0.5); en este caso se estar\u00edan efectuando predicciones invertidas, es decir, se tender\u00eda a predecir la clase negativa cuando esta fuera realmente positiva y viceversa.</p> <p>\u00bfPor qu\u00e9 usar las curvas ROC?</p> <ul> <li><p>Una ventaja que presentan las curvas ROC es que nos ayudan a encontrar un umbral de clasificaci\u00f3n que se adapte a nuestro problema espec\u00edfico.</p> <ul> <li><p>Ejemplo: si estuvi\u00e9ramos evaluando un clasificador de correo no deseado, querr\u00edamos que la tasa de falsos positivos fuera realmente baja. No quer\u00edamos que nadie perdiera un correo electr\u00f3nico importante en el filtro de spam solo porque nuestro algoritmo es demasiado agresivo. Probablemente, incluso permitir\u00edamos una buena cantidad de correos electr\u00f3nicos no deseados reales (verdaderos positivos) a trav\u00e9s del filtro solo para asegurarnos de que no se perdieran correos electr\u00f3nicos importantes.</p> </li> <li><p>Ejemplo: si nuestro clasificador estuviera prediciendo si alguien tiene una enfermedad terminal, podr\u00edamos estar de acuerdo con un mayor n\u00famero de falsos positivos (diagn\u00f3sticos incorrectos de la enfermedad), solo para asegurarnos de que no perdamos ning\u00fan positivo verdadero (personas que realmente tienen la enfermedad).</p> </li> </ul> </li> </ul> <ul> <li>Adem\u00e1s, las curvas ROC y las puntuaciones AUC tambi\u00e9n nos permiten comparar el rendimiento de diferentes clasificadores para el mismo problema y as\u00ed poder elegir el de mayor rendimiento.</li> </ul> <p>Continuando con el ejemplo anterior:</p> In\u00a0[15]: Copied! <pre>from sklearn.metrics import roc_curve, auc\n</pre> from sklearn.metrics import roc_curve, auc In\u00a0[16]: Copied! <pre># Calcular la curva ROC y el AUC\nfpr, tpr, thresholds = roc_curve(df['y'],df['y_estimado'])\nroc_auc = auc(fpr, tpr)\n</pre> # Calcular la curva ROC y el AUC fpr, tpr, thresholds = roc_curve(df['y'],df['y_estimado']) roc_auc = auc(fpr, tpr) In\u00a0[17]: Copied! <pre># Graficar la curva ROC\nplt.figure(figsize=(5,5))\nplt.plot(fpr, tpr, label='Curva ROC (AUC = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')  # L\u00ednea de referencia para un clasificador aleatorio\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.00])\nplt.xlabel('Tasa de Falsos Positivos')\nplt.ylabel('Tasa de Verdaderos Positivos')\nplt.title('Curva ROC')\nplt.legend(loc=\"lower right\")\nplt.show()\n</pre> # Graficar la curva ROC plt.figure(figsize=(5,5)) plt.plot(fpr, tpr, label='Curva ROC (AUC = %0.2f)' % roc_auc) plt.plot([0, 1], [0, 1], 'k--')  # L\u00ednea de referencia para un clasificador aleatorio plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.00]) plt.xlabel('Tasa de Falsos Positivos') plt.ylabel('Tasa de Verdaderos Positivos') plt.title('Curva ROC') plt.legend(loc=\"lower right\") plt.show()"},{"location":"introduction/011_concepts/#conceptos-basicos-ml","title":"Conceptos B\u00e1sicos ML\u00b6","text":""},{"location":"introduction/011_concepts/#representacion-de-datos","title":"Representaci\u00f3n de Datos\u00b6","text":"<p>Una tabla es una cuadr\u00edcula bidimensional de datos, en la que las filas representan elementos individuales del conjunto de datos y las columnas representan cantidades relacionadas con cada uno de estos elementos.</p> <p>Por ejemplo, consideremos el conjunto de datos Iris, analizado por Ronald Fisher en 1936.</p>"},{"location":"introduction/011_concepts/#matriz-de-caracteristicas-y-arreglo-de-etiquetas","title":"Matriz de caracter\u00edsticas y Arreglo de Etiquetas\u00b6","text":""},{"location":"introduction/011_concepts/#matriz-de-caracteristicas","title":"Matriz de caracter\u00edsticas\u00b6","text":"<p>La matriz de caracter\u00edsticas (features matrix en ingl\u00e9s) es una representaci\u00f3n de los datos de entrada en un problema de aprendizaje autom\u00e1tico. Es una matriz bidimensional (con forma <code>[n_samples, n_features]</code>), en la que las filas representan las muestras o instancias de los datos, y las columnas representan las caracter\u00edsticas o atributos que describen cada muestra.</p> <p>Las caracter\u00edsticas (es decir, columnas) siempre se refieren a las observaciones distintas que describen cada muestra de manera cuantitativa. Las caracter\u00edsticas generalmente tienen valores reales, pero en algunos casos pueden ser valores booleanos o discretos.</p> <p>Por convenci\u00f3n, esta matriz de caracter\u00edsticas a menudo se almacena en una variable llamada <code>X</code>.</p>"},{"location":"introduction/011_concepts/#arreglo-de-etiquetas","title":"Arreglo de Etiquetas\u00b6","text":"<p>Adem\u00e1s de la matriz de caracter\u00edsticas <code>X</code>, generalmente tambi\u00e9n trabajamos con un arreglo de etiquetas (target array), que por convenci\u00f3n generalmente llamamos <code>y</code>. El target array suele ser unidimensional, con una longitud de <code>n_samples</code>, y generalmente se encuentra en un arreglo NumPy o una Serie de Pandas.</p> <p>El target array puede tener valores num\u00e9ricos continuos o clases/etiquetas discretas. Aunque algunos estimadores de Scikit-Learn manejan varios valores de target en forma de una matriz de objetivos bidimensional <code>[n_samples, n_targets]</code>, principalmente trabajaremos con el caso com\u00fan de un target array unidimensional.</p>"},{"location":"introduction/011_concepts/#conjunto-de-entrenamiento-y-prueba","title":"Conjunto de Entrenamiento y Prueba\u00b6","text":"<p>El conjunto de entrenamiento (train set) y el conjunto de prueba (test set) son dos conjuntos de datos distintos que se utilizan en el aprendizaje autom\u00e1tico para entrenar y evaluar un modelo. </p>"},{"location":"introduction/011_concepts/#definicion","title":"Definici\u00f3n\u00b6","text":"<ul> <li><p>El conjunto de entrenamiento es un subconjunto de los datos de entrada que se utiliza para ajustar los par\u00e1metros del modelo y encontrar la mejor funci\u00f3n que pueda predecir la variable de salida a partir de las variables de entrada. En otras palabras, se utiliza para entrenar el modelo.</p> </li> <li><p>El conjunto de prueba es otro subconjunto de los datos de entrada que se utiliza para evaluar el rendimiento del modelo. Este conjunto de datos no se utiliza en el entrenamiento del modelo, sino que se utiliza \u00fanicamente para evaluar el rendimiento del modelo en datos nuevos y no vistos durante el entrenamiento.</p> </li> </ul> <p></p>"},{"location":"introduction/011_concepts/#motivacion","title":"Motivaci\u00f3n\u00b6","text":"<p>La raz\u00f3n de utilizar dos conjuntos de datos separados para entrenar y evaluar el modelo es evitar el sobreajuste (overfitting) del modelo. El sobreajuste ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento y no generaliza bien a nuevos datos. Si se utilizan los mismos datos para entrenar y evaluar el modelo, es probable que el modelo tenga un buen rendimiento en los datos de entrenamiento pero un rendimiento pobre en datos nuevos.</p> <p></p>"},{"location":"introduction/011_concepts/#reglas-de-separacion","title":"Reglas de separaci\u00f3n\u00b6","text":"<p>En general, se divide el conjunto de datos de entrada en dos conjuntos: el conjunto de entrenamiento y el conjunto de prueba. El conjunto de entrenamiento suele ser el 70-80% del conjunto de datos y el conjunto de prueba el 20-30%. La proporci\u00f3n puede variar seg\u00fan el tama\u00f1o del conjunto de datos y la complejidad del modelo.</p> <p>A modo de regla emp\u00edrica, se considerar\u00e1 el tama\u00f1o \u00f3ptimo basado en la siguiente tabla:</p> n\u00famero de filas train set test set entre 100-1000 67% 33% entre 1.000- 100.000 80% 20% mayor a 100.000 99% 1%"},{"location":"introduction/011_concepts/#mas-informacion","title":"M\u00e1s informaci\u00f3n\u00b6","text":"<p>Es importante destacar que, en algunos casos, se utiliza un tercer conjunto de datos, llamado conjunto de validaci\u00f3n (validation set), que se utiliza para ajustar los hiperpar\u00e1metros del modelo y evitar el sobreajuste. En este caso,</p> <ul> <li>el conjunto de entrenamiento se utiliza para ajustar los par\u00e1metros del modelo,</li> <li>el conjunto de validaci\u00f3n se utiliza para ajustar los hiperpar\u00e1metros</li> <li>y el conjunto de prueba se utiliza para evaluar el rendimiento final del modelo.</li> </ul> <p></p>"},{"location":"introduction/011_concepts/#error-de-estimacion","title":"Error de estimaci\u00f3n\u00b6","text":"<p>El error de estimaci\u00f3n se refiere a la diferencia entre el valor real de una variable y el valor estimado por un modelo. En el contexto de los modelos de aprendizaje autom\u00e1tico, el error de estimaci\u00f3n se refiere a la diferencia entre la variable objetivo (tambi\u00e9n conocida como variable dependiente o variable de respuesta) y la variable predicha por el modelo.</p> <p>El objetivo de los modelos de aprendizaje autom\u00e1tico es minimizar el error de estimaci\u00f3n y, por lo tanto, mejorar la precisi\u00f3n del modelo en la predicci\u00f3n de valores desconocidos. El error de estimaci\u00f3n se puede calcular utilizando diferentes m\u00e9tricas seg\u00fan el tipo de modelo y el objetivo del an\u00e1lisis.</p> <p>Para medir el error de estimaci\u00f3n en machine learning es necesario identificar el tipo de dato del target: num\u00e9ricos o categ\u00f3ricos.</p> <p></p>"},{"location":"introduction/011_concepts/#error-de-estimacion-datos-numericos","title":"Error de estimaci\u00f3n: Datos Num\u00e9ricos\u00b6","text":"<p>En este caso, el error de estimaci\u00f3n corresponde a la diferencia entre el valor original y el valor predicho,es decir:</p> <p>$$e_{i}=y_{i}-\\hat{y}_{i} $$</p> <p></p>"},{"location":"introduction/011_concepts/#error-de-estimacion-datos-categoricos","title":"Error de estimaci\u00f3n: Datos Categ\u00f3ricos\u00b6","text":"<p>Para entender el error de estimaci\u00f3n es necesario entender algunos conceptos previamente, como lo es la matriz de confusi\u00f3n. Primero daremos un ejemplo intuitivo, los valores de $y$ corresponde a valores categ\u00f3ricos (ejemplo: alto y bajo). A menudo, estos valores se transforman n\u00fameros enteros  (ejemplo: alto = 1 y bajo=0). Trataremos de contar las veces que la estimaci\u00f3n ($\\hat{y}$) coincide con el valor original ($y$).</p> <p></p> <p>Veamos un ejemplo pr\u00e1tico en donde analizamos un problema de clasificaci\u00f3n binaria, es decir, con solo dos categor\u00edas:</p>"},{"location":"introduction/012_sklearn/","title":"Scikit-Learn","text":"In\u00a0[1]: Copied! <pre># Importar las bibliotecas necesarias\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n</pre> # Importar las bibliotecas necesarias import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LinearRegression In\u00a0[2]: Copied! <pre># Generar datos de ejemplo\nrng = np.random.RandomState(42)\n\nn_size = 500\nx = 10 * rng.rand(n_size)\ny = 2 * x - 1 + rng.randn(n_size)\ndf = pd.DataFrame({'x': x,'y': y })\n\n# graficar\nsns.scatterplot(data=df, x=\"x\", y=\"y\")\nplt.show()\n</pre> # Generar datos de ejemplo rng = np.random.RandomState(42)  n_size = 500 x = 10 * rng.rand(n_size) y = 2 * x - 1 + rng.randn(n_size) df = pd.DataFrame({'x': x,'y': y })  # graficar sns.scatterplot(data=df, x=\"x\", y=\"y\") plt.show() In\u00a0[3]: Copied! <pre># definir X e y\nX = df[['x']] # como dataframe\ny = df['y'] # como series\n</pre> # definir X e y X = df[['x']] # como dataframe y = df['y'] # como series In\u00a0[4]: Copied! <pre># Dividir los datos en conjuntos de entrenamiento y prueba\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X , y, test_size=0.2, random_state=42)\n\n# Mostrar el tama\u00f1o de cada elemento\nprint(\"Tama\u00f1o de x_train:\", x_train.shape)\nprint(\"Tama\u00f1o de y_train:\", y_train.shape)\nprint(\"Tama\u00f1o de x_test:\", x_test.shape)\nprint(\"Tama\u00f1o de y_test:\", y_test.shape)\n</pre> # Dividir los datos en conjuntos de entrenamiento y prueba  from sklearn.model_selection import train_test_split x_train, x_test, y_train, y_test = train_test_split(X , y, test_size=0.2, random_state=42)  # Mostrar el tama\u00f1o de cada elemento print(\"Tama\u00f1o de x_train:\", x_train.shape) print(\"Tama\u00f1o de y_train:\", y_train.shape) print(\"Tama\u00f1o de x_test:\", x_test.shape) print(\"Tama\u00f1o de y_test:\", y_test.shape) <pre>Tama\u00f1o de x_train: (400, 1)\nTama\u00f1o de y_train: (400,)\nTama\u00f1o de x_test: (100, 1)\nTama\u00f1o de y_test: (100,)\n</pre> In\u00a0[5]: Copied! <pre>from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n</pre> from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor In\u00a0[6]: Copied! <pre># Entrenar el modelo con los datos de entrenamiento\nmodel = LinearRegression(fit_intercept=True)\n#model = RandomForestRegressor()\nmodel\n</pre> # Entrenar el modelo con los datos de entrenamiento model = LinearRegression(fit_intercept=True) #model = RandomForestRegressor() model Out[6]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> <p>Tenga en cuenta que cuando se crea una instancia del modelo, la \u00fanica acci\u00f3n es almacenar estos valores de hiperpar\u00e1metro. En particular, a\u00fan no hemos aplicado el modelo a ning\u00fan dato: la API de Scikit-Learn deja muy clara la distinci\u00f3n entre la elecci\u00f3n del modelo y la aplicaci\u00f3n del modelo a los datos.</p> In\u00a0[7]: Copied! <pre># aplicar modelo a los datos\nmodel.fit(x_train, y_train)\n</pre> # aplicar modelo a los datos model.fit(x_train, y_train) Out[7]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> <p>El <code>fit()</code> hace que se lleven a cabo una serie de c\u00e1lculos internos dependientes del modelo, y los resultados de estos c\u00e1lculos se almacenan en atributos espec\u00edficos del modelo que el usuario puede explorar. En Scikit-Learn, por convenci\u00f3n, todos los par\u00e1metros del modelo que se aprendieron durante el proceso <code>fit()</code> tienen guiones bajos al final; por ejemplo en este modelo lineal, tenemos lo siguiente:</p> In\u00a0[8]: Copied! <pre># coeficientes\nmodel.coef_\n</pre> # coeficientes model.coef_ Out[8]: <pre>array([2.02327729])</pre> In\u00a0[9]: Copied! <pre># intercepto\nmodel.intercept_\n</pre> # intercepto model.intercept_ Out[9]: <pre>-1.1227537053702488</pre> <p>Estos dos par\u00e1metros representan la pendiente y la intersecci\u00f3n del ajuste lineal simple a los datos. En comparaci\u00f3n con la definici\u00f3n de datos, vemos que est\u00e1n muy cerca de la pendiente de entrada de 2 y la intersecci\u00f3n de -1.</p> In\u00a0[10]: Copied! <pre># nuevos valores con etiqueta (y) desconocido\nxfit = x_test.copy()\n</pre> # nuevos valores con etiqueta (y) desconocido xfit = x_test.copy() In\u00a0[11]: Copied! <pre># predecir a partir del modelo el valor de y\nyfit = model.predict(xfit)\n</pre> # predecir a partir del modelo el valor de y yfit = model.predict(xfit) <p>Finalmente, visualicemos los resultados trazando primero los datos sin procesar y luego ajuste este modelo:</p> In\u00a0[12]: Copied! <pre># visualizar resultados\nplt.scatter(x_test, y_test)\nplt.plot(xfit, yfit);\n</pre> # visualizar resultados plt.scatter(x_test, y_test) plt.plot(xfit, yfit); <p>Nos gustar\u00eda evaluar el modelo con datos que no ha visto antes, por lo que dividiremos los datos en un conjunto de entrenamiento (train set) y un conjunto de prueba (test set). Esto podr\u00eda hacerse a mano, pero es m\u00e1s conveniente usar la funci\u00f3n de utilidad <code>train_test_split</code>:</p> In\u00a0[13]: Copied! <pre>import seaborn as sns\niris = sns.load_dataset('iris')\niris.head()\n</pre> import seaborn as sns iris = sns.load_dataset('iris') iris.head() Out[13]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa In\u00a0[14]: Copied! <pre># definir X e y\nX_iris = iris.drop('species',axis=1) # como dataframe\ny_iris= iris['species'] # como series\n</pre> # definir X e y X_iris = iris.drop('species',axis=1) # como dataframe y_iris= iris['species'] # como series In\u00a0[15]: Copied! <pre>from sklearn.model_selection import train_test_split\n\n# separar informacion\nx_train, x_test, y_train, y_test = train_test_split(X_iris , y_iris, test_size=0.2, random_state=42)\n\n# Mostrar el tama\u00f1o de cada elemento\nprint(\"Tama\u00f1o de x_train:\", x_train.shape)\nprint(\"Tama\u00f1o de y_train:\", y_train.shape)\nprint(\"Tama\u00f1o de x_test:\", x_test.shape)\nprint(\"Tama\u00f1o de y_test:\", y_test.shape)\n</pre> from sklearn.model_selection import train_test_split  # separar informacion x_train, x_test, y_train, y_test = train_test_split(X_iris , y_iris, test_size=0.2, random_state=42)  # Mostrar el tama\u00f1o de cada elemento print(\"Tama\u00f1o de x_train:\", x_train.shape) print(\"Tama\u00f1o de y_train:\", y_train.shape) print(\"Tama\u00f1o de x_test:\", x_test.shape) print(\"Tama\u00f1o de y_test:\", y_test.shape) <pre>Tama\u00f1o de x_train: (120, 4)\nTama\u00f1o de y_train: (120,)\nTama\u00f1o de x_test: (30, 4)\nTama\u00f1o de y_test: (30,)\n</pre> <p>Ahora aplicamos el modelo de regresi\u00f3n log\u00edstica:</p> In\u00a0[16]: Copied! <pre># 1. elige la clase de modelo\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n</pre> # 1. elige la clase de modelo from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier In\u00a0[17]: Copied! <pre># 2. instanciar el modelo\nmodel = LogisticRegression(solver='liblinear')\n#model = RandomForestClassifier()\n</pre> # 2. instanciar el modelo model = LogisticRegression(solver='liblinear') #model = RandomForestClassifier() In\u00a0[18]: Copied! <pre># 3. ajuste el modelo a los datos\nmodel.fit(x_train,y_train)\n</pre> # 3. ajuste el modelo a los datos model.fit(x_train,y_train) Out[18]: <pre>LogisticRegression(solver='liblinear')</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression(solver='liblinear')</pre> In\u00a0[19]: Copied! <pre># 4. predecir con nuevos datos\ny_model = model.predict(x_test)\n</pre> # 4. predecir con nuevos datos y_model = model.predict(x_test) <p>Finalmente, podemos usar <code>accuracy_score</code> para ver la fracci\u00f3n de etiquetas pronosticadas que coinciden con su valor real:</p> In\u00a0[20]: Copied! <pre>from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\naccuracy_score(y_test, y_model)\n</pre> from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score accuracy_score(y_test, y_model) Out[20]: <pre>1.0</pre> <p>Con una precisi\u00f3n igual al 100%, vemos que incluso este algoritmo de clasificaci\u00f3n muy ingenuo es efectivo para este conjunto de datos en particular.</p> <p>Nota: m\u00e1s adelante entraremos en detalles respecto a las m\u00e9tricas de un modelo supervisado (regresi\u00f3n y clasificaci\u00f3n)</p> In\u00a0[21]: Copied! <pre># 1. Elija la clase de modelo\nfrom sklearn.decomposition import PCA\n</pre> # 1. Elija la clase de modelo from sklearn.decomposition import PCA   In\u00a0[22]: Copied! <pre># 2. Instancia el modelo con hiperpar\u00e1metros\nmodel = PCA(n_components=2)\n</pre> # 2. Instancia el modelo con hiperpar\u00e1metros model = PCA(n_components=2)     In\u00a0[23]: Copied! <pre># 3. Ajuste a los datos. \u00a1Observe que y no est\u00e1 especificado!\nmodel.fit(X_iris)\n</pre> # 3. Ajuste a los datos. \u00a1Observe que y no est\u00e1 especificado! model.fit(X_iris)     Out[23]: <pre>PCA(n_components=2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PCA<pre>PCA(n_components=2)</pre> In\u00a0[24]: Copied! <pre># 4. Transforma los datos a dos dimensiones\nX_2D = model.transform(X_iris)\n</pre> # 4. Transforma los datos a dos dimensiones X_2D = model.transform(X_iris)      <p>Ahora vamos a graficar nuestros resultados:</p> In\u00a0[25]: Copied! <pre>iris['PCA1'] = X_2D[:, 0]\niris['PCA2'] = X_2D[:, 1]\n\nsns.scatterplot(x=\"PCA1\", y =\"PCA2\", hue='species', data=iris);\n</pre> iris['PCA1'] = X_2D[:, 0] iris['PCA2'] = X_2D[:, 1]  sns.scatterplot(x=\"PCA1\", y =\"PCA2\", hue='species', data=iris); <p>Vemos que en la representaci\u00f3n bidimensional, las especies est\u00e1n bastante bien separadas, \u00a1aunque el algoritmo PCA no ten\u00eda conocimiento de las etiquetas de las especies!.</p> <p>Esto nos indica que una clasificaci\u00f3n relativamente sencilla probablemente ser\u00e1 efectiva en el conjunto de datos, como vimos antes.</p> In\u00a0[26]: Copied! <pre># 1. Elija la clase de modelo\nfrom sklearn.cluster import KMeans\n</pre> # 1. Elija la clase de modelo from sklearn.cluster import KMeans In\u00a0[27]: Copied! <pre># 2. Instancia el modelo con hiperpar\u00e1metros\nmodel = KMeans(n_clusters=3, n_init=100 )\n</pre> # 2. Instancia el modelo con hiperpar\u00e1metros model = KMeans(n_clusters=3, n_init=100 ) In\u00a0[28]: Copied! <pre># 3. Ajuste a los datos. \u00a1Observe que y no est\u00e1 especificado!\nmodel.fit(X_iris)\n</pre> # 3. Ajuste a los datos. \u00a1Observe que y no est\u00e1 especificado! model.fit(X_iris)  Out[28]: <pre>KMeans(n_clusters=3, n_init=100)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeans<pre>KMeans(n_clusters=3, n_init=100)</pre> In\u00a0[29]: Copied! <pre># 4. Determinar las etiquetas de los grupos                 \ny_kmeans = model.predict(X_iris)\n</pre> # 4. Determinar las etiquetas de los grupos                  y_kmeans = model.predict(X_iris)         <p>Como antes, agregaremos la etiqueta del grupo al Iris DataFrame y usaremos Seaborn para trazar los resultados:</p> In\u00a0[30]: Copied! <pre>iris['cluster'] = y_kmeans\n\nsns.lmplot(x = \"PCA1\", y = \"PCA2\", data=iris,\n           hue='species',col='cluster', fit_reg=False);\n</pre> iris['cluster'] = y_kmeans  sns.lmplot(x = \"PCA1\", y = \"PCA2\", data=iris,            hue='species',col='cluster', fit_reg=False); <p>Al dividir los datos por n\u00famero de grupo, vemos exactamente qu\u00e9 tan bien el algoritmo KMeans ha recuperado la etiqueta subyacente.</p> <p>Esto significa que incluso sin un experto que nos diga las etiquetas de especies de las flores individuales, las medidas de estas flores son lo suficientemente distintas como para que podamos identificar autom\u00e1ticamente la presencia de estos diferentes grupos de especies con un simple algoritmo de agrupaci\u00f3n.</p>"},{"location":"introduction/012_sklearn/#scikit-learn","title":"Scikit-Learn\u00b6","text":"<p>Scikit-Learn (tambi\u00e9n conocido como sklearn) es una de las bibliotecas m\u00e1s populares de aprendizaje autom\u00e1tico en Python. Proporciona una amplia gama de herramientas y algoritmos de aprendizaje autom\u00e1tico para tareas como clasificaci\u00f3n, regresi\u00f3n, agrupamiento y reducci\u00f3n de la dimensionalidad.</p> <p>Algunas caracter\u00edsticas:</p> <ul> <li><p>Scikit-Learn se basa en otras bibliotecas populares de Python, como NumPy, SciPy y matplotlib, y es compatible con una variedad de herramientas y paquetes de an\u00e1lisis de datos de Python, como pandas.</p> </li> <li><p>Scikit-Learn se destaca en su facilidad de uso, la eficiencia computacional y la documentaci\u00f3n detallada. Scikit-Learn tambi\u00e9n ofrece una amplia gama de herramientas para la selecci\u00f3n de caracter\u00edsticas, la evaluaci\u00f3n del rendimiento y la validaci\u00f3n cruzada.</p> </li> <li><p>Scikit-Learn es una herramienta \u00fatil para cualquier persona que trabaje en el campo del aprendizaje autom\u00e1tico, desde principiantes hasta expertos en la materia.</p> </li> </ul>"},{"location":"introduction/012_sklearn/#api-de-scikit-learn","title":"Api de Scikit-Learn\u00b6","text":"<p>La API de Scikit-Learn est\u00e1 dise\u00f1ada con varios principios rectores en mente, tal como se describe en Scikit-Learn API paper.</p> <p>Estos principios incluyen:</p> <ul> <li><p>Consistencia: todos los objetos en Scikit-Learn deben tener una interfaz consistente. Esto significa que deben compartir un conjunto com\u00fan de m\u00e9todos, como <code>fit</code>, <code>predict</code> y <code>transform</code>, para que puedan utilizarse de manera intercambiable en tuber\u00edas y otros contextos.</p> </li> <li><p>Inspecci\u00f3n: los objetos de Scikit-Learn deben exponer su estado interno como atributos p\u00fablicamente accesibles. Esto permite a los usuarios inspeccionar el estado del objeto y comprender c\u00f3mo realiza predicciones.</p> </li> <li><p>Jerarqu\u00eda limitada de objetos: Scikit-Learn debe tener una jerarqu\u00eda limitada de objetos, con solo unos pocos objetos principales que se pueden componer de diferentes maneras. Esto mantiene la API simple y f\u00e1cil de aprender.</p> </li> <li><p>Composici\u00f3n: Scikit-Learn debe proporcionar una manera simple y flexible de componer algoritmos en tuber\u00edas. Esto permite a los usuarios construir modelos complejos a partir de bloques de construcci\u00f3n simples.</p> </li> <li><p>Valores predeterminados razonables: Scikit-Learn debe proporcionar valores predeterminados razonables para la mayor\u00eda de los par\u00e1metros, para que los usuarios puedan obtener resultados razonables sin tener que ajustar una gran cantidad de hiperpar\u00e1metros.</p> </li> </ul>"},{"location":"introduction/012_sklearn/#utilizando-scikit-learn","title":"Utilizando Scikit-Learn\u00b6","text":"<p>Los pasos para utilizar la API de Scikit-Learn son los siguientes:</p> <ul> <li>Organizar los datos en una matriz de caracter\u00edsticas ($X$) y un vector de targets ($y$) siguiendo la discusi\u00f3n anterior.</li> <li>Elegir una clase de modelo importando la clase de estimador adecuada de Scikit-Learn.</li> <li>Elegir los hiperpar\u00e1metros del modelo instanciando esta clase con los valores deseados.</li> <li>Ajustar el modelo a los datos llamando al m\u00e9todo <code>fit()</code> de la instancia del modelo.</li> <li>Aplicar el modelo a nuevos datos:<ul> <li>Para el aprendizaje supervisado, a menudo predecimos etiquetas para datos desconocidos utilizando el m\u00e9todo <code>predict()</code>.</li> <li>Para el aprendizaje no supervisado, a menudo transformamos o inferimos propiedades de los datos utilizando los m\u00e9todos <code>transform()</code> o <code>predict()</code>. Ahora veremos algunos ejemplos simples de aplicar m\u00e9todos de aprendizaje supervisado y no supervisado.</li> </ul> </li> </ul>"},{"location":"introduction/012_sklearn/#ejemplo-de-aprendizaje-supervisado-regresion-lineal-simple","title":"Ejemplo de aprendizaje supervisado: regresi\u00f3n lineal simple\u00b6","text":"<p>Como ejemplo de este proceso, consideremos una simple regresi\u00f3n lineal, es decir, el caso com\u00fan de ajustar una l\u00ednea a datos $(x, y)$. Utilizaremos los siguientes datos simples para nuestro ejemplo de regresi\u00f3n:</p>"},{"location":"introduction/012_sklearn/#trabajar-matriz-de-caracteristicas-y-arreglo-de-etiquetas","title":"Trabajar Matriz de Caracter\u00edsticas y Arreglo de Etiquetas\u00b6","text":"<p>Definiremos la matriz de caracter\u00edsticas bidimensional y una matriz unidimensional de objetivos.</p>"},{"location":"introduction/012_sklearn/#elige-una-clase-de-modelo","title":"Elige una clase de modelo\u00b6","text":"<p>En Scikit-Learn, cada clase de modelo est\u00e1 representada por una clase de Python. Entonces, por ejemplo, si nos gustar\u00eda calcular un modelo de regresi\u00f3n lineal simple, podemos importar la clase de regresi\u00f3n lineal:</p>"},{"location":"introduction/012_sklearn/#elija-los-hiperparametros-del-modelo","title":"Elija los hiperpar\u00e1metros del modelo\u00b6","text":"<p>Un punto importante es que una clase de modelo no es lo mismo que una instancia de un modelo.</p> <p>Dependiendo de la clase de modelo con la que estemos trabajando, podr\u00edamos necesitar responder una o varias preguntas como las siguientes:</p> <ul> <li>\u00bf Queremos ajustar el offset (es decir, la intersecci\u00f3n en <code>y</code>) ?</li> <li>\u00bf Queremos que el modelo est\u00e9 normalizado ?</li> <li>\u00bf Queremos preprocesar nuestras caracter\u00edsticas para agregar flexibilidad al modelo ?</li> <li>\u00bf Qu\u00e9 grado de regularizaci\u00f3n queremos usar en nuestro modelo ?</li> <li>\u00bf Cu\u00e1ntos componentes de modelo queremos usar ?</li> </ul> <p>Estos son ejemplos de las decisiones importantes que deben tomarse una vez que se selecciona la clase de modelo. Estas decisiones suelen representarse como hiperpar\u00e1metros, o par\u00e1metros que deben establecerse antes de que el modelo se ajuste a los datos. En Scikit-Learn, los hiperpar\u00e1metros se eligen pasando valores en la instanciaci\u00f3n del modelo. Exploraremos c\u00f3mo escoger los mejores hiperpar\u00e1metros m\u00e1s adelante.</p> <p>Para nuestro ejemplo de regresi\u00f3n lineal, podemos instanciar la clase <code>LinearRegression</code> y especificar que nos gustar\u00eda ajustar la intersecci\u00f3n usando el hiperpar\u00e1metro <code>fit_intercept</code>:</p>"},{"location":"introduction/012_sklearn/#ajusta-el-modelo-a-tus-datos","title":"Ajusta el modelo a tus datos\u00b6","text":"<p>Ahora es el momento de aplicar nuestro modelo a los datos. Esto se puede hacer con el m\u00e9todo <code>fit()</code> del modelo:</p>"},{"location":"introduction/012_sklearn/#predecir-datos-desconocidos","title":"Predecir  datos desconocidos\u00b6","text":"<p>Una vez que se entrena el modelo, la tarea principal del aprendizaje autom\u00e1tico supervisado es evaluarlo en funci\u00f3n de lo que dice sobre los nuevos datos que no formaban parte del conjunto de entrenamiento. En Scikit-Learn, esto se puede hacer usando el m\u00e9todo <code>predict()</code>. Por el bien de este ejemplo, nuestros \"nuevos datos\" ser\u00e1n una cuadr\u00edcula de valores de x, y preguntaremos qu\u00e9 valores de y predice el modelo:</p>"},{"location":"introduction/012_sklearn/#ejemplo-de-aprendizaje-supervisado-clasificacion-iris","title":"Ejemplo de aprendizaje supervisado: clasificaci\u00f3n Iris\u00b6","text":"<p>Utilizando el conjunto de datos de Iris ,la pregunta ser\u00e1 la siguiente: dado un modelo entrenado en una parte de los datos de Iris, \u00bfqu\u00e9 tan bien podemos predecir las etiquetas restantes?</p>"},{"location":"introduction/012_sklearn/#ejemplo-de-aprendizaje-no-supervisado-dimensionalidad-del-iris","title":"Ejemplo de aprendizaje no supervisado: dimensionalidad del iris\u00b6","text":"<p>En este ejemplo, buscamos reducir la dimensionalidad de los datos de Iris para visualizarlos m\u00e1s f\u00e1cilmente. Recuerde que los datos de Iris son de cuatro dimensiones: hay cuatro caracter\u00edsticas registradas para cada muestra.</p> <p>La tarea de la reducci\u00f3n de la dimensionalidad es preguntar si existe una representaci\u00f3n adecuada de menor dimensi\u00f3n que conserve las caracter\u00edsticas esenciales de los datos. A menudo, la reducci\u00f3n de la dimensionalidad se utiliza como ayuda para visualizar datos: despu\u00e9s de todo, \u00a1es mucho m\u00e1s f\u00e1cil trazar datos en dos dimensiones que en cuatro dimensiones o m\u00e1s!.</p> <p>Aqu\u00ed utilizaremos el an\u00e1lisis de componentes principales (PCA), que es una t\u00e9cnica de reducci\u00f3n de dimensionalidad lineal r\u00e1pida. Le pediremos al modelo que devuelva dos componentes, es decir, una representaci\u00f3n bidimensional de los datos.</p> <p>Siguiendo la secuencia de pasos descrita anteriormente, tenemos:</p>"},{"location":"introduction/012_sklearn/#aprendizaje-no-supervisado-iris-clustering","title":"Aprendizaje no supervisado: Iris clustering\u00b6","text":"<p>Veamos a continuaci\u00f3n c\u00f3mo aplicar la agrupaci\u00f3n en cl\u00fasteres a los datos de Iris. Un algoritmo de agrupamiento intenta encontrar distintos grupos de datos sin referencia a ninguna etiqueta. Aqu\u00ed usaremos un poderoso m\u00e9todo de agrupamiento llamado KMeans.</p>"},{"location":"introduction/0131_variables/","title":"Tipos de caracter\u00edsticas","text":"<ul> <li><p>Escala: Normalmente significa cambiar el rango de los valores. La forma de la distribuci\u00f3n no cambia. Piensen que un modo de escala de un edificio tiene las mismas proporciones que el original, pero m\u00e1s peque\u00f1as. Es por eso que decimos que est\u00e1 \u201cdibujado a escala\u201d.</p> <ul> <li><p>MinMaxScaler:  se normaliza ocupando los valores de los m\u00ednimos y m\u00e1ximo del conjunto de datos. $$x_{prep} = \\dfrac{x-x_{min}}{x_{min}-x_{max}}$$</p> <p>Esta forma de normalizar resulta \u00fatil cuando la desviaci\u00f3n estandar $s$ es muy peque\u00f1a (cercana) a cero, por lo que lo convierte en un estimador m\u00e1s roubusto que el StandardScaler.</p> </li> </ul> </li> </ul> <p>Nota: Los valores escalados pierden sus unidades originales.  Los d\u00f3lares ya no est\u00e1n en unidades de d\u00f3lar, los metros ya no est\u00e1n en unidades de metro, etc.</p> In\u00a0[1]: Copied! <pre># librerias\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# cargar datos\ndf = pd.read_csv(\"data/apartments.csv\")\ndf.head()\n</pre> # librerias import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split  # cargar datos df = pd.read_csv(\"data/apartments.csv\") df.head() Out[1]: Sold SqFt Price 0 1 200 906442 1 0 425 272629 2 1 675 824862 3 1 984 720344 4 0 727 879679 <p>El objetivo es la columna Sold (vendido) que indica si el departamento se vendi\u00f3 en las 2 semanas siguientes a su publicaci\u00f3n. Las caracter\u00edsticas son los metros cuadrados y el precio de lista del apartamento.</p> In\u00a0[2]: Copied! <pre># Asignen la columna de objetivo como y\ny = df['Sold']\n# Asignen el resto de las columnas como X\nX = df.drop(columns = 'Sold')\n</pre> # Asignen la columna de objetivo como y y = df['Sold'] # Asignen el resto de las columnas como X X = df.drop(columns = 'Sold') <p>Ahora dividiremos los datos en un conjunto de entrenamiento y conjunto de prueba.</p> In\u00a0[3]: Copied! <pre># train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n</pre> # train/test split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) <p>Antes de hace un escalamiento, exploremos nuestros datos originales. Observen que solo estamos explorando el conjunto de entrenamiento. Mantenemos oculta en nuestro an\u00e1lisis cualquier informaci\u00f3n sobre el conjunto de prueba.</p> In\u00a0[4]: Copied! <pre># Obtengan estad\u00edstica descriptiva de las caracter\u00edsticas\nX_train.describe().round(0)\n</pre> # Obtengan estad\u00edstica descriptiva de las caracter\u00edsticas X_train.describe().round(0) Out[4]: SqFt Price count 75.0 75.0 mean 564.0 524950.0 std 285.0 274185.0 min 114.0 109277.0 25% 320.0 272804.0 50% 588.0 503613.0 75% 836.0 786078.0 max 997.0 995878.0 <p>La estad\u00edstica descriptiva de arriba les ayudar\u00e1 a comprender los datos originales (antes que los escalemos).  El rengo de SqFt es 114-997 y el de Price es 109277 - 995878.  La media de SqFt es 564 y de Price es 524950.</p> In\u00a0[5]: Copied! <pre># instancien el escalador\nscaler = StandardScaler()\n# ajusten el escalador en los datos de entrenamiento\nscaler.fit(X_train)\n</pre> # instancien el escalador scaler = StandardScaler() # ajusten el escalador en los datos de entrenamiento scaler.fit(X_train) Out[5]: <pre>StandardScaler()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler<pre>StandardScaler()</pre> <p>El paso de ajuste realiza los c\u00e1lculos, pero NO los aplica. Despu\u00e9s de ajustarlos, los datos siguen siendo los mismos.</p> <p>Para aplicar los c\u00e1lculos hechos en el primer paso, hay que transformar los datos. Transformaremos el conjunto de entrenamiento y de prueba.</p> In\u00a0[6]: Copied! <pre># transformen los datos de entrenamiento\ntrain_scaled = scaler.transform(X_train)\n# transformen los datos de prueba\ntest_scaled = scaler.transform(X_test)\n# observen las 5 primeras filas de train_scaled\ntrain_scaled[:5]\n</pre> # transformen los datos de entrenamiento train_scaled = scaler.transform(X_train) # transformen los datos de prueba test_scaled = scaler.transform(X_test) # observen las 5 primeras filas de train_scaled train_scaled[:5] Out[6]: <pre>array([[-1.37431725,  1.72912293],\n       [ 1.34901239, -0.33899825],\n       [ 1.35959527, -0.23730597],\n       [ 1.1197165 , -1.01916082],\n       [ 0.98919422,  1.07150845]])</pre> <p>Observen que <code>StandardScaler</code>, como todos los transformadores <code>sklearn</code> que vamos a conocer, da salida a arrays de <code>Numpy</code>, no a DataFrames Pandas.  Si queremos convertir un array de NumPy en un DataFrame (que no tenemos que hacerlo necesariamente), podemos usar <code>pd.DataFrame()</code>.</p> <p>Vamos a convertir de nuevo a un DataFrame para permitirnos explorar y comprender los efectos de transformar nuestros datos con StandardScaler</p> In\u00a0[7]: Copied! <pre># transformen de nuevo a un DataFrame\nX_train_scaled = pd.DataFrame(train_scaled, columns=X_train.columns)\nX_train_scaled.head()\n</pre> # transformen de nuevo a un DataFrame X_train_scaled = pd.DataFrame(train_scaled, columns=X_train.columns) X_train_scaled.head() Out[7]: SqFt Price 0 -1.374317 1.729123 1 1.349012 -0.338998 2 1.359595 -0.237306 3 1.119716 -1.019161 4 0.989194 1.071508 <p>Exploren los datos escalados:</p> In\u00a0[8]: Copied! <pre># Obtengan una estad\u00edstica descriptiva de los datos escalados\n# Utilicen .round(2) para eliminar la notaci\u00f3n cient\u00edfica y mantener 2 lugares desp\u00faes del decimal\nX_train_scaled.describe().round(2)\n</pre> # Obtengan una estad\u00edstica descriptiva de los datos escalados # Utilicen .round(2) para eliminar la notaci\u00f3n cient\u00edfica y mantener 2 lugares desp\u00faes del decimal X_train_scaled.describe().round(2) Out[8]: SqFt Price count 75.00 75.00 mean -0.00 0.00 std 1.01 1.01 min -1.59 -1.53 25% -0.86 -0.93 50% 0.09 -0.08 75% 0.96 0.96 max 1.53 1.73 <p>Lo primero que deber\u00edan notar sobre la estad\u00edstica descriptiva es que la media de las caracter\u00edsticas ser\u00e1 aproximadamente de 0 y que la desviaci\u00f3n est\u00e1ndar ser\u00e1 de aproximadamente de 1.</p> <p>Los datos originales estaban en diferentes escalas. La magnitud del valor ahora representa qu\u00e9 tan lejos est\u00e1 cada valor a partir de la media de cada caracter\u00edstica, en unidades de desviaci\u00f3n est\u00e1ndar.  Los valores que est\u00e1n m\u00e1s cerca de la media estar\u00e1n m\u00e1s cercanos a cero. A medida que un valor se vuelve notablemente diferente que la media, tendr\u00e1 una magnitud m\u00e1s grande.</p> <p>Tambi\u00e9n observar\u00e1n que algunos valores son negativos y otros positivos. Con la nueva media fijada en 0, cualquier valor por debajo de la media es negativo y cualquier valor por encima de la media es positivo.</p> <p>Los valores con magnitudes grandes (ya sea en la direcci\u00f3n + o -) se consideran valores at\u00edpicos. Si bien no existe un umbral exacto para establecer los valores at\u00edpicos, generalmente se consideran at\u00edpicos los valores de escala superiores a -3 o 3.</p> In\u00a0[9]: Copied! <pre># ejemplo\nimport pandas as pd\n\ndf =pd.DataFrame({\n'Size': ['S','M','M','XL','L','S','L','XL','S'],\n'Color':['Blue','Red','Green','Green','Red','Blue','Blue','Red','Green'],\n'Cost': [5.00, 7.49, 8.00, 4.00, 9.99, 6.45, 4.32, 8.90, 9.00],\n'Sold':['Y','Y','N','N','Y','Y','N','N','Y']\n})\n\ndf.head()\n</pre> # ejemplo import pandas as pd  df =pd.DataFrame({ 'Size': ['S','M','M','XL','L','S','L','XL','S'], 'Color':['Blue','Red','Green','Green','Red','Blue','Blue','Red','Green'], 'Cost': [5.00, 7.49, 8.00, 4.00, 9.99, 6.45, 4.32, 8.90, 9.00], 'Sold':['Y','Y','N','N','Y','Y','N','N','Y'] })  df.head() Out[9]: Size Color Cost Sold 0 S Blue 5.00 Y 1 M Red 7.49 Y 2 M Green 8.00 N 3 XL Green 4.00 N 4 L Red 9.99 Y <p>Reemplacen la caracter\u00edstica ordinal (tama\u00f1o) con n\u00fameros</p> <p>Existen varias maneras para lograr esto en Python. Este ejemplo muestra c\u00f3mo pueden crear un diccionario y utilizar .replace para hacer los cambios.  Tengan en cuenta que estamos adquiriendo el h\u00e1bito de mantener nuestro conjunto de pruebas por separado, pero aplicaremos el mismo preprocesamiento a nuestro conjunto de prueba cuando estemos listos para evaluar nuestro modelo.</p> In\u00a0[10]: Copied! <pre># asignen el objetivo y, y las caracter\u00edsticas X\nsizes = {'S': 0, 'M': 1, 'L': 2, 'XL': 3}\ndf['Size'] = df['Size'].replace(sizes)\ndf.head()\n</pre> # asignen el objetivo y, y las caracter\u00edsticas X sizes = {'S': 0, 'M': 1, 'L': 2, 'XL': 3} df['Size'] = df['Size'].replace(sizes) df.head() Out[10]: Size Color Cost Sold 0 0 Blue 5.00 Y 1 1 Red 7.49 Y 2 1 Green 8.00 N 3 3 Green 4.00 N 4 2 Red 9.99 Y <p></p> <p>La primera fila del DataFrame en la izquiera tiene el valor \u201crojo\u201d en la columna \u201ccolor\u201d.  En el nuevo DataFrame a la derecha, la primera fila tiene un 1 bajo la columna \u201ccolor_red\u201d y un 0 en las otras dos columnas.  Las categor\u00edas desordenadas se han codificado como n\u00fameros.</p> In\u00a0[11]: Copied! <pre># ejemplo\n\n# librerias\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_selector\nfrom sklearn.preprocessing import OneHotEncoder\n\n# leer datos\n\ndf = pd.read_csv('data/medical_data.csv')\ndf.head()\n</pre> # ejemplo  # librerias import pandas as pd from sklearn.model_selection import train_test_split from sklearn.compose import make_column_selector from sklearn.preprocessing import OneHotEncoder  # leer datos  df = pd.read_csv('data/medical_data.csv') df.head() Out[11]: State Lat Lng Area Children Age Income Marital Gender ReAdmis ... Hyperlipidemia BackPain Anxiety Allergic_rhinitis Reflux_esophagitis Asthma Services Initial_days TotalCharge Additional_charges 0 AL 34.34960 -86.72508 Suburban 1.0 53 86575.93 Divorced Male 0 ... 0.0 1.0 1.0 1.0 0 1 Blood Work 10.585770 3726.702860 17939.403420 1 FL 30.84513 -85.22907 Urban 3.0 51 46805.99 Married Female 0 ... 0.0 0.0 0.0 0.0 1 0 Intravenous 15.129562 4193.190458 17612.998120 2 SD 43.54321 -96.63772 Suburban 3.0 53 14370.14 Widowed Female 0 ... 0.0 0.0 0.0 0.0 0 0 Blood Work 4.772177 2434.234222 17505.192460 3 MN 43.89744 -93.51479 Suburban 0.0 78 39741.49 Married Male 0 ... 0.0 0.0 0.0 0.0 1 1 Blood Work 1.714879 2127.830423 12993.437350 4 VA 37.59894 -76.88958 Rural 1.0 22 1209.56 Widowed Female 0 ... 1.0 0.0 0.0 1.0 0 0 CT Scan 1.254807 2113.073274 3716.525786 <p>5 rows \u00d7 32 columns</p> In\u00a0[12]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 32 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   State               995 non-null    object \n 1   Lat                 1000 non-null   float64\n 2   Lng                 1000 non-null   float64\n 3   Area                995 non-null    object \n 4   Children            993 non-null    float64\n 5   Age                 1000 non-null   int64  \n 6   Income              1000 non-null   float64\n 7   Marital             995 non-null    object \n 8   Gender              995 non-null    object \n 9   ReAdmis             1000 non-null   int64  \n 10  VitD_levels         1000 non-null   float64\n 11  Doc_visits          1000 non-null   int64  \n 12  Full_meals_eaten    1000 non-null   int64  \n 13  vitD_supp           1000 non-null   int64  \n 14  Soft_drink          1000 non-null   int64  \n 15  Initial_admin       995 non-null    object \n 16  HighBlood           1000 non-null   int64  \n 17  Stroke              1000 non-null   int64  \n 18  Complication_risk   995 non-null    object \n 19  Overweight          1000 non-null   int64  \n 20  Arthritis           994 non-null    float64\n 21  Diabetes            994 non-null    float64\n 22  Hyperlipidemia      998 non-null    float64\n 23  BackPain            992 non-null    float64\n 24  Anxiety             998 non-null    float64\n 25  Allergic_rhinitis   994 non-null    float64\n 26  Reflux_esophagitis  1000 non-null   int64  \n 27  Asthma              1000 non-null   int64  \n 28  Services            995 non-null    object \n 29  Initial_days        1000 non-null   float64\n 30  TotalCharge         1000 non-null   float64\n 31  Additional_charges  1000 non-null   float64\ndtypes: float64(14), int64(11), object(7)\nmemory usage: 250.1+ KB\n</pre> <p>Hay numerosas columnas de \u201cobjeto\u201d que tendr\u00e1n que ser codificadas.</p> <p>La columna <code>Complication_Risk</code> est\u00e1 ordenada, y podemos usar codificaci\u00f3n ordinal para esta.  El m\u00e9todo <code>df.replace()</code> funciona para esto.</p> In\u00a0[13]: Copied! <pre>df['Complication_risk'].value_counts()\n</pre> df['Complication_risk'].value_counts() Out[13]: <pre>Medium    459\nHigh      311\nLow       221\nMed         4\nName: Complication_risk, dtype: int64</pre> In\u00a0[14]: Copied! <pre>df['Complication_risk'].replace({'Low':0, 'Med':1, 'Medium':1, 'High':2}, inplace=True)\ndf['Complication_risk'].value_counts()\n</pre> df['Complication_risk'].replace({'Low':0, 'Med':1, 'Medium':1, 'High':2}, inplace=True) df['Complication_risk'].value_counts() Out[14]: <pre>1.0    463\n2.0    311\n0.0    221\nName: Complication_risk, dtype: int64</pre> <p>Nota: La codificaci\u00f3n ordinal que hicimos arriba no dependi\u00f3 de la informaci\u00f3n de los datos de prueba, por lo que no causar\u00eda una fuga de datos.  Podemos hacer esto antes de dividir los datos.</p> <p>Para esta tarea, el prop\u00f3sito es predecir los Additional charges (gastos adicionales) que se basan en las otras caracter\u00edsticas en el conjunto de datos.</p> <p>Asignaremos Additional_charges como nuestro objetivo (y).</p> <p>Asignaremos el resto de las columnas como las caracter\u00edsticas (X).  Tambi\u00e9n eliminaremos la columna \"Unnamed: 0\", ya que no es una caracter\u00edstica relevante.</p> In\u00a0[15]: Copied! <pre>X = df.drop(columns = ['Additional_charges'])\ny = df['Additional_charges']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n</pre> X = df.drop(columns = ['Additional_charges']) y = df['Additional_charges'] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) <p>Solo queremos una codificaci\u00f3n one-hot para las caracter\u00edsticas categ\u00f3ricas, NO para las caracter\u00edsticas num\u00e9ricas, por lo que necesitamos una manera de dividir las caracter\u00edsticas categ\u00f3ricas de las num\u00e9ricas.  Podr\u00edamos hacer esto manualmente, pero <code>make_column_selector</code> nos permitir\u00e1 hacer esto m\u00e1s eficazmente.</p> <p>En este caso le diremos a make_column_selector que solo seleccione las columnas con el tipo <code>object</code>.</p> In\u00a0[16]: Copied! <pre># hagan un selector categ\u00f3rico\ncat_selector = make_column_selector(dtype_include='object')\n</pre> # hagan un selector categ\u00f3rico cat_selector = make_column_selector(dtype_include='object') <p>Cuando aplicamos el cat_selector aun DataFrame, volver\u00e1 a una lista con los nombres de las columnas que coinciden con el patr\u00f3n que le dimos.</p> In\u00a0[17]: Copied! <pre>cat_selector(X_train)\n</pre> cat_selector(X_train) Out[17]: <pre>['State', 'Area', 'Marital', 'Gender', 'Initial_admin', 'Services']</pre> <p>Ahora podemos utilizar esa lista a un subconjunto de un conjunto de datos original.</p> <p>El c\u00f3digo de abajo crea dos DataFrames nuevos (train_cat_data and test_cat_data) que contiene solo seleccionado las caracter\u00edsticas del objeto con <code>cat_selector</code>.</p> In\u00a0[18]: Copied! <pre># creen un subconjunto de datos solo para las columnas categ\u00f3ricas\ntrain_cat_data = X_train[cat_selector(X_train)]\ntest_cat_data = X_test[cat_selector(X_test)]\ntrain_cat_data.head()\n</pre> # creen un subconjunto de datos solo para las columnas categ\u00f3ricas train_cat_data = X_train[cat_selector(X_train)] test_cat_data = X_test[cat_selector(X_test)] train_cat_data.head() Out[18]: State Area Marital Gender Initial_admin Services 82 TN Urban Never Married Female Emergency Admission Intravenous 991 AL Urban Married Male Emergency Admission Blood Work 789 TN Urban Married Nonbinary Observation Admission Intravenous 894 SD Rural Never Married Male Observation Admission Blood Work 398 MI Suburban Widowed Female Elective Admission Blood Work <p>Estamos listos para la codificaci\u00f3n one-hot en las caracter\u00edsticas categ\u00f3ricas</p> In\u00a0[19]: Copied! <pre># instancien la codificaci\u00f3n one-hot\nohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n# ajusten el OneHotEncoder en los datos de entrenamiento\nohe_encoder.fit(train_cat_data)\n# transforme los datos de entrenamiento y de prueba\ntrain_ohe = ohe_encoder.transform(train_cat_data)\ntest_ohe = ohe_encoder.transform(test_cat_data)\ntrain_ohe\n</pre> # instancien la codificaci\u00f3n one-hot ohe_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') # ajusten el OneHotEncoder en los datos de entrenamiento ohe_encoder.fit(train_cat_data) # transforme los datos de entrenamiento y de prueba train_ohe = ohe_encoder.transform(train_cat_data) test_ohe = ohe_encoder.transform(test_cat_data) train_ohe Out[19]: <pre>array([[0., 0., 0., ..., 1., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])</pre> <p>Observen que la salida de un array de NumPy en lugar de un DataFrame.</p> <p>Podemos usar el m\u00e9todo <code>get_feature_names_out()</code> para obtener una lista de las nuevas caracter\u00edsticas, pero debemos pasarlas a la lista de columnas originales (de antes de la codificaci\u00f3n de los datos).</p> In\u00a0[20]: Copied! <pre># conviertan a un DataFrame, obtengan una nueva columna de nombres a partir de la codificaci\u00f3n \n# establezcan prefijos a la columna de nombres original\nohe_column_names = ohe_encoder.get_feature_names_out(train_cat_data.columns)\ntrain_ohe = pd.DataFrame(train_ohe, columns=ohe_column_names)\ntest_ohe = pd.DataFrame(test_ohe, columns=ohe_column_names)\ntrain_ohe.head()\n</pre> # conviertan a un DataFrame, obtengan una nueva columna de nombres a partir de la codificaci\u00f3n  # establezcan prefijos a la columna de nombres original ohe_column_names = ohe_encoder.get_feature_names_out(train_cat_data.columns) train_ohe = pd.DataFrame(train_ohe, columns=ohe_column_names) test_ohe = pd.DataFrame(test_ohe, columns=ohe_column_names) train_ohe.head() Out[20]: State_AK State_AL State_AR State_AZ State_CA State_CO State_CT State_DC State_FL State_GA ... Gender_nan Initial_admin_Elective Admission Initial_admin_Emergency Admission Initial_admin_Observation Admission Initial_admin_nan Services_Blood Work Services_CT Scan Services_Intravenous Services_MRI Services_nan 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 <p>5 rows \u00d7 78 columns</p> <p>Observen que habr\u00e1 un \u201c0,0\u201d para todas las columnas de \u201cstate\u201d de cada fila, excepto una. La columna que corresponde a \u201cstate\u201d para esa fila tendr\u00e1 un \u201c1,0\u201d.</p> <p>Concatenar las caracter\u00edsticas categ\u00f3ricas de codificaci\u00f3n one-hot con las num\u00e9ricas</p> <p>Podemos usar la funci\u00f3n de Pandas: <code>pd.concat()</code> para alcanzar esto. La funci\u00f3n <code>pd.concat()</code> toma una lista de los DataFrames para concatenarlos juntos.  Por defecto los concatena en el eje 0, las filas.</p> <p>Nota: Tambi\u00e9n hay que resaltar que Pandas intentar\u00e1 usar el \u00edndice para determinar c\u00f3mo igualar las filas.  Cuando dividimos los datos, se mezclan con el \u00edndice, y cuando hace una codificaci\u00f3n one-hot en los datos, restablecen el \u00edndice.</p> In\u00a0[21]: Copied! <pre># creen un selector num\u00e9rico\nnum_selector = make_column_selector(dtype_include='number')\n# aislen las columnas num\u00e9ricas\ntrain_nums = X_train[num_selector(X_train)].reset_index(drop=True)\ntest_nums = X_test[num_selector(X_test)].reset_index(drop=True)\n# recombinen los conjuntos de entrenamiento y de prueba en el eje 1 (columnas)\nX_train_processed = pd.concat([train_nums, train_ohe], axis=1)\nX_test_processed = pd.concat([test_nums, test_ohe], axis=1)\nX_train_processed.head()\n</pre> # creen un selector num\u00e9rico num_selector = make_column_selector(dtype_include='number') # aislen las columnas num\u00e9ricas train_nums = X_train[num_selector(X_train)].reset_index(drop=True) test_nums = X_test[num_selector(X_test)].reset_index(drop=True) # recombinen los conjuntos de entrenamiento y de prueba en el eje 1 (columnas) X_train_processed = pd.concat([train_nums, train_ohe], axis=1) X_test_processed = pd.concat([test_nums, test_ohe], axis=1) X_train_processed.head() Out[21]: Lat Lng Children Age Income ReAdmis VitD_levels Doc_visits Full_meals_eaten vitD_supp ... Gender_nan Initial_admin_Elective Admission Initial_admin_Emergency Admission Initial_admin_Observation Admission Initial_admin_nan Services_Blood Work Services_CT Scan Services_Intravenous Services_MRI Services_nan 0 36.16307 -86.66510 2.0 60 8459.99 0 19.034162 5 1 0 ... 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1 34.96594 -87.12179 5.0 78 22669.31 0 15.903388 7 1 0 ... 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 2 36.24648 -83.51232 1.0 60 25536.25 0 18.225040 4 1 0 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 3 45.42189 -97.91165 7.0 82 94863.57 0 15.809932 5 0 2 ... 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 4 42.33661 -83.28292 0.0 37 30898.36 0 20.640410 5 1 0 ... 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 <p>5 rows \u00d7 103 columns</p> <p>Todas las columnas en nuestro DataFrame son ahora num\u00e9ricas sin p\u00e9rdida de datos.  Este DataFrame se puede usar en un modelo, sin embargo, es probable que tambi\u00e9n queramos escalar las columnas num\u00e9ricas si estamos usando ciertos tipos de modelos.</p> <p>\u00bfPor qu\u00e9 no podemos hacer una codificaci\u00f3n one-hot en los datos antes de dividirlos?</p> <p>A veces solo hay unas pocas, incluso a veces una, muestras con una categor\u00eda en particular en una caracter\u00edstica.  Cuando dividimos los datos, esa muestra podr\u00eda posiblemente terminar en el conjunto de prueba.  Esto ser\u00eda an\u00e1logo a un modelo de producci\u00f3n desplegado que se encuentra con una categor\u00eda que nunca hab\u00eda visto antes.  \u00bfDeber\u00eda una columna existir antes para esa categor\u00eda?  No tenemos manera de saber (en teor\u00eda) como ser\u00edan esas categor\u00edas ocultas, por lo que no podemos crear columnas nuevas para ellas.</p>"},{"location":"introduction/0131_variables/#tipos-de-caracteristicas","title":"Tipos de caracter\u00edsticas\u00b6","text":""},{"location":"introduction/0131_variables/#definiciones","title":"Definiciones\u00b6","text":"<p>Las caracter\u00edsticas num\u00e9ricas son n\u00fameros enteros o flotantes que representan una cantidad. Tambi\u00e9n pueden denominarse caracter\u00edsticas cuantitativas.</p> <p>Las caracter\u00edsticas num\u00e9ricas deben ser n\u00fameros enteros o flotantes, pero si los tipos de datos indican \u201cobjeto\u201d, podr\u00eda deberse a la presencia de un car\u00e1cter no num\u00e9rico como el signo del d\u00f3lar: $3.00.  Aseg\u00farense de abordar estos caracteres y cambiar el tipo de datos a un formato num\u00e9rico.</p> <ul> <li><p>Ejemplos:</p> <ul> <li>Precio</li> <li>MPG</li> <li>N\u00famero de habitaciones</li> </ul> </li> </ul> <p>Las caracter\u00edsticas ordinales son categor\u00edas que representan diferentes clases y que tienen un ordenamiento distinto.</p> <p>Pueden ser cadenas o n\u00fameros enteros si estos \u00faltimos representan una clase ordenada.</p> <ul> <li><p>Ejemplos:</p> <ul> <li>bajo, medio, alto</li> <li>Una estrella, dos estrellas, tres estrellas, cuatro estrellas y cinco estrellas</li> <li>0,1,2,3 donde 0 es peque\u00f1o, 1 es medio, 2 es grande y 3 es extragrande.</li> </ul> </li> </ul> <p>Las caracter\u00edsticas nominales son categor\u00edas que representan diferentes clases. No est\u00e1n ordenados.</p> <p>Pueden ser cadenas o n\u00fameros enteros si estos \u00faltimos representan una clase no ordenada.</p> <ul> <li><p>Ejemplos:</p> <ul> <li>hombre, mujer, no binario</li> <li>rojo, verde, azul, amarillo, p\u00farpura, verde</li> <li>0, 1, 2, 3 donde 0 es soltero, 1 es casado, 2 es divorciado y 3 es viudo</li> </ul> </li> </ul>"},{"location":"introduction/0131_variables/#transformar-caracteristicas","title":"Transformar caracter\u00edsticas\u00b6","text":""},{"location":"introduction/0131_variables/#transformar-caracteristicas-numericas","title":"Transformar caracter\u00edsticas num\u00e9ricas\u00b6","text":"<p>Estos ya est\u00e1n en formato num\u00e9rico, por lo que pueden utilizarse sin m\u00e1s transformaci\u00f3n. Sin embargo, es muy com\u00fan, y a veces necesario, escalar las caracter\u00edsticas num\u00e9ricas.</p> <p>\u00bfPor qu\u00e9 estandarizaci\u00f3n o escalamiento de datos?</p> <p>El aprendizaje autom\u00e1tico es un tema dif\u00edcil de aprender porque no solo pueden tener errores de programaci\u00f3n, sino errores en \u00e1reas diferentes. Un error com\u00fan es el de no comprender los supuestos de un modelo de aprendizaje autom\u00e1tico.</p> <p>Una suposici\u00f3n com\u00fan en muchos tipos de modelos que los datos est\u00e1n escalados</p>"},{"location":"introduction/0131_variables/#definiciones","title":"Definiciones\u00b6","text":"<ul> <li><p>Estandarizaci\u00f3n: La estandarizaci\u00f3n es uno de los diversos tipos de escalamiento.  Significa el escalamiento de valores para que la distribuci\u00f3n tenga una desviaci\u00f3n est\u00e1ndar de 1 con una media de 0.  El resultado es algo muy parecido a una distribuci\u00f3n normal.</p> <ul> <li><p>StandardScaler: se normaliza  restando la media y escalando por su desviaci\u00f3n estanda. $$x_{prep} = \\dfrac{x-u}{s}$$</p> <p>La ventaja es que la media del nuevo conjunto de datos cumple con la propiedad que su media $\\mu$ es igual a cero y su desviaci\u00f3n estandar $s$ es igual a 1.</p> </li> </ul> </li> </ul>"},{"location":"introduction/0131_variables/#ejemplos","title":"Ejemplos\u00b6","text":"<p>En Python se pueden escalar datos usando <code>StandardScaler</code> de <code>Scikit-learn</code>.</p> <p>Para evitar una fuga de datos, el escalador solo deber\u00eda ajustarse en el conjunto de entrenamiento.</p> <p>Cuando el escalador se ajusta a los datos, calcula las medias y las desviaciones est\u00e1ndar para cada caracter\u00edstica.  Luego el escalador se puede usar para transformar los conjuntos de entrenamiento y de prueba bas\u00e1ndose en los c\u00e1lculos que se hicieron en el primer paso.  Esto significa que el promedio y la variaci\u00f3n (desviaci\u00f3n est\u00e1ndar) se calcular\u00e1 usando solo los datos de entrenamiento, ya que queremos mantener informaci\u00f3n en los datos de prueba, en particular, informaci\u00f3n sobre las medias y variaciones, que se reservan solo para una evaluaci\u00f3n del modelo final.</p> <p>El escalamiento de los valores objetivos ($y$) no se requiere generalmente.</p>"},{"location":"introduction/0131_variables/#transformar-caracteristicas-ordinales","title":"Transformar caracter\u00edsticas ordinales\u00b6","text":"<p>Si son strings, habr\u00e1 que convertirlas en valores num\u00e9ricos que representen el orden de las clases.</p> <p>Observen que a veces esta conversi\u00f3n ya se ha producido, como en el ejemplo anterior, donde 0 es peque\u00f1o, 1 es medio, 2 es grande y 3 es extragrande. En este caso, no es necesaria ninguna otra transformaci\u00f3n.</p> <ul> <li><p>Ejemplos de una transformaci\u00f3n ordinal de cadena:</p> <ul> <li>bajo, medio, alto se transformar\u00e1n en 0, 1, 2</li> <li>Una estrella, dos estrellas, tres estrellas, cuatro estrellas, cinco estrellas se transformar\u00e1n en 0, 1, 2, 3, 4</li> </ul> </li> </ul>"},{"location":"introduction/0131_variables/#transformar-caracteristicas-nominales","title":"Transformar caracter\u00edsticas nominales\u00b6","text":"<p>Hay que tener cuidado cuando las categor\u00edas no est\u00e1n ordenadas. No basta con asignar cada clase a un n\u00famero, como puede hacerse con las caracter\u00edsticas ordinales.</p> <p>Esto se debe a que los modelos de aprendizaje autom\u00e1tico interpretar\u00e1n n\u00fameros m\u00e1s altos como si tuvieran un valor m\u00e1s alto que los n\u00fameros m\u00e1s bajos. Sin embargo, no deber\u00eda haber un valor alto o bajo asociado con variables nominales. Por ejemplo, si solo reemplazaron los valores rojos, verdes, azules con los n\u00fameros 0, 1, 2, el modelo aprendizaje autom\u00e1tico interpretar\u00e1 que el azul es mayor que el rojo y el verde. Sabemos que este no deber\u00eda ser el caso. Todos estos colores se deben tratar como valores iguales ya que ning\u00fan color es intr\u00ednsecamente mejor/m\u00e1s alto que otro.</p> <p>Para hacer frente esto, podemos codificar nuestras categor\u00edas de una sola vez. Lo que hace esto es crear una columna binaria para cada clase en la columna.</p> <p>\u00bfQu\u00e9 es la codificaci\u00f3n one-hot?</p> <p>En primer lugar, la codificaci\u00f3n one-hot NO captura el significado de las palabras.  La computadora no sabe c\u00f3mo es el color azul, pero s\u00ed puede encontrar relaciones entre el color y otras variables en el contexto de un conjunto de datos.  Para representar las caracter\u00edsticas no ordenadas o \u201cnominales\u201d, haremos lo siguiente:</p> <ol> <li><p>Crear una nueva columna para cada categor\u00eda presente en la caracter\u00edstica.</p> </li> <li><p>Establecer el valor de cada una de las nuevas columnas a 1 si esa fila corresponde a la categor\u00eda original</p> </li> <li><p>Establecer el valor de cada una de las nuevas columnas a 0 si no lo hacen.</p> </li> <li><p>Eliminar la columna original.</p> </li> </ol>"},{"location":"introduction/0132_transform/","title":"Transformador de columna","text":"<p>Ejemplo</p> <p>Este ejemplo utiliza el conjunto de datos m\u00e9dicos. Este conjunto de datos muestra una serie de informaci\u00f3n sobre el paciente, en particular, su informaci\u00f3n socioecon\u00f3mica junto con la condici\u00f3n m\u00e9dica que les diagnosticaron.</p> <p>La \u201cCodificaci\u00f3n ordinal y OneHotEncoder\u201d, se mostr\u00f3 como seleccionar las caracter\u00edsticas del objeto y ponelas en un DataFrame separado para la codificaci\u00f3n one-hot. Luego tuvieron que unir el DataFrame de codificaci\u00f3n one-hot con las columnas num\u00e9ricas.  En este ejemplo, ver\u00e1n c\u00f3mo realizar diferentes transformaciones de procesamiento a diferentes caracter\u00edsticas en los datos en un paso usando un transformador de columna.  Simult\u00e1neamente:</p> <ul> <li>Escalar\u00e1n columnas num\u00e9ricas usando Standard Scaler.</li> <li>Realizar\u00e1n una codificaci\u00f3n one-hot a las columnas categ\u00f3ricas usando OneHotEncoder.</li> </ul> In\u00a0[1]: Copied! <pre># importaciones\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_selector\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n</pre> # importaciones import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.compose import make_column_selector from sklearn.preprocessing import StandardScaler, OneHotEncoder In\u00a0[2]: Copied! <pre># leer datos\ndf = pd.read_csv('data/medical_data.csv') # or your path here\ndf.head()\n</pre> # leer datos df = pd.read_csv('data/medical_data.csv') # or your path here df.head() Out[2]: State Lat Lng Area Children Age Income Marital Gender ReAdmis ... Hyperlipidemia BackPain Anxiety Allergic_rhinitis Reflux_esophagitis Asthma Services Initial_days TotalCharge Additional_charges 0 AL 34.34960 -86.72508 Suburban 1.0 53 86575.93 Divorced Male 0 ... 0.0 1.0 1.0 1.0 0 1 Blood Work 10.585770 3726.702860 17939.403420 1 FL 30.84513 -85.22907 Urban 3.0 51 46805.99 Married Female 0 ... 0.0 0.0 0.0 0.0 1 0 Intravenous 15.129562 4193.190458 17612.998120 2 SD 43.54321 -96.63772 Suburban 3.0 53 14370.14 Widowed Female 0 ... 0.0 0.0 0.0 0.0 0 0 Blood Work 4.772177 2434.234222 17505.192460 3 MN 43.89744 -93.51479 Suburban 0.0 78 39741.49 Married Male 0 ... 0.0 0.0 0.0 0.0 1 1 Blood Work 1.714879 2127.830423 12993.437350 4 VA 37.59894 -76.88958 Rural 1.0 22 1209.56 Widowed Female 0 ... 1.0 0.0 0.0 1.0 0 0 CT Scan 1.254807 2113.073274 3716.525786 <p>5 rows \u00d7 32 columns</p> In\u00a0[3]: Copied! <pre># Formato para ML y train test split\nX = df.drop(columns = 'Additional_charges')\ny = df['Additional_charges']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n</pre> # Formato para ML y train test split X = df.drop(columns = 'Additional_charges') y = df['Additional_charges'] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42) <p>Realizar selectores de columnas</p> <p>Estamos haciendo dos selectores: uno para los datos del objeto y otro para los datos de los n\u00fameros.</p> In\u00a0[4]: Copied! <pre># Instanciar los selectores de columnas categ\u00f3ricas y num\u00e9ricas para seleccionar las columnas adecuadas\ncat_selector = make_column_selector(dtype_include='object')\nnum_selector = make_column_selector(dtype_include='number')\n</pre> # Instanciar los selectores de columnas categ\u00f3ricas y num\u00e9ricas para seleccionar las columnas adecuadas cat_selector = make_column_selector(dtype_include='object') num_selector = make_column_selector(dtype_include='number') <p>Instanciar los transformadores</p> <p>Ahora que han realizado los selectores, necesitan instanciar cada uno de los transformadores de columnas que quieran usar.  Ambos son procesadores comunes que a menudo usar\u00e1n en el procesamiento. Un StandardScaler para escalar las columnas num\u00e9ricas y un OneHotEncoder para codificar las columnas categ\u00f3ricas.</p> <p>Nota: Matrices dispersas vs. arrays densos</p> <p>F\u00edjense que en el siguiente c\u00f3digo que no est\u00e9n usando el argumento <code>sparse=False</code> en el constructor de OneHotEncoder.  Las clases de modelo que usar\u00e1n despu\u00e9s pueden procesar bien las matrices dispersas, y estas suelen ser necesarias para conservar memoria cuando el DataFrame resultante es muy largo.  En ejemplos anteriores establecimos <code>sparse=False</code> debido a que los arrays vac\u00edos son dif\u00edciles de interpretar visualmente y no se pueden transformar f\u00e1cilmente en un DataFrames de Pandas.</p> <p>El <code>ColumnTransformer</code> utilizado posteriormente devuelve la matriz dispersa a un array denso internamente cuando la concatena con los datos num\u00e9ricos escalados durante la transformaci\u00f3n.</p> In\u00a0[5]: Copied! <pre># Instanciar el escalador est\u00e1ndar y el codificador one hot\nscaler = StandardScaler()\nohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n</pre> # Instanciar el escalador est\u00e1ndar y el codificador one hot scaler = StandardScaler() ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  <p>Emparejar el transformador con las columnas</p> <p>El siguiente paso se trata de emparejar las columnas con la transformaci\u00f3n que queremos aplicar. Vamos a escalar los valores num\u00e9ricos para nuestros n\u00fameros usando Standard Scaler.  Vamos a confidicar one-hot cualquier valor categ\u00f3rico para nuestras columnas categ\u00f3ricas usando la codificaci\u00f3n one-hot. Los vamos a emparejar al crear tuplas donde el primer elemento es el transformador y el segundo elemento es una lista de columnas o un objeto ColumnSelector.</p> In\u00a0[6]: Copied! <pre># Make tuples for preprocessing the categorical and numeric columns\nnum_tuple = (scaler, num_selector)\ncat_tuple = (ohe, cat_selector)\n</pre> # Make tuples for preprocessing the categorical and numeric columns num_tuple = (scaler, num_selector) cat_tuple = (ohe, cat_selector) <p>Instanciaci\u00f3n de ColumnTransformer</p> <p>Ahora tenemos de instanciar el transformador de columna y agregar cada una de las tuplas que creamos antes. Observen que tenemos un nuevo import para obtener make_column_transformer.</p> In\u00a0[7]: Copied! <pre>from sklearn.compose import make_column_transformer\ncol_transformer = make_column_transformer(num_tuple, cat_tuple, remainder = 'passthrough')\n</pre> from sklearn.compose import make_column_transformer col_transformer = make_column_transformer(num_tuple, cat_tuple, remainder = 'passthrough') In\u00a0[8]: Copied! <pre># Encajar el transformador\ncol_transformer.fit(X_train)\n</pre> # Encajar el transformador col_transformer.fit(X_train) Out[8]: <pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F1684E5A30&gt;),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F1684E5A60&gt;)])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer<pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('standardscaler', StandardScaler(),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F1684E5A30&gt;),\n                                ('onehotencoder',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse_output=False),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F1684E5A60&gt;)])</pre>standardscaler<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F1684E5A30&gt;</pre>StandardScaler<pre>StandardScaler()</pre>onehotencoder<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F1684E5A60&gt;</pre>OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore', sparse_output=False)</pre>remainder<pre>[]</pre>passthrough<pre>passthrough</pre> In\u00a0[9]: Copied! <pre># Transformaci\u00f3n\nX_train_processed = col_transformer.transform(X_train)\nX_test_processed = col_transformer.transform(X_test)\n</pre> # Transformaci\u00f3n X_train_processed = col_transformer.transform(X_train) X_test_processed = col_transformer.transform(X_test) In\u00a0[10]: Copied! <pre># Ver las transformaciones\nX_train_df = pd.DataFrame(X_train_processed)\nX_train_df.head()\n</pre> # Ver las transformaciones X_train_df = pd.DataFrame(X_train_processed) X_train_df.head() Out[10]: 0 1 2 3 4 5 6 7 8 9 ... 97 98 99 100 101 102 103 104 105 106 0 -0.508205 0.281935 -0.065104 0.272586 -1.123467 0.0 0.509399 -0.008943 0.014639 -0.620174 ... 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 1 -0.720642 0.252836 1.235813 1.119125 -0.619881 0.0 -0.999823 1.907372 0.014639 -0.620174 ... 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 2 -0.493403 0.482823 -0.498743 0.272586 -0.518276 0.0 0.119354 -0.967100 0.014639 -0.620174 ... 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 3 1.134821 -0.434666 2.103090 1.307245 1.938720 0.0 -1.044875 -0.008943 -0.983474 2.762592 ... 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 4 0.587322 0.497439 -0.932382 -0.809103 -0.328240 0.0 1.283708 -0.008943 0.014639 -0.620174 ... 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 <p>5 rows \u00d7 107 columns</p> In\u00a0[11]: Copied! <pre>X_train_df.info()\n</pre> X_train_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 750 entries, 0 to 749\nColumns: 107 entries, 0 to 106\ndtypes: float64(107)\nmemory usage: 627.1 KB\n</pre> <p>Ahora todas nuestras transformaciones est\u00e1n completas y est\u00e1n listas para ser modeladas. Podemos ver que las columnas num\u00e9ricas se han escalado y las columnas categ\u00f3ricas se han codificado como n\u00fameros. Todas las columnas aparecen ahora como columnas num\u00e9ricas y est\u00e1n listas para el aprendizaje autom\u00e1tico. \u00a1Misi\u00f3n cumplida!</p> In\u00a0[12]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import set_config\nset_config(display='diagram')\n</pre> import pandas as pd from sklearn.model_selection import train_test_split from sklearn.compose import make_column_selector, make_column_transformer from sklearn.impute import SimpleImputer from sklearn import set_config set_config(display='diagram') In\u00a0[13]: Copied! <pre># leer datos\ndf = pd.read_csv(\"data/medical_data.csv\")\ndf.head()\n</pre> # leer datos df = pd.read_csv(\"data/medical_data.csv\") df.head() Out[13]: State Lat Lng Area Children Age Income Marital Gender ReAdmis ... Hyperlipidemia BackPain Anxiety Allergic_rhinitis Reflux_esophagitis Asthma Services Initial_days TotalCharge Additional_charges 0 AL 34.34960 -86.72508 Suburban 1.0 53 86575.93 Divorced Male 0 ... 0.0 1.0 1.0 1.0 0 1 Blood Work 10.585770 3726.702860 17939.403420 1 FL 30.84513 -85.22907 Urban 3.0 51 46805.99 Married Female 0 ... 0.0 0.0 0.0 0.0 1 0 Intravenous 15.129562 4193.190458 17612.998120 2 SD 43.54321 -96.63772 Suburban 3.0 53 14370.14 Widowed Female 0 ... 0.0 0.0 0.0 0.0 0 0 Blood Work 4.772177 2434.234222 17505.192460 3 MN 43.89744 -93.51479 Suburban 0.0 78 39741.49 Married Male 0 ... 0.0 0.0 0.0 0.0 1 1 Blood Work 1.714879 2127.830423 12993.437350 4 VA 37.59894 -76.88958 Rural 1.0 22 1209.56 Widowed Female 0 ... 1.0 0.0 0.0 1.0 0 0 CT Scan 1.254807 2113.073274 3716.525786 <p>5 rows \u00d7 32 columns</p> In\u00a0[14]: Copied! <pre># Examinar los valores faltantes\n\ndf.info()\n</pre> # Examinar los valores faltantes  df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 32 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   State               995 non-null    object \n 1   Lat                 1000 non-null   float64\n 2   Lng                 1000 non-null   float64\n 3   Area                995 non-null    object \n 4   Children            993 non-null    float64\n 5   Age                 1000 non-null   int64  \n 6   Income              1000 non-null   float64\n 7   Marital             995 non-null    object \n 8   Gender              995 non-null    object \n 9   ReAdmis             1000 non-null   int64  \n 10  VitD_levels         1000 non-null   float64\n 11  Doc_visits          1000 non-null   int64  \n 12  Full_meals_eaten    1000 non-null   int64  \n 13  vitD_supp           1000 non-null   int64  \n 14  Soft_drink          1000 non-null   int64  \n 15  Initial_admin       995 non-null    object \n 16  HighBlood           1000 non-null   int64  \n 17  Stroke              1000 non-null   int64  \n 18  Complication_risk   995 non-null    object \n 19  Overweight          1000 non-null   int64  \n 20  Arthritis           994 non-null    float64\n 21  Diabetes            994 non-null    float64\n 22  Hyperlipidemia      998 non-null    float64\n 23  BackPain            992 non-null    float64\n 24  Anxiety             998 non-null    float64\n 25  Allergic_rhinitis   994 non-null    float64\n 26  Reflux_esophagitis  1000 non-null   int64  \n 27  Asthma              1000 non-null   int64  \n 28  Services            995 non-null    object \n 29  Initial_days        1000 non-null   float64\n 30  TotalCharge         1000 non-null   float64\n 31  Additional_charges  1000 non-null   float64\ndtypes: float64(14), int64(11), object(7)\nmemory usage: 250.1+ KB\n</pre> In\u00a0[15]: Copied! <pre>print(df.isna().sum().sum(), 'missing values')\n</pre> print(df.isna().sum().sum(), 'missing values') <pre>72 missing values\n</pre> <p>Hay en total 72 valores que faltan repartidos en 14 columnas diferentes.</p> <p>Algunas columnas con datos faltantes son num\u00e9ricas y otras son categ\u00f3ricas (objeto). Podemos usar <code>mean</code>, <code>median</code>, <code>mode</code>, <code>most_frequent</code> o estrategias de imputaci\u00f3n (imputer) constante para los datos num\u00e9ricos, pero solo las estrategias constante o <code>most_frequent</code> para los datos categ\u00f3ricos</p> <p>Nota: Si este fuera un proyecto real, podr\u00edamos investigar m\u00e1s para ver si deber\u00edamos eliminar filas o columnas con datos faltantes, o imputar los datos faltantes.</p> <pre>df[df.isna().any(axis=1)].shape\n    &gt; (70, 32)\n</pre> <p>Faltan 70 filas de 100 en al menos un valor.  Eso es el 0,7 % de nuestros datos.  En un proyecto real podr\u00edamos solo eliminar las filas con valores faltantes con <code>df.dropna()</code>.</p> <p>La imputaci\u00f3n de los valores faltantes puede filtrar informaci\u00f3n de los datos de prueba a los datos de entrenamiento, as\u00ed que imputamos los valores DESPU\u00c9S de dividir los datos</p> In\u00a0[16]: Copied! <pre># train/test split\n\nX = df.drop(columns=['Additional_charges'])\ny = df['Additional_charges']\n# Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n</pre> # train/test split  X = df.drop(columns=['Additional_charges']) y = df['Additional_charges'] # Train Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) <p>Vamos a separar nuestras caracter\u00edsticas en dos tipos de columnas bas\u00e1ndonos en el tipo de datos.  Una ser\u00e1 nuestras columnas que incluir\u00e1n los n\u00fameros enteros y flotantes.  La otra columna ser\u00e1 las columnas categ\u00f3ricas que incluyen nuestras cadenas (objetos).</p> In\u00a0[17]: Copied! <pre># instancien los selectores a nuestros tipos de datos num\u00e9ricos y categ\u00f3ricos\nnum_selector = make_column_selector(dtype_include='number')\ncat_selector = make_column_selector(dtype_include='object')\n# seleccionen las columnas num\u00e9ricas de cada tipo\nnum_columns = num_selector(X_train)\ncat_columns = cat_selector(X_train)\n</pre> # instancien los selectores a nuestros tipos de datos num\u00e9ricos y categ\u00f3ricos num_selector = make_column_selector(dtype_include='number') cat_selector = make_column_selector(dtype_include='object') # seleccionen las columnas num\u00e9ricas de cada tipo num_columns = num_selector(X_train) cat_columns = cat_selector(X_train) In\u00a0[18]: Copied! <pre># comprueben las listas\nprint('numeric columns are:\\n', num_columns)\n</pre> # comprueben las listas print('numeric columns are:\\n', num_columns) <pre>numeric columns are:\n ['Lat', 'Lng', 'Children', 'Age', 'Income', 'ReAdmis', 'VitD_levels', 'Doc_visits', 'Full_meals_eaten', 'vitD_supp', 'Soft_drink', 'HighBlood', 'Stroke', 'Overweight', 'Arthritis', 'Diabetes', 'Hyperlipidemia', 'BackPain', 'Anxiety', 'Allergic_rhinitis', 'Reflux_esophagitis', 'Asthma', 'Initial_days', 'TotalCharge']\n</pre> In\u00a0[19]: Copied! <pre># comprueben las listas\nprint('categorical columns are:\\n', cat_columns)\n</pre> # comprueben las listas print('categorical columns are:\\n', cat_columns) <pre>categorical columns are:\n ['State', 'Area', 'Marital', 'Gender', 'Initial_admin', 'Complication_risk', 'Services']\n</pre> <p>Antes que decidamos cu\u00e1l estrategia utilizar para la imputaci\u00f3n, necesitamos comprender nuestros datos. El c\u00f3digo de abajo aislar\u00e1n las columnas num\u00e9ricas a las que les faltan datos.  Podemos hacer esto para ver qu\u00e9 estrategia de imputaci\u00f3n deber\u00edamos usar.</p> In\u00a0[20]: Copied! <pre># a\u00edslen las columnas num\u00e9ricas\ndf_num = df[num_columns]\n# a\u00edslen las columnas con datos faltantes\ndf_num.loc[:, df_num.isna().any()]\n</pre> # a\u00edslen las columnas num\u00e9ricas df_num = df[num_columns] # a\u00edslen las columnas con datos faltantes df_num.loc[:, df_num.isna().any()] Out[20]: Children Arthritis Diabetes Hyperlipidemia BackPain Anxiety Allergic_rhinitis 0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1 3.0 0.0 0.0 0.0 0.0 0.0 0.0 2 3.0 0.0 1.0 0.0 0.0 0.0 0.0 3 0.0 1.0 0.0 0.0 0.0 0.0 0.0 4 1.0 0.0 NaN 1.0 0.0 0.0 1.0 ... ... ... ... ... ... ... ... 995 3.0 0.0 1.0 1.0 0.0 0.0 0.0 996 2.0 1.0 0.0 NaN 1.0 1.0 1.0 997 0.0 1.0 0.0 1.0 1.0 0.0 0.0 998 0.0 1.0 0.0 0.0 0.0 0.0 0.0 999 1.0 0.0 0.0 0.0 1.0 0.0 0.0 <p>1000 rows \u00d7 7 columns</p> <p>Todas las columnas num\u00e9ricas parecen tener valores enteros (aunque no son tipos de datos enteros). Si utiliz\u00e1ramos una estrategia de \"mean\", se rellenar\u00edan con valores decimales (flotantes). Para rellenarlos con valores enteros necesitamos usar estrategias de \u201cmedian\u201d.</p> <p>En primer lugar, vamos a comprobar a qu\u00e9 columnas le faltan datos.</p> In\u00a0[21]: Copied! <pre>X_train.isna().any()\n</pre> X_train.isna().any() Out[21]: <pre>State                  True\nLat                   False\nLng                   False\nArea                   True\nChildren               True\nAge                   False\nIncome                False\nMarital                True\nGender                 True\nReAdmis               False\nVitD_levels           False\nDoc_visits            False\nFull_meals_eaten      False\nvitD_supp             False\nSoft_drink            False\nInitial_admin          True\nHighBlood             False\nStroke                False\nComplication_risk      True\nOverweight            False\nArthritis              True\nDiabetes               True\nHyperlipidemia         True\nBackPain               True\nAnxiety                True\nAllergic_rhinitis      True\nReflux_esophagitis    False\nAsthma                False\nServices               True\nInitial_days          False\nTotalCharge           False\ndtype: bool</pre> <p>El siguiente c\u00f3digo muestra c\u00f3mo aplicar el Simple Imputer a las columnas que fueron previamente seleccionadas como y definidas como num_columns.  \u00a1Tengan en cuenta que el paso con .fit solo se aplica al conjunto de entrenamiento!</p> In\u00a0[22]: Copied! <pre># Instancien el objeto imputer de la clase SimpleImputer con la estrategia 'median'\nmedian_imputer = SimpleImputer(strategy='median')\n# Encajen el objeto imputer en los datos de entrenamiento num\u00e9rico con .fit()\u00a0\n# calculen las medianas (medians) de las columnas en el conjunto de entrenamiento\nmedian_imputer.fit(X_train[num_columns])\n#Utilicen la mediana a partir de los datos de entrenamiento para rellenar los valores que en falten\n#las columnas numericas de los conjuntos de entrenamiento y de prueba con  .transform()\nX_train.loc[:, num_columns] = median_imputer.transform(X_train[num_columns])\nX_test.loc[:, num_columns] = median_imputer.transform(X_test[num_columns])\n</pre> # Instancien el objeto imputer de la clase SimpleImputer con la estrategia 'median' median_imputer = SimpleImputer(strategy='median') # Encajen el objeto imputer en los datos de entrenamiento num\u00e9rico con .fit()\u00a0 # calculen las medianas (medians) de las columnas en el conjunto de entrenamiento median_imputer.fit(X_train[num_columns]) #Utilicen la mediana a partir de los datos de entrenamiento para rellenar los valores que en falten #las columnas numericas de los conjuntos de entrenamiento y de prueba con  .transform() X_train.loc[:, num_columns] = median_imputer.transform(X_train[num_columns]) X_test.loc[:, num_columns] = median_imputer.transform(X_test[num_columns]) <p>\u00bfRellen\u00f3 SimpleImputer los valores faltantes en X_train?</p> In\u00a0[23]: Copied! <pre>X_train.isna().any()\n</pre> X_train.isna().any() Out[23]: <pre>State                  True\nLat                   False\nLng                   False\nArea                   True\nChildren              False\nAge                   False\nIncome                False\nMarital                True\nGender                 True\nReAdmis               False\nVitD_levels           False\nDoc_visits            False\nFull_meals_eaten      False\nvitD_supp             False\nSoft_drink            False\nInitial_admin          True\nHighBlood             False\nStroke                False\nComplication_risk      True\nOverweight            False\nArthritis             False\nDiabetes              False\nHyperlipidemia        False\nBackPain              False\nAnxiety               False\nAllergic_rhinitis     False\nReflux_esophagitis    False\nAsthma                False\nServices               True\nInitial_days          False\nTotalCharge           False\ndtype: bool</pre> <p>Las columnas num\u00e9ricas no tienen valores faltantes, pero las categ\u00f3ricas las siguen teniendo.</p> <p>Recreemos nuestro X_train original con todos los fatos faltantes y veamos c\u00f3mo <code>ColumnTransformer</code>, combinado con <code>SimpleImputer</code>, puede imputar las columnas num\u00e9ricas con medianas, y las columnas categ\u00f3ricas y el valor m\u00e1s frecuente.</p> In\u00a0[24]: Copied! <pre># Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nX_train.isna().any()\n</pre> # Train Test Split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) X_train.isna().any() Out[24]: <pre>State                  True\nLat                   False\nLng                   False\nArea                   True\nChildren               True\nAge                   False\nIncome                False\nMarital                True\nGender                 True\nReAdmis               False\nVitD_levels           False\nDoc_visits            False\nFull_meals_eaten      False\nvitD_supp             False\nSoft_drink            False\nInitial_admin          True\nHighBlood             False\nStroke                False\nComplication_risk      True\nOverweight            False\nArthritis              True\nDiabetes               True\nHyperlipidemia         True\nBackPain               True\nAnxiety                True\nAllergic_rhinitis      True\nReflux_esophagitis    False\nAsthma                False\nServices               True\nInitial_days          False\nTotalCharge           False\ndtype: bool</pre> <p>Ambas columnas categ\u00f3ricas y num\u00e9ricas tienen valores faltantes.</p> In\u00a0[25]: Copied! <pre># instancien los selectores a nuestros tipos de datos num\u00e9ricos y categ\u00f3ricos\nnum_selector = make_column_selector(dtype_include='number')\ncat_selector = make_column_selector(dtype_include='object')\n</pre> # instancien los selectores a nuestros tipos de datos num\u00e9ricos y categ\u00f3ricos num_selector = make_column_selector(dtype_include='number') cat_selector = make_column_selector(dtype_include='object') <p>Rellenaremos los datos faltantes en columnas num\u00e9ricas con \u201cmedian\u201d de cada columna y datos faltantes en columnas categ\u00f3ricas con el valor m\u00e1s frecuente.  Estas no son las \u00fanicas opciones, pero es lo que haremos hoy.</p> In\u00a0[26]: Copied! <pre># Instanciar SimpleImputers con estrategias most_frequent y median\nfreq_imputer = SimpleImputer(strategy='most_frequent')\nmedian_imputer = SimpleImputer(strategy='median')\n</pre> # Instanciar SimpleImputers con estrategias most_frequent y median freq_imputer = SimpleImputer(strategy='most_frequent') median_imputer = SimpleImputer(strategy='median') <p>Como podr\u00e1n recordar de las otras clases, <code>make_column_transformer()</code> toma las tuplas de la forma (transformador, columnas).  <code>ColumnSelectors</code> se puede usar en vez de las listas de columnas.  Los dos son aceptables.  Podemos establecer <code>remainder='passthrough'</code> si no aplicamos los transformadores a todas las columnas.  Podr\u00edamos hacerlo si ya hubi\u00e9ramos imputado algunas columnas a mano.</p> <p>El valor por defecto para ColumnTransformer es eliminar cualquier columna que no especificada en una tupla, mientras que <code>remainder = 'passthrough'</code> retiene los valores originales para alguna columna no especificada en una tupla sin ninguna transformaci\u00f3n.</p> In\u00a0[27]: Copied! <pre># creen tuplas de (imputer, selector) para cada tipo de dato\nnum_tuple = (median_imputer, num_selector)\ncat_tuple = (freq_imputer, cat_selector)\n# instanciaci\u00f3n de ColumnTransformer\ncol_transformer = make_column_transformer(num_tuple, cat_tuple, remainder='passthrough')\ncol_transformer\n</pre> # creen tuplas de (imputer, selector) para cada tipo de dato num_tuple = (median_imputer, num_selector) cat_tuple = (freq_imputer, cat_selector) # instanciaci\u00f3n de ColumnTransformer col_transformer = make_column_transformer(num_tuple, cat_tuple, remainder='passthrough') col_transformer Out[27]: <pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('simpleimputer-1',\n                                 SimpleImputer(strategy='median'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F168EAB3D0&gt;),\n                                ('simpleimputer-2',\n                                 SimpleImputer(strategy='most_frequent'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F168EABEB0&gt;)])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer<pre>ColumnTransformer(remainder='passthrough',\n                  transformers=[('simpleimputer-1',\n                                 SimpleImputer(strategy='median'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F168EAB3D0&gt;),\n                                ('simpleimputer-2',\n                                 SimpleImputer(strategy='most_frequent'),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F168EABEB0&gt;)])</pre>simpleimputer-1<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F168EAB3D0&gt;</pre>SimpleImputer<pre>SimpleImputer(strategy='median')</pre>simpleimputer-2<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001F168EABEB0&gt;</pre>SimpleImputer<pre>SimpleImputer(strategy='most_frequent')</pre>remainder<pre></pre>passthrough<pre>passthrough</pre> In\u00a0[28]: Copied! <pre># ajustar ColumnTransformer en los datos de entrenamiento\ncol_transformer.fit(X_train)\n# transformen los datos de entrenamiento y de prueba (esto generar\u00e1 un array de NumPy)\nX_train_imputed = col_transformer.transform(X_train)\nX_test_imputed = col_transformer.transform(X_test)\n# cambien el resultado regreso a un DataFrame\nX_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns)\nX_train_imputed.isna().any()\n</pre> # ajustar ColumnTransformer en los datos de entrenamiento col_transformer.fit(X_train) # transformen los datos de entrenamiento y de prueba (esto generar\u00e1 un array de NumPy) X_train_imputed = col_transformer.transform(X_train) X_test_imputed = col_transformer.transform(X_test) # cambien el resultado regreso a un DataFrame X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns) X_train_imputed.isna().any() Out[28]: <pre>State                 False\nLat                   False\nLng                   False\nArea                  False\nChildren              False\nAge                   False\nIncome                False\nMarital               False\nGender                False\nReAdmis               False\nVitD_levels           False\nDoc_visits            False\nFull_meals_eaten      False\nvitD_supp             False\nSoft_drink            False\nInitial_admin         False\nHighBlood             False\nStroke                False\nComplication_risk     False\nOverweight            False\nArthritis             False\nDiabetes              False\nHyperlipidemia        False\nBackPain              False\nAnxiety               False\nAllergic_rhinitis     False\nReflux_esophagitis    False\nAsthma                False\nServices              False\nInitial_days          False\nTotalCharge           False\ndtype: bool</pre>"},{"location":"introduction/0132_transform/#transformador-de-columna","title":"Transformador de columna\u00b6","text":""},{"location":"introduction/0132_transform/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Pueden ver que el transformador de columna funciona en paralelo. Escala los datos num\u00e9ricos, realiza una codificaci\u00f3n one-hot en los datos categ\u00f3ricos y todo lo dem\u00e1s que no est\u00e1 en esas categor\u00edas se pasa al conjunto de datos inalterado final. Estos pasos ocurren al mismo tiempo.</p> <p>Tambi\u00e9n observen que las columnas categ\u00f3ricas son ahora m\u00e1s largas de lo que eran antes del transformador de columna. Esto se debe porque cuando se realiza una codificaci\u00f3n one-hot, hay una columna separada que se cre\u00f3 para cada categor\u00eda en cada columna categ\u00f3rica.</p> <p>Al usar un transformador de columna les permitir\u00e1 aplicar diferentes tipos de transformador a diferentes columnas en los datos.  Es conveniente y permite ver f\u00e1cilmente qu\u00e9 transformaciones se aplicaron a qu\u00e9 columnas durante la fase de preprocesamiento.</p>"},{"location":"introduction/0132_transform/#simpleimputer","title":"SimpleImputer\u00b6","text":"<p>SimpleImputer es una clase en la biblioteca scikit-learn de Python que se utiliza para completar valores faltantes o nulos en un conjunto de datos. Cuando trabajas con conjuntos de datos reales, es com\u00fan encontrar valores faltantes en algunas columnas, lo que puede dificultar el an\u00e1lisis y el modelado de datos.</p> <p>SimpleImputer proporciona una forma sencilla de manejar estos valores faltantes al reemplazarlos por valores predeterminados o estad\u00edsticamente calculados. Puedes especificar c\u00f3mo deseas completar los valores faltantes utilizando diferentes estrategias.</p> <p>Algunas de las estrategias de imputaci\u00f3n que se pueden utilizar con SimpleImputer son:</p> <ul> <li><code>mean</code> (media): reemplaza los valores faltantes con la media de la columna en la que se encuentran.</li> <li><code>median</code> (mediana): reemplaza los valores faltantes con la mediana de la columna.</li> <li><code>most_frequent</code> (m\u00e1s frecuente): reemplaza los valores faltantes con el valor m\u00e1s frecuente en la columna.</li> <li><code>constant</code> (constante): reemplaza los valores faltantes con un valor constante especificado por el usuario.</li> </ul> <p>Estas estrategias permiten imputar los valores faltantes de forma r\u00e1pida y sencilla antes de realizar an\u00e1lisis o entrenar modelos de aprendizaje autom\u00e1tico en scikit-learn.</p>"},{"location":"introduction/0133_pipelines/","title":"Pipelines","text":"<p>Un pipeline contiene m\u00faltiples transformadores (\u00a1o incluso modelos!) y realiza operaciones en datos EN SECUENCIA.  Comparen esto en ColumnTransformers que realiza operaciones en los datos EN PARALELO.</p> <p>Cuando un pipeline se ajusta a los datos, se ajustan todos los transformadores dentro de ella.  Cuando los datos se transforman usando un pipeline, los datos son transformados por el primer transformador primero, el segundo transformador segundo, etc.  Un pipeline pueden contener cualquier n\u00famero de transformadores siempre y cuando tengan los m\u00e9todos <code>.fit()</code> y <code>.transform()</code>.  Esto se llaman <code>steps</code> (pasos).</p> <p>Si lo necesitan, un solo estimador o modelo se puede colocar al final de un pipeline.  Aprender\u00e1n m\u00e1s sobre esto despu\u00e9s.</p> <p>azones para utilizar pipelines:</p> <ol> <li><p>Los pipelines usan menos c\u00f3digos que hacer cada transformador individualmente.  Debido a que cada transformador se ajusta en una sola llamada <code>.fit()</code>, y los datos son transformados por todos los transformadores en el pipeline en una sola llamada <code>.transform()</code>, los pipelines usan muchos menos c\u00f3digos.</p> </li> <li><p>Los pipelines hacen que el procesamiento del flujo de trabajo sea m\u00e1s f\u00e1ciles de entender.  Al reducir el c\u00f3digo y mostrar el diagrama del pipeline, les pueden mostrar a sus lectores claramente c\u00f3mo sus datos se transforman antes de modelarlos.</p> </li> <li><p>Los pipelines son f\u00e1ciles de usar en la producci\u00f3n de modelos.  Cuando est\u00e9n listos para despleguar el modelo para usar los nuevos datos, un pipeline de preprocesamiento puede garantizar que los nuevos datos puedan ser r\u00e1pidas y f\u00e1cilmente preprocesados para el modelado.</p> </li> <li><p>Los pipelines pueden evitar una fuga de datos.  Los pipelines est\u00e1n dise\u00f1ados para ajustarse \u00fanicamente a los datos de entrenamiento.  Despu\u00e9s aprender\u00e1n una t\u00e9cnica llamada \u201ccross-validation\u201d, y los pipelines simplificar\u00e1n la realizaci\u00f3n de los mismos sin que se filtren los datos.</p> </li> </ol> <p>Veamos un ejemplo con Python:</p> In\u00a0[1]: Copied! <pre># Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import set_config\nset_config(display='diagram')\n</pre> # Imports import pandas as pd import numpy as np from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn import set_config set_config(display='diagram') In\u00a0[2]: Copied! <pre># leer datos\n#load the data\ndf = pd.read_csv(\"data/life_expectancy.csv\", index_col='CountryYear')\ndf.head()\n</pre> # leer datos #load the data df = pd.read_csv(\"data/life_expectancy.csv\", index_col='CountryYear') df.head() Out[2]: Status Life expectancy Adult Mortality infant deaths Alcohol percentage expenditure Hepatitis B Measles BMI under-five deaths Polio Total expenditure Diphtheria HIV/AIDS GDP Population thinness  1-19 years thinness 5-9 years Income composition of resources Schooling CountryYear Afghanistan2015 0 65.0 263 62 0.01 71.279624 65.0 1154 19.1 83 6.0 8.16 65.0 0.1 584.259210 33736494.0 17.2 17.3 0.479 10.1 Afghanistan2014 0 59.9 271 64 0.01 73.523582 62.0 492 18.6 86 58.0 8.18 62.0 0.1 612.696514 327582.0 17.5 17.5 0.476 10.0 Afghanistan2013 0 59.9 268 66 0.01 73.219243 64.0 430 18.1 89 62.0 8.13 64.0 0.1 631.744976 31731688.0 17.7 17.7 0.470 9.9 Afghanistan2012 0 59.5 272 69 0.01 78.184215 67.0 2787 17.6 93 67.0 8.52 67.0 0.1 669.959000 3696958.0 17.9 18.0 0.463 9.8 Afghanistan2011 0 59.2 275 71 0.01 7.097109 68.0 3013 17.2 97 68.0 7.87 68.0 0.1 63.537231 2978599.0 18.2 18.2 0.454 9.5 <p>Podemos ver que los valores en las columnas est\u00e1n en diferentes escalas.  Muchos tipos de modelo asumen que los datos se escalan antes de ajustar el modelo, por lo que en este caso querremos escalar los datos antes de modelar.</p> In\u00a0[3]: Copied! <pre>#inspect the data\nprint(df.info(), '\\n')\n</pre> #inspect the data print(df.info(), '\\n') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 2928 entries, Afghanistan2015 to Zimbabwe2000\nData columns (total 20 columns):\n #   Column                           Non-Null Count  Dtype  \n---  ------                           --------------  -----  \n 0   Status                           2928 non-null   int64  \n 1   Life expectancy                  2928 non-null   float64\n 2   Adult Mortality                  2928 non-null   int64  \n 3   infant deaths                    2928 non-null   int64  \n 4   Alcohol                          2735 non-null   float64\n 5   percentage expenditure           2928 non-null   float64\n 6   Hepatitis B                      2375 non-null   float64\n 7   Measles                          2928 non-null   int64  \n 8   BMI                              2896 non-null   float64\n 9   under-five deaths                2928 non-null   int64  \n 10  Polio                            2909 non-null   float64\n 11  Total expenditure                2702 non-null   float64\n 12  Diphtheria                       2909 non-null   float64\n 13  HIV/AIDS                         2928 non-null   float64\n 14  GDP                              2485 non-null   float64\n 15  Population                       2284 non-null   float64\n 16  thinness  1-19 years             2896 non-null   float64\n 17  thinness 5-9 years               2896 non-null   float64\n 18  Income composition of resources  2768 non-null   float64\n 19  Schooling                        2768 non-null   float64\ndtypes: float64(15), int64(5)\nmemory usage: 480.4+ KB\nNone \n\n</pre> In\u00a0[4]: Copied! <pre>print(df.isna().sum())\n</pre> print(df.isna().sum()) <pre>Status                               0\nLife expectancy                      0\nAdult Mortality                      0\ninfant deaths                        0\nAlcohol                            193\npercentage expenditure               0\nHepatitis B                        553\nMeasles                              0\nBMI                                 32\nunder-five deaths                    0\nPolio                               19\nTotal expenditure                  226\nDiphtheria                          19\nHIV/AIDS                             0\nGDP                                443\nPopulation                         644\nthinness  1-19 years                32\nthinness 5-9 years                  32\nIncome composition of resources    160\nSchooling                          160\ndtype: int64\n</pre> <p>Podemos ver que diversas columnas le faltan datos.  Se quiere imputar los datos faltantes antes que escalemos los datos, por lo que el pipeline se ordenar\u00e1 como:</p> <ul> <li>Paso 1. Imputar</li> <li>Paso 2. Escalar</li> </ul> <p>Todos nuestros datos son num\u00e9ricos, as\u00ed que no necesitamos realizar una codificaci\u00f3n one-hot a los datos.  Tambi\u00e9n podemos usar una imputaci\u00f3n de mediana o de media en todas las columnas.</p> <p>Si quisi\u00e9ramos, PODR\u00cdAMOS usar <code>ColumnTransformer</code> para dividir las columnas por n\u00fameros enteros y flotantes y aplicar la imputaci\u00f3n de la media a los flotantes, y la imputaci\u00f3n de la mediana a los enteros, y luego escalarlos a todos.</p> <p>Vamos a predecir la \"'Life expectancy\" (esperanza de vida), por lo que la fijaremos como objetivo.</p> In\u00a0[5]: Copied! <pre># dividan la caracter\u00edstica y el objetivo y realicen un train/test split.\nX = df.drop(columns=['Life expectancy'])\ny = df['Life expectancy']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n</pre> # dividan la caracter\u00edstica y el objetivo y realicen un train/test split. X = df.drop(columns=['Life expectancy']) y = df['Life expectancy'] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42) In\u00a0[6]: Copied! <pre># instancien un imputer y un scaler\nmedian_imputer = SimpleImputer(strategy='median')\nscaler = StandardScaler()\n</pre> # instancien un imputer y un scaler median_imputer = SimpleImputer(strategy='median') scaler = StandardScaler() In\u00a0[7]: Copied! <pre># combinen el imputer y scale en un pipeline\npreprocessing_pipeline = make_pipeline(median_imputer, scaler)\npreprocessing_pipeline\n</pre> # combinen el imputer y scale en un pipeline preprocessing_pipeline = make_pipeline(median_imputer, scaler) preprocessing_pipeline Out[7]: <pre>Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])</pre>SimpleImputer<pre>SimpleImputer(strategy='median')</pre>StandardScaler<pre>StandardScaler()</pre> <p>Podemos ver en el diagrama anterior que el primer paso en la tuber\u00eda es el imputer y el segundo paso es el scaler.</p> In\u00a0[8]: Copied! <pre># ajustar el pipeline en los datos de entrenamiento\npreprocessing_pipeline.fit(X_train)\n</pre> # ajustar el pipeline en los datos de entrenamiento preprocessing_pipeline.fit(X_train) Out[8]: <pre>Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])</pre>SimpleImputer<pre>SimpleImputer(strategy='median')</pre>StandardScaler<pre>StandardScaler()</pre> In\u00a0[9]: Copied! <pre># transformen los conjuntos de entrenamiento y de prueba\nX_train_processed = preprocessing_pipeline.transform(X_train)\nX_test_processed = preprocessing_pipeline.transform(X_test)\n</pre> # transformen los conjuntos de entrenamiento y de prueba X_train_processed = preprocessing_pipeline.transform(X_train) X_test_processed = preprocessing_pipeline.transform(X_test) <p>Los transformadores scikit-learn y pipelines siempre devuelven arrays de NumPy, no en DataFrames de Pandas. Podemos usar <code>np.isnan(array).sum().sum()</code> (no el m\u00e9todo <code>.isna()</code>) para contar los valores faltantes en el array resultante.  Podemos ver que no hay valores faltantes y que todos los valores parecen estar escalados.</p> In\u00a0[10]: Copied! <pre># inspeccionen el resultado de la transformaci\u00f3n\nprint(np.isnan(X_train_processed).sum().sum(), 'missing values \\n')\n</pre> # inspeccionen el resultado de la transformaci\u00f3n print(np.isnan(X_train_processed).sum().sum(), 'missing values \\n') <pre>0 missing values \n\n</pre> In\u00a0[11]: Copied! <pre>X_train_processed\n</pre> X_train_processed Out[11]: <pre>array([[ 0.        , -0.81229166, -0.26366021, ..., -0.87868801,\n         1.19451878,  1.92222335],\n       [ 0.        ,  1.43809769,  0.15576412, ...,  0.58477555,\n         0.22791761,  0.08271906],\n       [ 0.        ,  2.02690924, -0.18501814, ...,  0.87303352,\n        -0.68443553, -0.80637468],\n       ...,\n       [ 0.        , -1.10266448, -0.11511409, ..., -0.10260885,\n        -0.88170108, -1.17427554],\n       [ 0.        , -0.73163255, -0.24618419, ..., -0.96738278,\n         0.97259504,  0.87983758],\n       [ 0.        ,  1.43003177, -0.20249416, ...,  1.07259673,\n        -3.11080174, -2.24731971]])</pre> <p>Veamos un ejemplo en python!</p> In\u00a0[12]: Copied! <pre># imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import set_config\nset_config(display='diagram')\n</pre> # imports import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.impute import SimpleImputer from sklearn.compose import make_column_transformer, make_column_selector from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split from sklearn import set_config set_config(display='diagram') In\u00a0[13]: Copied! <pre># Import the data\ndf = pd.read_csv(\"data/medical_data.csv\")\ndf.head()\n</pre> # Import the data df = pd.read_csv(\"data/medical_data.csv\") df.head() Out[13]: State Lat Lng Area Children Age Income Marital Gender ReAdmis ... Hyperlipidemia BackPain Anxiety Allergic_rhinitis Reflux_esophagitis Asthma Services Initial_days TotalCharge Additional_charges 0 AL 34.34960 -86.72508 Suburban 1.0 53 86575.93 Divorced Male 0 ... 0.0 1.0 1.0 1.0 0 1 Blood Work 10.585770 3726.702860 17939.403420 1 FL 30.84513 -85.22907 Urban 3.0 51 46805.99 Married Female 0 ... 0.0 0.0 0.0 0.0 1 0 Intravenous 15.129562 4193.190458 17612.998120 2 SD 43.54321 -96.63772 Suburban 3.0 53 14370.14 Widowed Female 0 ... 0.0 0.0 0.0 0.0 0 0 Blood Work 4.772177 2434.234222 17505.192460 3 MN 43.89744 -93.51479 Suburban 0.0 78 39741.49 Married Male 0 ... 0.0 0.0 0.0 0.0 1 1 Blood Work 1.714879 2127.830423 12993.437350 4 VA 37.59894 -76.88958 Rural 1.0 22 1209.56 Widowed Female 0 ... 1.0 0.0 0.0 1.0 0 0 CT Scan 1.254807 2113.073274 3716.525786 <p>5 rows \u00d7 32 columns</p> <p>Al revisar la cabecera de los datos vemos que \"Complication_risk\" es un valor categ\u00f3rico ordinal (Volveremos a hablar de ello despu\u00e9s de seguir explorando los datos.).</p> <p>Comprobaremos los tipos de datos con <code>df.info()</code></p> In\u00a0[14]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 32 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   State               995 non-null    object \n 1   Lat                 1000 non-null   float64\n 2   Lng                 1000 non-null   float64\n 3   Area                995 non-null    object \n 4   Children            993 non-null    float64\n 5   Age                 1000 non-null   int64  \n 6   Income              1000 non-null   float64\n 7   Marital             995 non-null    object \n 8   Gender              995 non-null    object \n 9   ReAdmis             1000 non-null   int64  \n 10  VitD_levels         1000 non-null   float64\n 11  Doc_visits          1000 non-null   int64  \n 12  Full_meals_eaten    1000 non-null   int64  \n 13  vitD_supp           1000 non-null   int64  \n 14  Soft_drink          1000 non-null   int64  \n 15  Initial_admin       995 non-null    object \n 16  HighBlood           1000 non-null   int64  \n 17  Stroke              1000 non-null   int64  \n 18  Complication_risk   995 non-null    object \n 19  Overweight          1000 non-null   int64  \n 20  Arthritis           994 non-null    float64\n 21  Diabetes            994 non-null    float64\n 22  Hyperlipidemia      998 non-null    float64\n 23  BackPain            992 non-null    float64\n 24  Anxiety             998 non-null    float64\n 25  Allergic_rhinitis   994 non-null    float64\n 26  Reflux_esophagitis  1000 non-null   int64  \n 27  Asthma              1000 non-null   int64  \n 28  Services            995 non-null    object \n 29  Initial_days        1000 non-null   float64\n 30  TotalCharge         1000 non-null   float64\n 31  Additional_charges  1000 non-null   float64\ndtypes: float64(14), int64(11), object(7)\nmemory usage: 250.1+ KB\n</pre> <p>Aqu\u00ed veremos una mezcla de tipos de datos con datos faltantes en columnas flotantes y columnas de objetos.  No faltan datos enteros.</p> <p>Podemos codificar datos de forma ordinal sin demasiado riesgo de fuga de datos.  Generalmente son un n\u00famero peque\u00f1o de variables ordinales y es probable que est\u00e9n en datos de entrenamiento y de prueba.  Si ese no es el caso, el transformador sklearn llamado OrdinalEncoder se puede agregar a un pipeline de preprocesamiento.</p> In\u00a0[15]: Copied! <pre>df['Complication_risk'].value_counts()\n</pre> df['Complication_risk'].value_counts() Out[15]: <pre>Medium    459\nHigh      311\nLow       221\nMed         4\nName: Complication_risk, dtype: int64</pre> <p>Podemos ver que hay algunos valores incoherentes (Medium y Med). Podemos corregirlos en el mismo paso que codificamos de forma ordinal esta columna.</p> In\u00a0[16]: Copied! <pre># Codificaci\u00f3n ordinal \"Complication_risk\"\nreplacement_dictionary = {'High':2, 'Medium':1, 'Med':1, 'Low':0}\ndf['Complication_risk'].replace(replacement_dictionary, inplace=True)\ndf['Complication_risk']\n</pre> # Codificaci\u00f3n ordinal \"Complication_risk\" replacement_dictionary = {'High':2, 'Medium':1, 'Med':1, 'Low':0} df['Complication_risk'].replace(replacement_dictionary, inplace=True) df['Complication_risk'] Out[16]: <pre>0      1.0\n1      2.0\n2      1.0\n3      1.0\n4      0.0\n      ... \n995    2.0\n996    2.0\n997    1.0\n998    1.0\n999    0.0\nName: Complication_risk, Length: 1000, dtype: float64</pre> <p>\u201cComplication_risk\u201d es ahora un tipo de dato flotante y con codificaci\u00f3n ordinal.</p> In\u00a0[17]: Copied! <pre># Dividan\nX = df.drop('Additional_charges', axis=1)\ny = df['Additional_charges']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n</pre> # Dividan X = df.drop('Additional_charges', axis=1) y = df['Additional_charges'] X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) <p>Crearemos nuestros selectores de columnas para usarlos con nuestro transformador de columna m\u00e1s tarde.  En su lugar, podemos utilizar listas de columnas, pero un selector de columna lo hace m\u00e1s algor\u00edtmico.  En este caso, el c\u00f3digo seguir\u00e1 funcionando, incluso si las columnas en un DataFrame cambian despu\u00e9s que el pipeline se haya puesto en producci\u00f3n.</p> In\u00a0[18]: Copied! <pre># Selectors\ncat_selector = make_column_selector(dtype_include='object')\nnum_selector = make_column_selector(dtype_include='number')\n</pre> # Selectors cat_selector = make_column_selector(dtype_include='object') num_selector = make_column_selector(dtype_include='number') <p>Usaremos tres diferentes transformadores: SimpleImputer, StandardScaler y OneHotEncoder.  Habr\u00e1 dos diferentes SimpleImputers con diferentes estrategias de imputaci\u00f3n: \u201cmost_frequent\u201d y \u201cmean\u201d</p> In\u00a0[19]: Copied! <pre># Imputers\nfreq_imputer = SimpleImputer(strategy='most_frequent')\nmean_imputer = SimpleImputer(strategy='mean')\n# Scaler\nscaler = StandardScaler()\n# One-hot encoder\nohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n</pre> # Imputers freq_imputer = SimpleImputer(strategy='most_frequent') mean_imputer = SimpleImputer(strategy='mean') # Scaler scaler = StandardScaler() # One-hot encoder ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False) <p>Usaremos DOS diferentes pipelines.  Uno para los datos num\u00e9ricos y otros para los datos nominales categ\u00f3ricos.</p> In\u00a0[20]: Copied! <pre># Numeric pipeline\nnumeric_pipe = make_pipeline(mean_imputer, scaler)\nnumeric_pipe\n</pre> # Numeric pipeline numeric_pipe = make_pipeline(mean_imputer, scaler) numeric_pipe Out[20]: <pre>Pipeline(steps=[('simpleimputer', SimpleImputer()),\n                ('standardscaler', StandardScaler())])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('simpleimputer', SimpleImputer()),\n                ('standardscaler', StandardScaler())])</pre>SimpleImputer<pre>SimpleImputer()</pre>StandardScaler<pre>StandardScaler()</pre> In\u00a0[21]: Copied! <pre># Categorical pipeline\ncategorical_pipe = make_pipeline(freq_imputer, ohe)\ncategorical_pipe\n</pre> # Categorical pipeline categorical_pipe = make_pipeline(freq_imputer, ohe) categorical_pipe Out[21]: <pre>Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='most_frequent')),\n                ('onehotencoder',\n                 OneHotEncoder(handle_unknown='ignore', sparse_output=False))])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline<pre>Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='most_frequent')),\n                ('onehotencoder',\n                 OneHotEncoder(handle_unknown='ignore', sparse_output=False))])</pre>SimpleImputer<pre>SimpleImputer(strategy='most_frequent')</pre>OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore', sparse_output=False)</pre> <p><code>make_column_transformer</code> utiliza tuplas para hacer coincidir los transformadores con los tipos de datos sobre los que deben actuar.  Podemos usar pipelines como esos transformadores, que es lo que haremos a continuaci\u00f3n.</p> In\u00a0[22]: Copied! <pre># Tuples para Column Transformer\nnumber_tuple = (numeric_pipe, num_selector)\ncategory_tuple = (categorical_pipe, cat_selector)\n# ColumnTransformer\npreprocessor = make_column_transformer(number_tuple, category_tuple)\npreprocessor\n</pre> # Tuples para Column Transformer number_tuple = (numeric_pipe, num_selector) category_tuple = (categorical_pipe, cat_selector) # ColumnTransformer preprocessor = make_column_transformer(number_tuple, category_tuple) preprocessor Out[22]: <pre>ColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385130&gt;),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385220&gt;)])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer<pre>ColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385130&gt;),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385220&gt;)])</pre>pipeline-1<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385130&gt;</pre>SimpleImputer<pre>SimpleImputer()</pre>StandardScaler<pre>StandardScaler()</pre>pipeline-2<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385220&gt;</pre>SimpleImputer<pre>SimpleImputer(strategy='most_frequent')</pre>OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore', sparse_output=False)</pre> <p>Ajustaremos el ColumnTransformer, el cual se llamar\u00e1 \u201cpreprocessor\u201d en los datos de entrenamiento.  (Nunca en los datos de prueba)</p> In\u00a0[23]: Copied! <pre># fit on train\npreprocessor.fit(X_train)\n</pre> # fit on train preprocessor.fit(X_train) Out[23]: <pre>ColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385130&gt;),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385220&gt;)])</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformer<pre>ColumnTransformer(transformers=[('pipeline-1',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer()),\n                                                 ('standardscaler',\n                                                  StandardScaler())]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385130&gt;),\n                                ('pipeline-2',\n                                 Pipeline(steps=[('simpleimputer',\n                                                  SimpleImputer(strategy='most_frequent')),\n                                                 ('onehotencoder',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse_output=False))]),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385220&gt;)])</pre>pipeline-1<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385130&gt;</pre>SimpleImputer<pre>SimpleImputer()</pre>StandardScaler<pre>StandardScaler()</pre>pipeline-2<pre>&lt;sklearn.compose._column_transformer.make_column_selector object at 0x000001AB20385220&gt;</pre>SimpleImputer<pre>SimpleImputer(strategy='most_frequent')</pre>OneHotEncoder<pre>OneHotEncoder(handle_unknown='ignore', sparse_output=False)</pre> <p>El m\u00e9todo fit funcion\u00f3 para ajustar todos los 4 transformadores dentro de ColumnTransformer.  Usaremos este ColumnTransformer ajustado para transformar nuestros conjuntos de datos de entrenamiento y de prueba.</p> In\u00a0[24]: Copied! <pre># transform train and test\nX_train_processed = preprocessor.transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n</pre> # transform train and test X_train_processed = preprocessor.transform(X_train) X_test_processed = preprocessor.transform(X_test) <p>Todos los transformadores Scikit-Learn devuelven arrays de NumPy, NO DataFrames de Pandas.  Debido a esto, necesitamos usar funciones de Numpy, como np.isnan(), para inspeccionar nuestros datos.  En algunos casos podemos transformar f\u00e1cilmente nuestros datos devuelta a un DataFrame de Pandas, pero no siempre es f\u00e1cil obtener la columna de nombres devuelta.  El OneHotEncoder cre\u00f3 columnas extras y es complicado recuperar los nombres de columna correctos para todas las columnas.</p> <p>Nos aseguraremos de que sustituyan los datos faltantes. que los datos categ\u00f3ricos realicen una codificaci\u00f3n one-hot y que los datos num\u00e9ricos se escalen.</p> In\u00a0[25]: Copied! <pre># Comprueben los valores faltantes y que los datos se escalen y tengan una codificaci\u00f3n one-hot\nprint(np.isnan(X_train_processed).sum().sum(), 'missing values in training data')\nprint(np.isnan(X_test_processed).sum().sum(), 'missing values in testing data')\n</pre> # Comprueben los valores faltantes y que los datos se escalen y tengan una codificaci\u00f3n one-hot print(np.isnan(X_train_processed).sum().sum(), 'missing values in training data') print(np.isnan(X_test_processed).sum().sum(), 'missing values in testing data') <pre>0 missing values in training data\n0 missing values in testing data\n</pre> In\u00a0[26]: Copied! <pre>print('All data in X_train_processed are', X_train_processed.dtype)\nprint('All data in X_test_processed are', X_test_processed.dtype)\n</pre> print('All data in X_train_processed are', X_train_processed.dtype) print('All data in X_test_processed are', X_test_processed.dtype) <pre>All data in X_train_processed are float64\nAll data in X_test_processed are float64\n</pre> In\u00a0[27]: Copied! <pre>print('shape of data is', X_train_processed.shape)\n</pre> print('shape of data is', X_train_processed.shape) <pre>shape of data is (750, 97)\n</pre> In\u00a0[28]: Copied! <pre>X_train_processed\n</pre> X_train_processed Out[28]: <pre>array([[-0.50820472,  0.28193545, -0.06527826, ...,  0.        ,\n         1.        ,  0.        ],\n       [-0.72064168,  0.25283631,  1.23912135, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.49340318,  0.48282262, -0.50007813, ...,  0.        ,\n         1.        ,  0.        ],\n       ...,\n       [ 0.27295848,  0.63816773, -0.93487801, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.89653885, -1.73729615, -0.93487801, ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.30727477,  1.1082109 , -0.93487801, ...,  0.        ,\n         0.        ,  0.        ]])</pre> <p>Si bien podemos ver todas las columnas aqu\u00ed, observamos que no faltan datos, todos los datos est\u00e1n en tipo float64 y que hay 97 columnas ahora, en lugar del original, 32.  Es justo asumir que las columnas categ\u00f3ricas han realizado una codificaci\u00f3n one-hot.</p>"},{"location":"introduction/0133_pipelines/#pipelines","title":"Pipelines\u00b6","text":""},{"location":"introduction/0133_pipelines/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"introduction/0133_pipelines/#pipelines-y-columntransformers-juntos","title":"Pipelines y ColumnTransformers juntos\u00b6","text":"<p>Los pipelines pueden ir dentro de ColumnTransformer para realizar una transformaci\u00f3n secuencial despu\u00e9s de dividir las columnas.  Y los objetos ColumnTransformer pueden colocarse dentro de los pipelines.  Pueden lograr las transformaciones descritas anteriormente ya sea con un conjunto de ColumnTransformer en un pipeline O dos pipelines dentro de un ColumnTransformer.  Hasta podr\u00edan poner un ColumnTransformer en un pipeline dentro de un ColumnTransformer dentro de un pipeline.</p> <p>Como pueden observar, esto se puede volver un poco complicado, as\u00ed que puede ser \u00fatil diagramar las transformaciones que quieren en los datos.  \u00bfQuieren imputar la mediana de los datos num\u00e9ricos, imputar la media de los datos flotantes, escalar ambos tipos, imputar datos de objetos con los valores m\u00e1s frecuentes y luego realizar una codificaci\u00f3n one-hot?</p> <p></p> <p>El diagrama anterior usa un ColumnTransformer con dos pipelines dentro.  Uno de esos pipelines tambi\u00e9n tiene un ColumnTransformer dentro.  De hecho, cuando estemos listos para usar esto para modelar, pondremos todo este objeto de preprocesamiento dentro de OTRO pipeline con el modelo, al final de \u00e9l.</p>"},{"location":"introduction/013_feature_engineering/","title":"Introducci\u00f3n","text":""},{"location":"introduction/013_feature_engineering/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>Las secciones anteriores describen las ideas fundamentales del aprendizaje autom\u00e1tico, pero todos los ejemplos asumen que tiene datos num\u00e9ricos en un formato ordenado, <code>[n_samples, n_features]</code>. En el mundo real, los datos rara vez vienen en esa forma. Con esto en mente, uno de los pasos m\u00e1s importantes en el uso del aprendizaje autom\u00e1tico en la pr\u00e1ctica es la ingenier\u00eda de caracter\u00edsticas (Feature Engineering): es decir, tomar cualquier informaci\u00f3n que tenga sobre su problema y convertirla en n\u00fameros que pueda usar para construir su matriz de caracter\u00edsticas.</p> <p>En esta secci\u00f3n, cubriremos algunos ejemplos comunes de tareas de Feature Engineering.</p>"},{"location":"introduction/014_model_validation/","title":"Hiperpar\u00e1metros y Validaci\u00f3n del Modelo","text":"In\u00a0[1]: Copied! <pre>from sklearn.datasets import load_iris\niris = load_iris()\nX = iris.data\ny = iris.target\n</pre> from sklearn.datasets import load_iris iris = load_iris() X = iris.data y = iris.target <p>A continuaci\u00f3n, elegimos un modelo e hiperpar\u00e1metros. Aqu\u00ed usaremos un clasificador de k-vecinos m\u00e1s cercanos con <code>n_neighbors=1</code>. Este es un modelo muy simple e intuitivo que dice \"la etiqueta de un punto desconocido es la misma que la etiqueta de su punto de entrenamiento m\u00e1s cercano\":</p> In\u00a0[2]: Copied! <pre>from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=1)\n</pre> from sklearn.neighbors import KNeighborsClassifier model = KNeighborsClassifier(n_neighbors=1) <p>Luego entrenamos el modelo y lo usamos para predecir etiquetas para datos cuyas etiquetas ya conocemos:</p> In\u00a0[3]: Copied! <pre>model.fit(X, y)\ny_model = model.predict(X)\n</pre> model.fit(X, y) y_model = model.predict(X) <p>Finalmente, calculamos la fracci\u00f3n de puntos correctamente etiquetados:</p> In\u00a0[4]: Copied! <pre>from sklearn.metrics import accuracy_score\naccuracy_score(y, y_model)\n</pre> from sklearn.metrics import accuracy_score accuracy_score(y, y_model) Out[4]: <pre>1.0</pre> <p>\u00a1Vemos un puntaje de precisi\u00f3n de 1.0, lo que indica que el 100% de los puntos fueron etiquetados correctamente por nuestro modelo!</p> <p>Pero, \u00bfest\u00e1 esto realmente midiendo la precisi\u00f3n esperada? \u00bfRealmente nos hemos encontrado con un modelo que esperamos sea correcto el 100% de las veces?</p> <p>Como te habr\u00e1s dado cuenta, la respuesta es no. De hecho, este enfoque contiene una falla fundamental: entrena y eval\u00faa el modelo con los mismos datos. Adem\u00e1s, este modelo de vecino m\u00e1s cercano es un estimador basado en instancias que simplemente almacena los datos de entrenamiento y predice etiquetas comparando nuevos datos con estos puntos almacenados: \u00a1excepto en casos artificiales, obtendr\u00e1 el 100% de precisi\u00f3n cada vez!</p> In\u00a0[5]: Copied! <pre>from sklearn.model_selection import train_test_split\n# split the data with 50% in each set\nX1, X2, y1, y2 = train_test_split(X, y, random_state=0,\n                                  train_size=0.5)\n\n# fit the model on one set of data\nmodel.fit(X1, y1)\n\n# evaluate the model on the second set of data\ny2_model = model.predict(X2)\naccuracy_score(y2, y2_model)\n</pre> from sklearn.model_selection import train_test_split # split the data with 50% in each set X1, X2, y1, y2 = train_test_split(X, y, random_state=0,                                   train_size=0.5)  # fit the model on one set of data model.fit(X1, y1)  # evaluate the model on the second set of data y2_model = model.predict(X2) accuracy_score(y2, y2_model) Out[5]: <pre>0.9066666666666666</pre> <p>Vemos aqu\u00ed un resultado m\u00e1s razonable: el clasificador del vecino m\u00e1s cercano tiene una precisi\u00f3n de alrededor del 90 % en este conjunto reservado. El conjunto reservado es similar a los datos desconocidos, porque el modelo no los ha \"visto\" antes.</p> In\u00a0[6]: Copied! <pre>y2_model = model.fit(X1, y1).predict(X2)\ny1_model = model.fit(X2, y2).predict(X1)\naccuracy_score(y1, y1_model), accuracy_score(y2, y2_model)\n</pre> y2_model = model.fit(X1, y1).predict(X2) y1_model = model.fit(X2, y2).predict(X1) accuracy_score(y1, y1_model), accuracy_score(y2, y2_model) Out[6]: <pre>(0.96, 0.9066666666666666)</pre> <p>Lo que sale son dos puntajes de precisi\u00f3n, que podr\u00edamos combinar (por ejemplo, tomando la media) para obtener una mejor medida del rendimiento del modelo global. Esta forma particular de validaci\u00f3n cruzada es una validaci\u00f3n cruzada doble, es decir, una en la que hemos dividido los datos en dos conjuntos y usamos cada uno a su vez como un conjunto de validaci\u00f3n.</p> <p>Podr\u00edamos ampliar esta idea para usar a\u00fan m\u00e1s ensayos y m\u00e1s pliegues en los datos; por ejemplo, la siguiente figura muestra una representaci\u00f3n visual de la validaci\u00f3n cruzada de cinco pliegues.</p> <p>Aqu\u00ed dividimos los datos en cinco grupos y usamos cada uno de ellos para evaluar el ajuste del modelo en las otras cuatro quintas partes de los datos. Esto ser\u00eda bastante tedioso de hacer a mano, pero podemos usar la rutina de conveniencia <code>cross_val_score</code> de Scikit-Learn para hacerlo de manera sucinta:</p> In\u00a0[7]: Copied! <pre>from sklearn.model_selection import cross_val_score\ncross_val_score(model, X, y, cv=5)\n</pre> from sklearn.model_selection import cross_val_score cross_val_score(model, X, y, cv=5) Out[7]: <pre>array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1.        ])</pre> <p>Repetir la validaci\u00f3n en diferentes subconjuntos de datos nos da una idea a\u00fan mejor del rendimiento del algoritmo.</p> <p>Scikit-Learn implementa una serie de esquemas de validaci\u00f3n cruzada que son \u00fatiles en situaciones particulares; estos se implementan a trav\u00e9s de iteradores en el m\u00f3dulo <code>model_selection</code>. Por ejemplo, podr\u00edamos querer ir al caso extremo en el que nuestro n\u00famero de pliegues es igual al n\u00famero de puntos de datos: es decir, entrenamos en todos los puntos menos uno en cada ensayo. Este tipo de validaci\u00f3n cruzada se conoce como validaci\u00f3n cruzada dejar uno fuera y se puede utilizar de la siguiente manera:</p> In\u00a0[8]: Copied! <pre>from sklearn.model_selection import LeaveOneOut\nscores = cross_val_score(model, X, y, cv=LeaveOneOut())\nscores\n</pre> from sklearn.model_selection import LeaveOneOut scores = cross_val_score(model, X, y, cv=LeaveOneOut()) scores Out[8]: <pre>array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])</pre> <p>Debido a que tenemos 150 muestras, la validaci\u00f3n cruzada sin uno arroja puntajes para 150 ensayos, y cada puntaje indica una predicci\u00f3n exitosa (1.0) o fallida (0.0). Tomando la media de estos da una estimaci\u00f3n de la tasa de error:</p> In\u00a0[9]: Copied! <pre>scores.mean()\n</pre> scores.mean() Out[9]: <pre>0.96</pre> In\u00a0[10]: Copied! <pre>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\ndef PolynomialRegression(degree=2, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree),\n                         LinearRegression(**kwargs))\n</pre> from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline  def PolynomialRegression(degree=2, **kwargs):     return make_pipeline(PolynomialFeatures(degree),                          LinearRegression(**kwargs)) <p>Ahora vamos a crear algunos datos a los que ajustaremos nuestro modelo:</p> In\u00a0[11]: Copied! <pre>import numpy as np\n\ndef make_data(N, err=1.0, rseed=1):\n    # randomly sample the data\n    rng = np.random.RandomState(rseed)\n    X = rng.rand(N, 1) ** 2\n    y = 10 - 1. / (X.ravel() + 0.1)\n    if err &gt; 0:\n        y += err * rng.randn(N)\n    return X, y\n\nX, y = make_data(40)\n</pre> import numpy as np  def make_data(N, err=1.0, rseed=1):     # randomly sample the data     rng = np.random.RandomState(rseed)     X = rng.rand(N, 1) ** 2     y = 10 - 1. / (X.ravel() + 0.1)     if err &gt; 0:         y += err * rng.randn(N)     return X, y  X, y = make_data(40) <p>Ahora podemos visualizar nuestros datos, junto con ajustes polin\u00f3micos de varios grados (ver la siguiente figura):</p> In\u00a0[12]: Copied! <pre>%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nX_test = np.linspace(-0.1, 1.1, 500)[:, None]\n\nplt.scatter(X.ravel(), y, color='black')\naxis = plt.axis()\nfor degree in [1, 3, 5]:\n    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n    plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))\nplt.xlim(-0.1, 1.0)\nplt.ylim(-2, 12)\nplt.legend(loc='best');\n</pre> %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns  sns.set_style(\"whitegrid\")  X_test = np.linspace(-0.1, 1.1, 500)[:, None]  plt.scatter(X.ravel(), y, color='black') axis = plt.axis() for degree in [1, 3, 5]:     y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)     plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree)) plt.xlim(-0.1, 1.0) plt.ylim(-2, 12) plt.legend(loc='best'); <p>La perilla que controla la complejidad del modelo en este caso es el grado del polinomio, que puede ser cualquier n\u00famero entero no negativo. Una pregunta \u00fatil para responder es la siguiente: \u00bfqu\u00e9 grado de polinomio proporciona una compensaci\u00f3n adecuada entre el sesgo (ajuste insuficiente) y la varianza (ajuste excesivo)?</p> <p>Podemos avanzar en esto visualizando la curva de validaci\u00f3n para este modelo y datos en particular; esto se puede hacer directamente usando la rutina de conveniencia <code>validation_curve</code> proporcionada por Scikit-Learn. Dado un modelo, datos, nombre de par\u00e1metro y un rango para explorar, esta funci\u00f3n calcular\u00e1 autom\u00e1ticamente tanto el puntaje de entrenamiento como el puntaje de validaci\u00f3n en todo el rango (consulte la siguiente figura):</p> In\u00a0[13]: Copied! <pre>from sklearn.model_selection import validation_curve\ndegree = np.arange(0, 21)\ntrain_score, val_score = validation_curve(\n    PolynomialRegression(), X, y,\n    param_name='polynomialfeatures__degree',\n    param_range=degree, cv=7)\n\nplt.plot(degree, np.median(train_score, 1),\n         color='blue', label='training score')\nplt.plot(degree, np.median(val_score, 1),\n         color='red', label='validation score')\nplt.legend(loc='best')\nplt.ylim(0, 1)\nplt.xlabel('degree')\nplt.ylabel('score');\n</pre> from sklearn.model_selection import validation_curve degree = np.arange(0, 21) train_score, val_score = validation_curve(     PolynomialRegression(), X, y,     param_name='polynomialfeatures__degree',     param_range=degree, cv=7)  plt.plot(degree, np.median(train_score, 1),          color='blue', label='training score') plt.plot(degree, np.median(val_score, 1),          color='red', label='validation score') plt.legend(loc='best') plt.ylim(0, 1) plt.xlabel('degree') plt.ylabel('score'); <p>Esto muestra precisamente el comportamiento cualitativo que esperamos: el puntaje de entrenamiento es en todas partes m\u00e1s alto que el puntaje de validaci\u00f3n, el puntaje de entrenamiento mejora mon\u00f3tonamente con una mayor complejidad del modelo y el puntaje de validaci\u00f3n alcanza un m\u00e1ximo antes de caer cuando el modelo se vuelve demasiado ajustado.</p> <p>A partir de la curva de validaci\u00f3n, podemos determinar que el equilibrio \u00f3ptimo entre sesgo y varianza se encuentra para un polinomio de tercer orden. Podemos calcular y mostrar este ajuste sobre los datos originales de la siguiente manera (ver la siguiente figura):</p> In\u00a0[14]: Copied! <pre>plt.scatter(X.ravel(), y)\nlim = plt.axis()\ny_test = PolynomialRegression(3).fit(X, y).predict(X_test)\nplt.plot(X_test.ravel(), y_test);\nplt.axis(lim);\n</pre> plt.scatter(X.ravel(), y) lim = plt.axis() y_test = PolynomialRegression(3).fit(X, y).predict(X_test) plt.plot(X_test.ravel(), y_test); plt.axis(lim); <p>Tenga en cuenta que encontrar este modelo \u00f3ptimo en realidad no requiri\u00f3 que calcul\u00e1ramos el puntaje de entrenamiento, pero examinar la relaci\u00f3n entre el puntaje de entrenamiento y el puntaje de validaci\u00f3n puede brindarnos informaci\u00f3n \u00fatil sobre el rendimiento del modelo.</p> In\u00a0[15]: Copied! <pre>X2, y2 = make_data(200)\nplt.scatter(X2.ravel(), y2);\n</pre> X2, y2 = make_data(200) plt.scatter(X2.ravel(), y2); <p>Ahora, dupliquemos el c\u00f3digo anterior para trazar la curva de validaci\u00f3n para este conjunto de datos m\u00e1s grande; como referencia, tambi\u00e9n representaremos gr\u00e1ficamente los resultados anteriores (vea la siguiente figura):</p> In\u00a0[16]: Copied! <pre>degree = np.arange(21)\ntrain_score2, val_score2 = validation_curve(\n    PolynomialRegression(), X2, y2,\n    param_name='polynomialfeatures__degree',\n    param_range=degree, cv=7)\n\nplt.plot(degree, np.median(train_score2, 1),\n         color='blue', label='training score')\nplt.plot(degree, np.median(val_score2, 1),\n         color='red', label='validation score')\nplt.plot(degree, np.median(train_score, 1),\n         color='blue', alpha=0.3, linestyle='dashed')\nplt.plot(degree, np.median(val_score, 1),\n         color='red', alpha=0.3, linestyle='dashed')\nplt.legend(loc='lower center')\nplt.ylim(0, 1)\nplt.xlabel('degree')\nplt.ylabel('score');\n</pre> degree = np.arange(21) train_score2, val_score2 = validation_curve(     PolynomialRegression(), X2, y2,     param_name='polynomialfeatures__degree',     param_range=degree, cv=7)  plt.plot(degree, np.median(train_score2, 1),          color='blue', label='training score') plt.plot(degree, np.median(val_score2, 1),          color='red', label='validation score') plt.plot(degree, np.median(train_score, 1),          color='blue', alpha=0.3, linestyle='dashed') plt.plot(degree, np.median(val_score, 1),          color='red', alpha=0.3, linestyle='dashed') plt.legend(loc='lower center') plt.ylim(0, 1) plt.xlabel('degree') plt.ylabel('score'); <p>Las l\u00edneas continuas muestran los nuevos resultados, mientras que las l\u00edneas discontinuas m\u00e1s tenues muestran los resultados del conjunto de datos m\u00e1s peque\u00f1o anterior. Est\u00e1 claro a partir de la curva de validaci\u00f3n que el conjunto de datos m\u00e1s grande puede admitir un modelo mucho m\u00e1s complicado: el pico aqu\u00ed es probablemente de alrededor de un grado 6, pero incluso un modelo de grado 20 no est\u00e1 sobreajustando seriamente los datos: las puntuaciones de validaci\u00f3n y entrenamiento permanecen muy cerca.</p> <p>Entonces, el comportamiento de la curva de validaci\u00f3n no tiene una sino dos entradas importantes: la complejidad del modelo y el n\u00famero de puntos de entrenamiento. Podemos obtener m\u00e1s informaci\u00f3n explorando el comportamiento del modelo en funci\u00f3n de la cantidad de puntos de entrenamiento, lo que podemos hacer usando subconjuntos de datos cada vez m\u00e1s grandes para ajustar nuestro modelo. Una gr\u00e1fica de la puntuaci\u00f3n de entrenamiento/validaci\u00f3n con respecto al tama\u00f1o del conjunto de entrenamiento a veces se conoce como curva de aprendizaje.</p> <p>El comportamiento general que esperar\u00edamos de una curva de aprendizaje es este:</p> <ul> <li>Un modelo de una complejidad dada sobreajustar\u00e1 un peque\u00f1o conjunto de datos: esto significa que el puntaje de entrenamiento ser\u00e1 relativamente alto, mientras que el puntaje de validaci\u00f3n ser\u00e1 relativamente bajo.</li> <li>Un modelo de una complejidad dada no se ajustar\u00e1 a un gran conjunto de datos: esto significa que el puntaje de entrenamiento disminuir\u00e1, pero el puntaje de validaci\u00f3n aumentar\u00e1.</li> <li>Un modelo nunca, excepto por casualidad, otorgar\u00e1 una mejor puntuaci\u00f3n al conjunto de validaci\u00f3n que al conjunto de entrenamiento: esto significa que las curvas deben seguir acerc\u00e1ndose pero nunca cruzarse.</li> </ul> <p>Con estas caracter\u00edsticas en mente, esperar\u00edamos que la curva de aprendizaje se pareciera cualitativamente a la que se muestra en la siguiente figura:</p> <p>La caracter\u00edstica notable de la curva de aprendizaje es la convergencia a un puntaje particular a medida que crece el n\u00famero de muestras de entrenamiento. En particular, una vez que tenga suficientes puntos de que un modelo en particular ha convergido, * \u00a1agregar m\u00e1s datos de entrenamiento no lo ayudar\u00e1! * La \u00fanica forma de aumentar el rendimiento del modelo en este caso es utilizar otro modelo (a menudo m\u00e1s complejo).</p> In\u00a0[17]: Copied! <pre>from sklearn.model_selection import learning_curve\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nfor i, degree in enumerate([2, 9]):\n    N, train_lc, val_lc = learning_curve(\n        PolynomialRegression(degree), X, y, cv=7,\n        train_sizes=np.linspace(0.3, 1, 25))\n\n    ax[i].plot(N, np.mean(train_lc, 1),\n               color='blue', label='training score')\n    ax[i].plot(N, np.mean(val_lc, 1),\n               color='red', label='validation score')\n    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0],\n                 N[-1], color='gray', linestyle='dashed')\n\n    ax[i].set_ylim(0, 1)\n    ax[i].set_xlim(N[0], N[-1])\n    ax[i].set_xlabel('training size')\n    ax[i].set_ylabel('score')\n    ax[i].set_title('degree = {0}'.format(degree), size=14)\n    ax[i].legend(loc='best')\n</pre> from sklearn.model_selection import learning_curve  fig, ax = plt.subplots(1, 2, figsize=(16, 6)) fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)  for i, degree in enumerate([2, 9]):     N, train_lc, val_lc = learning_curve(         PolynomialRegression(degree), X, y, cv=7,         train_sizes=np.linspace(0.3, 1, 25))      ax[i].plot(N, np.mean(train_lc, 1),                color='blue', label='training score')     ax[i].plot(N, np.mean(val_lc, 1),                color='red', label='validation score')     ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0],                  N[-1], color='gray', linestyle='dashed')      ax[i].set_ylim(0, 1)     ax[i].set_xlim(N[0], N[-1])     ax[i].set_xlabel('training size')     ax[i].set_ylabel('score')     ax[i].set_title('degree = {0}'.format(degree), size=14)     ax[i].legend(loc='best') <p>Este es un diagn\u00f3stico valioso, porque nos brinda una descripci\u00f3n visual de c\u00f3mo nuestro modelo responde a cantidades crecientes de datos de entrenamiento. En particular, cuando la curva de aprendizaje ya ha convergido (es decir, cuando las curvas de entrenamiento y validaci\u00f3n ya est\u00e1n cerca una de la otra) \u00a1agregar m\u00e1s datos de entrenamiento no mejorar\u00e1 significativamente el ajuste! Esta situaci\u00f3n se ve en el panel izquierdo, con la curva de aprendizaje para el modelo de grado-2.</p> <p>La \u00fanica forma de aumentar la puntuaci\u00f3n convergente es usar un modelo diferente (generalmente m\u00e1s complicado). Vemos esto en el panel de la derecha: al pasar a un modelo mucho m\u00e1s complicado, aumentamos el puntaje de convergencia (indicado por la l\u00ednea discontinua), pero a expensas de una mayor varianza del modelo (indicada por la diferencia entre los puntajes de entrenamiento y validaci\u00f3n). ). Si tuvi\u00e9ramos que agregar a\u00fan m\u00e1s puntos de datos, la curva de aprendizaje para el modelo m\u00e1s complicado eventualmente converger\u00eda.</p> <p>Trazar una curva de aprendizaje para su elecci\u00f3n particular de modelo y conjunto de datos puede ayudarlo a tomar este tipo de decisi\u00f3n sobre c\u00f3mo avanzar en la mejora de su an\u00e1lisis.</p> In\u00a0[18]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'polynomialfeatures__degree': np.arange(21),\n              'linearregression__fit_intercept': [True, False]}\n\ngrid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)\n</pre> from sklearn.model_selection import GridSearchCV  param_grid = {'polynomialfeatures__degree': np.arange(21),               'linearregression__fit_intercept': [True, False]}  grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7) <p>Tenga en cuenta que, como un estimador normal, esto a\u00fan no se ha aplicado a ning\u00fan dato. Llamar al m\u00e9todo <code>fit</code> ajustar\u00e1 el modelo en cada punto de la cuadr\u00edcula, manteniendo un registro de las puntuaciones a lo largo del camino:</p> In\u00a0[19]: Copied! <pre>grid.fit(X, y);\n</pre> grid.fit(X, y); <p>Ahora que el modelo est\u00e1 ajustado, podemos pedir los mejores par\u00e1metros de la siguiente manera:</p> In\u00a0[20]: Copied! <pre>grid.best_params_\n</pre> grid.best_params_ Out[20]: <pre>{'linearregression__fit_intercept': False, 'polynomialfeatures__degree': 4}</pre> <p>Finalmente, si lo deseamos, podemos usar el mejor modelo y mostrar el ajuste a nuestros datos usando el c\u00f3digo anterior (ver la siguiente figura):</p> In\u00a0[21]: Copied! <pre>model = grid.best_estimator_\n\nplt.scatter(X.ravel(), y)\nlim = plt.axis()\ny_test = model.fit(X, y).predict(X_test)\nplt.plot(X_test.ravel(), y_test);\nplt.axis(lim);\n</pre> model = grid.best_estimator_  plt.scatter(X.ravel(), y) lim = plt.axis() y_test = model.fit(X, y).predict(X_test) plt.plot(X_test.ravel(), y_test); plt.axis(lim);"},{"location":"introduction/014_model_validation/#hiperparametros-y-validacion-del-modelo","title":"Hiperpar\u00e1metros y Validaci\u00f3n del Modelo\u00b6","text":""},{"location":"introduction/014_model_validation/#pensando-en-la-validacion-del-modelo","title":"Pensando en la validaci\u00f3n del modelo\u00b6","text":"<p>En principio, la validaci\u00f3n del modelo es muy simple: despu\u00e9s de elegir un modelo y sus hiperpar\u00e1metros, podemos estimar qu\u00e9 tan efectivo es aplic\u00e1ndolo a algunos de los datos de entrenamiento y comparando las predicciones con los valores conocidos.</p> <p>Esta secci\u00f3n mostrar\u00e1 primero un enfoque ingenuo para la validaci\u00f3n del modelo y por qu\u00e9 falla, antes de explorar el uso de conjuntos reservados y la validaci\u00f3n cruzada para obtener resultados m\u00e1s robustos. evaluaci\u00f3n del modelo.</p>"},{"location":"introduction/014_model_validation/#validacion-del-modelo-de-manera-incorrecta","title":"Validaci\u00f3n del modelo de manera incorrecta\u00b6","text":"<p>Comencemos con el enfoque ingenuo de la validaci\u00f3n utilizando el conjunto de datos de Iris, que vimos en el cap\u00edtulo anterior. Comenzaremos cargando los datos:</p>"},{"location":"introduction/014_model_validation/#validacion-del-modelo-de-la-manera-correcta-holdout-sets","title":"Validaci\u00f3n del modelo de la manera correcta: Holdout Sets\u00b6","text":"<p>Entonces, \u00bfqu\u00e9 puede hacerse? Se puede encontrar una mejor idea del rendimiento de un modelo usando lo que se conoce como un conjunto de exclusi\u00f3n (Holdout Sets): es decir, retenemos un subconjunto de los datos del entrenamiento del modelo y luego usamos este conjunto de exclusi\u00f3n para verificar el rendimiento del modelo. . Esta divisi\u00f3n se puede hacer usando la utilidad <code>train_test_split</code> en Scikit-Learn:</p>"},{"location":"introduction/014_model_validation/#validacion-del-modelo-mediante-validacion-cruzada","title":"Validaci\u00f3n del modelo mediante validaci\u00f3n cruzada\u00b6","text":"<p>Una desventaja de usar un conjunto reservado para la validaci\u00f3n del modelo es que hemos perdido una parte de nuestros datos en el entrenamiento del modelo. En el caso anterior, \u00a1la mitad del conjunto de datos no contribuye al entrenamiento del modelo! Esto no es \u00f3ptimo, especialmente si el conjunto inicial de datos de entrenamiento es peque\u00f1o.</p> <p>Una forma de abordar esto es usar validaci\u00f3n cruzada; es decir, hacer una secuencia de ajustes donde cada subconjunto de los datos se utiliza tanto como conjunto de entrenamiento como conjunto de validaci\u00f3n. Visualmente, podr\u00eda parecerse a la siguiente figura:</p> <p>Aqu\u00ed hacemos dos pruebas de validaci\u00f3n, usando alternativamente cada mitad de los datos como un conjunto reservado. Usando los datos divididos de antes, podr\u00edamos implementarlo as\u00ed:</p>"},{"location":"introduction/014_model_validation/#seleccion-del-mejor-modelo","title":"Selecci\u00f3n del mejor modelo\u00b6","text":"<p>Ahora que hemos explorado los conceptos b\u00e1sicos de validaci\u00f3n y validaci\u00f3n cruzada, profundizaremos un poco m\u00e1s en la selecci\u00f3n de modelos y la selecci\u00f3n de hiperpar\u00e1metros. Estos problemas son algunos de los aspectos m\u00e1s importantes de la pr\u00e1ctica del aprendizaje autom\u00e1tico, pero encuentro que esta informaci\u00f3n a menudo se pasa por alto en los tutoriales introductorios de aprendizaje autom\u00e1tico.</p> <p>De importancia central es la siguiente pregunta: si nuestro estimador tiene un rendimiento inferior, \u00bfc\u00f3mo debemos avanzar? Hay varias respuestas posibles:</p> <ul> <li>Utilizar un modelo m\u00e1s complicado/m\u00e1s flexible.</li> <li>Utilizar un modelo menos complicado/menos flexible.</li> <li>Re\u00fane m\u00e1s muestras de entrenamiento.</li> <li>Re\u00fana m\u00e1s datos para agregar caracter\u00edsticas a cada muestra.</li> </ul> <p>La respuesta a esta pregunta es a menudo contradictoria. En particular, a veces usar un modelo m\u00e1s complicado dar\u00e1 peores resultados, \u00a1y agregar m\u00e1s muestras de entrenamiento puede no mejorar sus resultados! La capacidad de determinar qu\u00e9 pasos mejorar\u00e1n su modelo es lo que separa a los profesionales exitosos del aprendizaje autom\u00e1tico de los que no lo tienen.</p>"},{"location":"introduction/014_model_validation/#la-compensacion-entre-sesgo-y-varianza","title":"La compensaci\u00f3n entre sesgo y varianza\u00b6","text":"<p>Fundamentalmente, encontrar \"el mejor modelo\" se trata de encontrar un punto \u00f3ptimo en el equilibrio entre sesgo y varianza. Considere la siguiente figura, que presenta dos ajustes de regresi\u00f3n al mismo conjunto de datos.</p> <p>Est\u00e1 claro que ninguno de estos modelos se ajusta particularmente bien a los datos, pero fallan de diferentes maneras.</p> <p>El modelo de la izquierda intenta encontrar un ajuste lineal a trav\u00e9s de los datos. Debido a que en este caso una l\u00ednea recta no puede dividir con precisi\u00f3n los datos, el modelo de l\u00ednea recta nunca podr\u00e1 describir bien este conjunto de datos. Se dice que dicho modelo no se adapta a los datos: es decir, no tiene suficiente flexibilidad para dar cuenta adecuadamente de todas las caracter\u00edsticas de los datos. Otra forma de decir esto es que el modelo tiene un alto sesgo.</p> <p>El modelo de la derecha intenta ajustar un polinomio de alto orden a trav\u00e9s de los datos. Aqu\u00ed, el ajuste del modelo tiene suficiente flexibilidad para dar cuenta casi perfectamente de las caracter\u00edsticas finas de los datos, pero aunque describe con mucha precisi\u00f3n los datos de entrenamiento, su forma precisa parece reflejar m\u00e1s las propiedades de ruido particulares de los datos que el intr\u00ednseco. propiedades de cualquier proceso que haya generado esos datos. Se dice que dicho modelo sobreajusta los datos: es decir, tiene tanta flexibilidad que el modelo termina teniendo en cuenta los errores aleatorios, as\u00ed como la distribuci\u00f3n de datos subyacente. Otra forma de decir esto es que el modelo tiene una gran varianza.</p> <p>Para ver esto bajo otra luz, considere lo que sucede si usamos estos dos modelos para predecir los valores y para algunos datos nuevos. En los gr\u00e1ficos de la siguiente figura, los puntos rojos/m\u00e1s claros indican datos que se omiten del conjunto de entrenamiento.</p> <p>El puntaje aqu\u00ed es el puntaje $R^2$, o coeficiente de determinaci\u00f3n, que mide qu\u00e9 tan bien se desempe\u00f1a un modelo en relaci\u00f3n con una media simple de los valores objetivo . $R^2=1$ indica una coincidencia perfecta, $R^2=0$ indica que el modelo no hace nada mejor que simplemente tomar la media de los datos, y los valores negativos significan modelos a\u00fan peores. A partir de las puntuaciones asociadas con estos dos modelos, podemos hacer una observaci\u00f3n que se sostiene de manera m\u00e1s general:</p> <ul> <li>Para modelos de alto sesgo, el rendimiento del modelo en el conjunto de validaci\u00f3n es similar al rendimiento en el conjunto de entrenamiento.</li> <li>Para modelos de alta varianza, el rendimiento del modelo en el conjunto de validaci\u00f3n es mucho peor que el rendimiento en el conjunto de entrenamiento.</li> </ul> <p>Si imaginamos que tenemos alguna capacidad para ajustar la complejidad del modelo, esperar\u00edamos que el puntaje de entrenamiento y el puntaje de validaci\u00f3n se comporten como se ilustra en la siguiente figura:</p> <p>El diagrama que se muestra aqu\u00ed a menudo se denomina curva de validaci\u00f3n, y vemos las siguientes caracter\u00edsticas:</p> <ul> <li>El puntaje de entrenamiento es en todas partes m\u00e1s alto que el puntaje de validaci\u00f3n. Este suele ser el caso: el modelo se ajustar\u00e1 mejor a los datos que ha visto que a los datos que no ha visto.</li> <li>Para una complejidad de modelo muy baja (un modelo de alto sesgo), los datos de entrenamiento no se ajustan bien, lo que significa que el modelo es un predictor deficiente tanto para los datos de entrenamiento como para cualquier dato no visto anteriormente.</li> <li>Para una complejidad de modelo muy alta (un modelo de varianza alta), los datos de entrenamiento est\u00e1n sobreajustados, lo que significa que el modelo predice muy bien los datos de entrenamiento, pero falla para los datos no vistos anteriormente.</li> <li>Para alg\u00fan valor intermedio, la curva de validaci\u00f3n tiene un m\u00e1ximo. Este nivel de complejidad indica un compromiso adecuado entre sesgo y varianza.</li> </ul> <p>Los medios para ajustar la complejidad del modelo var\u00edan de un modelo a otro; cuando analicemos los modelos individuales en profundidad en cap\u00edtulos posteriores, veremos c\u00f3mo cada modelo permite tal ajuste.</p>"},{"location":"introduction/014_model_validation/#curvas-de-validacion-en-scikit-learn","title":"Curvas de validaci\u00f3n en Scikit-Learn\u00b6","text":"<p>Veamos un ejemplo del uso de validaci\u00f3n cruzada para calcular la curva de validaci\u00f3n para una clase de modelos. Aqu\u00ed usaremos un modelo de regresi\u00f3n polinomial: este es un modelo lineal generalizado en el que el grado del polinomio es un par\u00e1metro ajustable. Por ejemplo, un polinomio de grado 1 ajusta una l\u00ednea recta a los datos; para los par\u00e1metros del modelo $a$ y $b$:</p> <p>$$ y = ax + b $$</p> <p>Un polinomio de grado 3 ajusta una curva c\u00fabica a los datos; para los par\u00e1metros del modelo $a, b, c, d$:</p> <p>$$ y = ax^3 + bx^2 + cx + d $$</p> <p>Podemos generalizar esto a cualquier n\u00famero de caracter\u00edsticas polin\u00f3micas. En Scikit-Learn, podemos implementar esto con un clasificador de regresi\u00f3n lineal combinado con el preprocesador polinomial. Usaremos una tuber\u00eda para unir estas operaciones:</p>"},{"location":"introduction/014_model_validation/#curvas-de-aprendizaje","title":"Curvas de aprendizaje\u00b6","text":"<p>Un aspecto importante de la complejidad del modelo es que el modelo \u00f3ptimo generalmente depender\u00e1 del tama\u00f1o de los datos de entrenamiento. Por ejemplo, generemos un nuevo conjunto de datos con cinco veces m\u00e1s puntos (ver la siguiente figura):</p>"},{"location":"introduction/014_model_validation/#curvas-de-aprendizaje-en-scikit-learn","title":"Curvas de aprendizaje en Scikit-Learn\u00b6","text":"<p>Scikit-Learn ofrece una utilidad conveniente para calcular dichas curvas de aprendizaje a partir de sus modelos; aqu\u00ed calcularemos una curva de aprendizaje para nuestro conjunto de datos original con un modelo polinomial de segundo orden y un polinomio de noveno orden (ver la siguiente figura):</p>"},{"location":"introduction/014_model_validation/#validacion-en-la-practica-grid-search","title":"Validaci\u00f3n en la pr\u00e1ctica: Grid Search\u00b6","text":"<p>La discusi\u00f3n anterior pretende brindarle cierta intuici\u00f3n sobre el equilibrio entre el sesgo y la varianza, y su dependencia de la complejidad del modelo y el tama\u00f1o del conjunto de entrenamiento. En la pr\u00e1ctica, los modelos generalmente tienen m\u00e1s de una perilla para girar, lo que significa que los gr\u00e1ficos de validaci\u00f3n y las curvas de aprendizaje cambian de l\u00edneas a superficies multidimensionales. En estos casos, tales visualizaciones son dif\u00edciles y preferir\u00edamos simplemente encontrar el modelo particular que maximiza el puntaje de validaci\u00f3n.</p> <p>Scikit-Learn proporciona algunas herramientas para que este tipo de b\u00fasqueda sea m\u00e1s conveniente: aqu\u00ed consideraremos el uso de la b\u00fasqueda en cuadr\u00edcula (Grid Search) para encontrar el modelo polinomial \u00f3ptimo.</p> <p>Exploraremos una cuadr\u00edcula bidimensional de caracter\u00edsticas del modelo, a saber, el grado del polinomio y la bandera que nos indica si debemos ajustar la intersecci\u00f3n. Esto se puede configurar usando el metaestimador <code>GridSearchCV</code> de Scikit-Learn:</p>"},{"location":"introduction/01_intro/","title":"Introducci\u00f3n","text":""},{"location":"introduction/01_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":"<p>El Machine Learning(aprendizaje autom\u00e1tico) es una rama de la inteligencia artificial que se centra en el desarrollo de algoritmos y modelos matem\u00e1ticos que permiten a una computadora aprender a realizar tareas espec\u00edficas a partir de datos, sin ser programada expl\u00edcitamente para realizar esas tareas.</p> <p>El proceso de machine learning implica proporcionar a la computadora un conjunto de datos de entrenamiento, que se utiliza para \"ense\u00f1ar\" al modelo a reconocer patrones y relaciones en los datos. Una vez que el modelo ha sido entrenado, puede aplicarse a nuevos datos para hacer predicciones o tomar decisiones basadas en lo que ha aprendido del conjunto de entrenamiento.</p> <p>El machine learning se utiliza en una amplia variedad de aplicaciones, desde el an\u00e1lisis de datos y la clasificaci\u00f3n de im\u00e1genes hasta el reconocimiento del habla y la conducci\u00f3n aut\u00f3noma de veh\u00edculos. En resumen, el machine learning es una t\u00e9cnica poderosa que permite a las computadoras aprender y mejorar con la experiencia, sin la necesidad de una programaci\u00f3n expl\u00edcita para cada tarea.</p>"},{"location":"introduction/01_intro/#modelos-matematicos","title":"Modelos Matem\u00e1ticos\u00b6","text":"<p>Un modelo matem\u00e1tico es una representaci\u00f3n matem\u00e1tica de un sistema, proceso o fen\u00f3meno del mundo real. Es una herramienta utilizada para describir y predecir el comportamiento de un sistema o proceso, y se construye mediante la formulaci\u00f3n de ecuaciones matem\u00e1ticas que relacionan las variables que intervienen en el sistema o proceso.</p> <p>Algunas Caracter\u00edsticas:</p> <ul> <li><p>Se utilizan en una amplia variedad de disciplinas, desde la f\u00edsica y la ingenier\u00eda hasta la econom\u00eda y la biolog\u00eda. Pueden ser simples o complejos, dependiendo de la complejidad del sistema que se est\u00e9 modelando, y se pueden expresar en diferentes formas, como ecuaciones diferenciales, funciones, gr\u00e1ficos, diagramas, entre otras.</p> </li> <li><p>La utilidad depende de su capacidad para describir y predecir el comportamiento del sistema o proceso que se est\u00e1 estudiando. Por lo tanto, es importante validar y verificar el modelo mediante pruebas y observaciones experimentales, para asegurarse de que refleje adecuadamente la realidad.</p> </li> </ul> <p>Por otro lado, un modelo no es:</p> <ul> <li>Igual al mundo real.</li> <li>Un sustituto para mediciones o experimentos.</li> </ul>"},{"location":"introduction/01_intro/#tipos-de-problemas","title":"Tipos de Problemas\u00b6","text":"<p>El machine learning es una t\u00e9cnica que puede abordar una amplia variedad de problemas en diferentes campos y aplicaciones. Algunos de los tipos de problemas que se pueden abordar con machine learning incluyen:</p> <ul> <li>Aprendizaje supervisado</li> <li>Aprendizaje no supervisado</li> <li>Aprendizaje por refuerzo</li> </ul>"},{"location":"introduction/01_intro/#aprendizaje-supervisado","title":"Aprendizaje supervisado\u00b6","text":"<ul> <li>El sistema aprende en base a datos estructurados o no estructurados.</li> <li>Clasificados previamente (se conoce la respuesta).</li> <li>El algoritmo produce una funci\u00f3n que establece una correspondencia entre las entradas y las salidas deseadas del sistema.</li> </ul>"},{"location":"introduction/01_intro/#aprendizaje-no-supervisado","title":"Aprendizaje no supervisado\u00b6","text":"<ul> <li>Modelo se construye usando un conjunto de datos como entrada, los cuales no han sido clasificados previamente.</li> <li>El sistema tiene que ser capaz de reconocer patrones para poder etiquetar las nuevas entradas.</li> </ul>"},{"location":"introduction/01_intro/#aprendizaje-por-refuerzo","title":"Aprendizaje por refuerzo\u00b6","text":"<p>Aprendizaje por refuerzo o Aprendizaje reforzado es un \u00e1rea del aprendizaje autom\u00e1tico inspirada en la psicolog\u00eda conductista, cuya ocupaci\u00f3n es determinar qu\u00e9 acciones debe escoger un agente de software en un entorno dado con el fin de maximizar alguna noci\u00f3n de \"recompensa\" o premio acumulado.</p> <p></p>"},{"location":"introduction/01_intro/#algoritmos-mas-utilizados","title":"Algoritmos m\u00e1s utilizados\u00b6","text":"<p>Los algoritmos que m\u00e1s se suelen utilizar en los problemas de Machine Learning son los siguientes:</p> <ol> <li>Regresi\u00f3n Lineal</li> <li>Regresi\u00f3n Log\u00edstica</li> <li>Arboles de Decision</li> <li>Random Forest</li> <li>SVM</li> <li>KNN</li> <li>K-means</li> </ol> <p>\u00bf Qu\u00e9 se necesita para aprender machine learning?</p> <p>Se necesita tener conocimientos de los siguientes t\u00f3picos.</p> <ul> <li>Algebra Lineal</li> <li>Probabilidad y estad\u00edstica</li> <li>Optimizaci\u00f3n</li> </ul>"},{"location":"introduction/01_intro/#librerias-de-machine-learning-en-python","title":"Librer\u00edas de machine learning en python\u00b6","text":"<p>Una de las grandes ventajas que ofrece python  sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning , las principales librer\u00edas que podemos utilizar son:</p>"},{"location":"introduction/01_intro/#scikit-learn","title":"Scikit-Learn\u00b6","text":"<p>Scikit-learn es la principal librer\u00eda que existe para trabajar con Machine Learning, incluye la implementaci\u00f3n de un gran n\u00famero de algoritmos  de aprendizaje. La podemos utilizar para clasificaciones, extraccion de caracter\u00edsticas, regresiones, agrupaciones, reducci\u00f3n de dimensiones, selecci\u00f3n de modelos, o preprocesamiento.</p>"},{"location":"introduction/01_intro/#statsmodels","title":"Statsmodels\u00b6","text":"<p>Statsmodels es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios.  Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos.</p>"},{"location":"introduction/01_intro/#metodologias-para-proyectos-de-data-science","title":"Metodolog\u00edas para proyectos de Data Science\u00b6","text":""},{"location":"introduction/01_intro/#tipos-de-metodologias","title":"Tipos de Metodolog\u00edas\u00b6","text":"<p>Existen varias metodolog\u00edas para proyectos de ciencia de datos, cada una con su propio enfoque y proceso. Algunas de las metodolog\u00edas m\u00e1s populares son:</p> <ul> <li><p>CRISP-DM: Como se mencion\u00f3 anteriormente, CRISP-DM (Proceso Est\u00e1ndar Cruzado para la Miner\u00eda de Datos en la Industria) es una metodolog\u00eda estructurada en seis fases para la miner\u00eda de datos.</p> </li> <li><p>KDD: Como tambi\u00e9n se mencion\u00f3 anteriormente, KDD (Knowledge Discovery in Databases) es una metodolog\u00eda para el descubrimiento de conocimiento a partir de grandes conjuntos de datos.</p> </li> <li><p>Agile Data Science: Es una metodolog\u00eda que se basa en los principios \u00e1giles del desarrollo de software y se enfoca en una colaboraci\u00f3n estrecha entre los miembros del equipo y los interesados del proyecto.</p> </li> <li><p>DataOps: Es una metodolog\u00eda centrada en la automatizaci\u00f3n y la integraci\u00f3n continua de los flujos de trabajo de datos para acelerar el tiempo de entrega y mejorar la calidad.</p> </li> <li><p>Lean Six Sigma: Es una metodolog\u00eda que se enfoca en la mejora continua y la eliminaci\u00f3n de defectos en los procesos.</p> </li> </ul> <p>Cada metodolog\u00eda tiene sus propias ventajas y desventajas, y la elecci\u00f3n de la metodolog\u00eda adecuada depender\u00e1 de las necesidades y objetivos espec\u00edficos del proyecto de ciencia de datos. En general, es importante tener un enfoque estructurado y sistem\u00e1tico para el proceso de ciencia de datos, y utilizar una metodolog\u00eda puede ayudar a garantizar que se aborden todos los aspectos importantes del proyecto.</p>"},{"location":"introduction/01_intro/#pasos-a-seguir","title":"Pasos a seguir\u00b6","text":"<p>Las metodolog\u00edas mencionadas anteriormente las podemos describir en 6 pasos:</p> <ol> <li><p>Recolectar los datos. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos.</p> </li> <li><p>Preprocesar los datos. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.</p> </li> <li><p>Explorar los datos. Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo.</p> </li> <li><p>Entrenar el modelo. En esta etapa se entrenan los modelos con los datos que venimos procesando en las etapas anteriores. La idea es que los modelos puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones.</p> </li> <li><p>Evaluar el modelo. Evaluamos que tan preciso es el modelo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el modelo cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.</p> </li> <li><p>Utilizar el modelo. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.</p> </li> </ol> <p>Los pasos 1,2,3 son los pasos que ya se han visto con detalle en este curso. Por otro lado, la etapa de modelamiento (entrenar, evaluar y predecir) ser\u00e1 necesario introducir nuevos conceptos.</p>"},{"location":"introduction/01_intro/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Basic Concepts in Machine Learning</li> <li>An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples</li> </ol>"},{"location":"machine_learning/cla_01/","title":"Clasificaci\u00f3n I","text":"<p>El modelo es entonces obtenido a base de lo que cada ensayo (valor de $i$) y el conjunto de variables explicativas/independientes puedan informar acerca de la probabilidad final. Estas variables explicativas pueden pensarse como un vector $X_i$ k-dimensional y el modelo toma entonces la forma:</p> <p>$$p_i=\\mathbb{E}(\\dfrac{Y_i}{n_i}|X_i)$$</p> <p>Los logits de las probabilidades binomiales desconocidas (i.e., los logaritmos de la raz\u00f3n de momios) son modeladas como una funci\u00f3n lineal de los $X_i$:</p> <p>$$logit(p_i) = ln(\\dfrac{p_i}{1-p_i}) = \\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i}$$</p> <p>Note que un elemento particular de $X_i$ puede ser ajustado a 1 para todo $i$ obteni\u00e9ndose una constante independiente en el modelo. Los par\u00e1metros desconocidos $\\beta _{j}$ son usualmente estimados a trav\u00e9s de m\u00e1xima verosimilitud.</p> <p>La interpretaci\u00f3n de los estimados del par\u00e1metro $\\beta _{j}$ es como los efectos aditivos en el logaritmo de la raz\u00f3n de momios para una unidad de cambio en la j\u00e9sima variable explicativa. En el caso de una variable explicativa dicot\u00f3mica, por ejemplo g\u00e9nero, $e^{\\beta}$ es la estimaci\u00f3n de la raz\u00f3n de momios (odds ratio) de tener el resultado para, por decir algo, hombres comparados con mujeres. El modelo tiene una formulaci\u00f3n equivalente dada por:</p> <p>$$p_i=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x_{1,i} +....+\\beta_kx_{k,i})}+1}$$</p> Valores categor\u00eda Probabilidad Logit $X_1$ $\\epsilon_1$ $\\pi(X_1)$ $g(X_1)$ $X_2$ $\\epsilon_2$ $\\pi(X_2)$ $g(X_2)$ $X_n$ $\\epsilon_n$ $\\pi(X_n)$ $g(X_n)$ <p>Donde $\u03b5_i$ es \"0\" o \"1\" seg\u00fan el caso y adem\u00e1s:</p> <p>$$0 \\leq \u03c0(X_i) = \\dfrac{1}{i}\\sum_{k=1}^i \u03b5_k\\leq 1 \\, \\ g(X_i) =  ln(\\dfrac{\\pi(X_i)}{1- \\pi(X_i)})=\\beta_0+\\beta_1 X_i $$</p> <p></p> <p>Veamos un peque\u00f1o ejemplo de como se implementa en python. En este ejemplo voy a utilizar el dataset Iris que ya viene junto con Scikit-learn y es ideal para practicar con regresiones log\u00edstica ; el mismo contiene los tipos de flores basado en en largo y ancho de su s\u00e9palo y p\u00e9talo.</p> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  from sklearn import datasets from sklearn.model_selection import train_test_split  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># cargar datos\niris = datasets.load_iris()\nprint(iris.DESCR)\n</pre> # cargar datos iris = datasets.load_iris() print(iris.DESCR) <pre>.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., &amp; Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n</pre> In\u00a0[3]: Copied! <pre># dejar en formato dataframe\n\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['TARGET'] = iris.target\niris_df.head() # estructura de nuestro dataset.\n</pre> # dejar en formato dataframe  iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) iris_df['TARGET'] = iris.target iris_df.head() # estructura de nuestro dataset. Out[3]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) TARGET 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 <p>Para ver gr\u00e1ficamente el modelo de regresi\u00f3n log\u00edstica, ajustemos el modelo solo a dos variables: petal length (cm), petal width (cm).</p> In\u00a0[4]: Copied! <pre># datos \nfrom sklearn.linear_model import LogisticRegression\n\nX = iris_df[['sepal length (cm)', 'sepal width (cm)']]\nY = iris_df['TARGET']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)\n</pre> # datos  from sklearn.linear_model import LogisticRegression  X = iris_df[['sepal length (cm)', 'sepal width (cm)']] Y = iris_df['TARGET']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)  In\u00a0[5]: Copied! <pre># print rows train and test sets\nprint('Separando informacion:\\n')\nprint('numero de filas data original : ',len(X))\nprint('numero de filas train set     : ',len(X_train))\nprint('numero de filas test set      : ',len(X_test))\n</pre> # print rows train and test sets print('Separando informacion:\\n') print('numero de filas data original : ',len(X)) print('numero de filas train set     : ',len(X_train)) print('numero de filas test set      : ',len(X_test)) <pre>Separando informacion:\n\nnumero de filas data original :  150\nnumero de filas train set     :  120\nnumero de filas test set      :  30\n</pre> In\u00a0[6]: Copied! <pre># Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n</pre> # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo Out[6]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> In\u00a0[7]: Copied! <pre>rlog.score(X_train,Y_train)\n</pre> rlog.score(X_train,Y_train) Out[7]: <pre>0.825</pre> In\u00a0[8]: Copied! <pre>rlog.predict_log_proba(X_train)\n</pre> rlog.predict_log_proba(X_train) Out[8]: <pre>array([[ -3.99669749,  -0.66190745,  -0.76409046],\n       [ -0.21107893,  -1.90464333,  -3.18413364],\n       [ -1.8065849 ,  -0.52153993,  -1.41807287],\n       [ -6.47991623,  -3.03584962,  -0.05083842],\n       [ -1.05110824,  -0.71678289,  -1.81936211],\n       [ -0.17004605,  -2.74876413,  -2.3819842 ],\n       [ -4.68728813,  -0.80298536,  -0.61101654],\n       [ -2.96770066,  -0.71008075,  -0.78312858],\n       [ -3.63294174,  -0.2154118 ,  -1.78765395],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -2.98670271,  -0.60263228,  -0.91086157],\n       [ -3.64615407,  -0.71257092,  -0.72664838],\n       [ -4.79873937,  -1.4202387 ,  -0.28754402],\n       [ -2.24998139,  -0.20623326,  -2.51385499],\n       [ -0.06253829,  -2.98157867,  -4.61419118],\n       [ -4.37794957,  -0.51550211,  -0.94097214],\n       [ -0.19806777,  -2.0180827 ,  -3.06239155],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -0.10414361,  -2.60012179,  -3.703401  ],\n       [ -0.1399092 ,  -2.36113102,  -3.31733432],\n       [ -0.33299812,  -1.72995085,  -2.24492641],\n       [ -1.36052475,  -0.39394677,  -2.67243331],\n       [ -2.00164029,  -1.07409158,  -0.64764129],\n       [ -7.3103791 ,  -3.31679977,  -0.03763674],\n       [ -0.34589815,  -1.61640035,  -2.36655737],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -2.32561455,  -1.12330948,  -0.54978327],\n       [ -8.62419813,  -2.24523278,  -0.11214189],\n       [ -5.53049122,  -0.33797257,  -1.26294071],\n       [ -0.06998861,  -2.75798325,  -5.47839753],\n       [ -0.18803569,  -1.9034554 ,  -3.80038456],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -2.4142983 ,  -0.45329007,  -1.29085265],\n       [ -1.43446384,  -0.44603054,  -2.10707824],\n       [ -9.03137722,  -2.37765551,  -0.09748871],\n       [ -4.69174057,  -0.68098728,  -0.72419992],\n       [ -3.66230617,  -0.60227249,  -0.85153142],\n       [ -0.05027585,  -3.34866781,  -4.27573592],\n       [ -2.4142983 ,  -0.45329007,  -1.29085265],\n       [ -0.06340959,  -3.33994623,  -3.6495755 ],\n       [ -2.71090648,  -0.4751419 ,  -1.16562856],\n       [ -3.422256  ,  -0.38407763,  -1.2507754 ],\n       [ -1.82139702,  -0.30530631,  -2.28964102],\n       [ -0.23342127,  -2.28448194,  -2.24098902],\n       [ -3.25980162,  -0.24347857,  -1.72761517],\n       [ -2.33611822,  -0.62801104,  -0.99521069],\n       [ -4.75300091,  -1.24804972,  -0.35053651],\n       [ -9.67524275,  -2.21910725,  -0.11515155],\n       [ -0.34353786,  -1.38299432,  -3.22095311],\n       [ -2.6423909 ,  -1.03887894,  -0.55345829],\n       [ -0.03480928,  -3.73440813,  -4.57337069],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -5.18402425,  -1.53076722,  -0.25099663],\n       [ -0.24523474,  -1.68589809,  -3.43575133],\n       [ -3.35645967,  -0.10909088,  -2.68102926],\n       [ -0.24523474,  -1.68589809,  -3.43575133],\n       [ -0.07746962,  -2.84820416,  -4.09855929],\n       [ -0.12756408,  -2.60168695,  -3.08752732],\n       [ -5.46678423,  -1.28586978,  -0.32938621],\n       [ -2.6423909 ,  -1.03887894,  -0.55345829],\n       [ -0.11158762,  -2.48111528,  -3.81957595],\n       [ -0.01601167,  -4.64447482,  -5.07204482],\n       [ -5.40710339,  -0.59393632,  -0.81336006],\n       [-10.32214698,  -2.06359771,  -0.13585311],\n       [ -5.53049122,  -0.33797257,  -1.26294071],\n       [ -5.07086967,  -2.1981711 ,  -0.12475057],\n       [ -1.66675571,  -0.29897084,  -2.66556294],\n       [ -3.66230617,  -0.60227249,  -0.85153142],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -0.10414361,  -2.60012179,  -3.703401  ],\n       [ -8.46843914,  -1.83657274,  -0.17384478],\n       [ -2.33611822,  -0.62801104,  -0.99521069],\n       [ -5.03844127,  -0.75293161,  -0.64906835],\n       [ -0.13370583,  -2.14573723,  -4.80718114],\n       [ -2.13015961,  -0.44390775,  -1.42854624],\n       [ -0.14884726,  -2.24361855,  -3.43500331],\n       [ -0.15976538,  -2.12808615,  -3.55465238],\n       [ -3.1182411 ,  -0.3548191 ,  -1.36859278],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -2.5357413 ,  -0.32183203,  -1.62975754],\n       [ -4.33853357,  -0.72898717,  -0.68409426],\n       [ -5.57382666,  -1.64581327,  -0.21896676],\n       [ -6.14300989,  -1.15968166,  -0.37940919],\n       [ -3.67733948,  -1.1231079 ,  -0.43164097],\n       [ -5.51537947,  -1.46091555,  -0.26925051],\n       [ -3.25980162,  -0.24347857,  -1.72761517],\n       [ -1.50374348,  -0.6199054 ,  -1.42833279],\n       [ -2.64559993,  -0.66273639,  -0.88286013],\n       [ -3.69170941,  -0.5052252 ,  -0.98966559],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -1.70787592,  -0.80218252,  -0.99317107],\n       [ -0.03480928,  -3.73440813,  -4.57337069],\n       [ -6.17079981,  -0.42876844,  -1.05958475],\n       [ -1.50374348,  -0.6199054 ,  -1.42833279],\n       [ -3.18432235,  -0.29444982,  -1.54340497],\n       [ -3.06265058,  -0.4256791 ,  -1.20427132],\n       [ -3.65400219,  -0.97332008,  -0.51703462],\n       [ -5.05989815,  -1.02728954,  -0.45306334],\n       [ -0.03094358,  -4.23634452,  -4.13458123],\n       [ -0.09199006,  -2.50522834,  -5.0785667 ],\n       [ -3.98888929,  -0.78054977,  -0.64755132],\n       [ -3.31830082,  -1.0388256 ,  -0.49443459],\n       [ -7.74336497,  -1.78746182,  -0.18370422],\n       [ -0.05357357,  -3.225515  ,  -4.38776458],\n       [ -6.62509295,  -1.61990942,  -0.2221981 ],\n       [ -0.03965368,  -3.48635147,  -4.79567696],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -4.08715355,  -1.38461613,  -0.31089182],\n       [ -0.34589815,  -1.61640035,  -2.36655737],\n       [ -3.31177254,  -0.65294575,  -0.81409912],\n       [ -0.07511489,  -2.74125422,  -4.84422965],\n       [ -5.0899367 ,  -1.18377862,  -0.37437096],\n       [ -1.76295435,  -0.6043599 ,  -1.26571138],\n       [ -0.11158762,  -2.48111528,  -3.81957595],\n       [ -4.35319491,  -0.99654955,  -0.48129372],\n       [ -4.71854451,  -1.08714279,  -0.42481105],\n       [ -0.07746962,  -2.84820416,  -4.09855929],\n       [ -0.01626132,  -4.2872282 ,  -6.03778143],\n       [ -5.40710339,  -0.59393632,  -0.81336006],\n       [ -0.03160108,  -4.48990308,  -3.91777686]])</pre> <p>Grafiquemos nuestro resultados:</p> In\u00a0[9]: Copied! <pre># dataframe a matriz\nX = X.values\nY = Y.values\n</pre> # dataframe a matriz X = X.values Y = Y.values In\u00a0[10]: Copied! <pre># grafica de la regresion logistica \nplt.figure(figsize=(12,4))\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = rlog.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.show()\n</pre> # grafica de la regresion logistica  plt.figure(figsize=(12,4))  x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5  h = .02  # step size in the mesh xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = rlog.predict(np.c_[xx.ravel(), yy.ravel()])  # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure(1, figsize=(4, 3)) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')  # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.show() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n</pre> <p>Gr\u00e1ficamente podemos decir que el modelo se ajusta bastante bien, puesto que las clasificaciones son adecuadas y el modelo no se confunde entre una clase y otra. Por otro lado, existe valores num\u00e9ricos que tambi\u00e9n nos pueden ayudar a convensernos de estos, que son las m\u00e9tricas que se habian definidos con anterioridad.</p> <p>Para ello, instanciaremos las distintas metricas del archivo metrics_classification.py y calcularemos sus distintos valores.</p> In\u00a0[11]: Copied! <pre>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n</pre> from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score In\u00a0[12]: Copied! <pre># Evaluar las m\u00e9tricas\ndef classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: dataframe con las columnas: ['y', 'yhat']\n    :return: dataframe con las m\u00e9tricas especificadas\n    \"\"\"\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    accuracy = round(accuracy_score(y_true, y_pred), 4)\n    recall = round(recall_score(y_true, y_pred, average='macro'), 4)\n    precision = round(precision_score(y_true, y_pred, average='macro'), 4)\n    fscore = round(f1_score(y_true, y_pred, average='macro'), 4)\n\n    df_result = pd.DataFrame({'accuracy': [accuracy],\n                              'recall': [recall],\n                              'precision': [precision],\n                              'fscore': [fscore]})\n\n    return df_result\n</pre> # Evaluar las m\u00e9tricas def classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: dataframe con las columnas: ['y', 'yhat']     :return: dataframe con las m\u00e9tricas especificadas     \"\"\"     y_true = df['y']     y_pred = df['yhat']      accuracy = round(accuracy_score(y_true, y_pred), 4)     recall = round(recall_score(y_true, y_pred, average='macro'), 4)     precision = round(precision_score(y_true, y_pred, average='macro'), 4)     fscore = round(f1_score(y_true, y_pred, average='macro'), 4)      df_result = pd.DataFrame({'accuracy': [accuracy],                               'recall': [recall],                               'precision': [precision],                               'fscore': [fscore]})      return df_result In\u00a0[13]: Copied! <pre># metrics\n\ny_true =  list(Y_test)\ny_pred = list(rlog.predict(X_test))\n\nprint('Valores:\\n')\nprint('originales: ', y_true)\nprint('predicho:   ', y_pred)\n</pre> # metrics  y_true =  list(Y_test) y_pred = list(rlog.predict(X_test))  print('Valores:\\n') print('originales: ', y_true) print('predicho:   ', y_pred) <pre>Valores:\n\noriginales:  [0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 2, 0, 2]\npredicho:    [0, 0, 1, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 0, 1, 0, 2]\n</pre> In\u00a0[14]: Copied! <pre>print('\\nMatriz de confusion:\\n ')\nprint(confusion_matrix(y_true,y_pred))\n</pre> print('\\nMatriz de confusion:\\n ') print(confusion_matrix(y_true,y_pred)) <pre>\nMatriz de confusion:\n \n[[13  1  0]\n [ 0  4  4]\n [ 0  2  6]]\n</pre> In\u00a0[15]: Copied! <pre># ejemplo \ndf_temp = pd.DataFrame(\n    {\n        'y':y_true,\n        'yhat':y_pred\n        }\n)\n\ndf_metrics = classification_metrics(df_temp)\nprint(\"\\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\")\nprint(\"\")\ndf_metrics\n</pre> # ejemplo  df_temp = pd.DataFrame(     {         'y':y_true,         'yhat':y_pred         } )  df_metrics = classification_metrics(df_temp) print(\"\\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\") print(\"\") df_metrics <pre>\nMetricas para los regresores : 'sepal length (cm)' y  'sepal width (cm)'\n\n</pre> Out[15]: accuracy recall precision fscore 0 0.7667 0.7262 0.7238 0.721 <p>Basado en las m\u00e9tricas y en la gr\u00e1fica, podemos concluir que el ajuste realizado es bastante asertado.</p> <p>Ahora, calculamos la curva AUC-ROC para nuestro ejemplo. Cabe destacar que esta curva es efectiva solo para clasificaci\u00f3n binaria, por lo que para efectos pr\u00e1cticos convertiremos nuestro TARGET en binarios (0 \u00f3 1).</p> <p>Para efectos pr\u00e1cticos tranformaremos la clase objetivo (en este caso, la clase 0) a 1, y el resto de las clases (clase 1 y 2) las dejaremos en la clase 0.</p> In\u00a0[16]: Copied! <pre>from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n</pre> from sklearn.metrics import roc_curve from sklearn.metrics import roc_auc_score In\u00a0[17]: Copied! <pre># graficar curva roc\ndef plot_roc_curve(fpr, tpr):\n    plt.figure(figsize=(8,8))\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n</pre> # graficar curva roc def plot_roc_curve(fpr, tpr):     plt.figure(figsize=(8,8))     plt.plot(fpr, tpr, color='orange', label='ROC')     plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate')     plt.title('Receiver Operating Characteristic (ROC) Curve')     plt.legend()     plt.show() In\u00a0[18]: Copied! <pre># separar clase 0 del resto\nX = iris_df[['sepal length (cm)', 'sepal width (cm)']]\nY = iris_df['TARGET'].apply(lambda x: 1 if x ==2 else 0)\nmodel =  LogisticRegression()\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 2)\n\n# ajustar modelo \nmodel.fit(X_train,Y_train)\n</pre> # separar clase 0 del resto X = iris_df[['sepal length (cm)', 'sepal width (cm)']] Y = iris_df['TARGET'].apply(lambda x: 1 if x ==2 else 0) model =  LogisticRegression()  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 2)  # ajustar modelo  model.fit(X_train,Y_train) Out[18]: <pre>LogisticRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression<pre>LogisticRegression()</pre> In\u00a0[19]: Copied! <pre># calcular score AUC\nprobs = model.predict_proba(X_test) # predecir probabilidades para X_test\nprobs_tp = probs[:, 1] # mantener solo las probabilidades de la clase positiva \n       \nauc = roc_auc_score(Y_test, probs_tp)  # calcular score AUC \n\nprint('AUC: %.2f' % auc)\n</pre> # calcular score AUC probs = model.predict_proba(X_test) # predecir probabilidades para X_test probs_tp = probs[:, 1] # mantener solo las probabilidades de la clase positiva          auc = roc_auc_score(Y_test, probs_tp)  # calcular score AUC   print('AUC: %.2f' % auc) <pre>AUC: 0.93\n</pre> In\u00a0[20]: Copied! <pre># calcular curva ROC\nfpr, tpr, thresholds = roc_curve(Y_test, probs_tp) # obtener curva ROC\nplot_roc_curve(fpr, tpr)\n</pre> # calcular curva ROC fpr, tpr, thresholds = roc_curve(Y_test, probs_tp) # obtener curva ROC plot_roc_curve(fpr, tpr) In\u00a0[21]: Copied! <pre>from sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom matplotlib.colors import ListedColormap\n\n\nh = .02  # step size in the mesh\nplt.figure(figsize=(12,12))\nnames = [\"Logistic\",\n         \"RBF SVM\", \n         \"Decision Tree\", \n         \"Random Forest\"\n]\n\nclassifiers = [\n    LogisticRegression(),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n]\n\n\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n\n\n\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()\n</pre> from sklearn.datasets import make_moons, make_circles, make_classification from sklearn.preprocessing import StandardScaler   from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier  from matplotlib.colors import ListedColormap   h = .02  # step size in the mesh plt.figure(figsize=(12,12)) names = [\"Logistic\",          \"RBF SVM\",           \"Decision Tree\",           \"Random Forest\" ]  classifiers = [     LogisticRegression(),     SVC(gamma=2, C=1),     DecisionTreeClassifier(max_depth=5),     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ]    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,                            random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y)  datasets = [make_moons(noise=0.3, random_state=0),             make_circles(noise=0.2, factor=0.5, random_state=1),             linearly_separable             ]    figure = plt.figure(figsize=(27, 9)) i = 1    # iterate over datasets for ds_cnt, ds in enumerate(datasets):     # preprocess dataset, split into training and test part     X, y = ds     X = StandardScaler().fit_transform(X)     X_train, X_test, y_train, y_test = \\         train_test_split(X, y, test_size=.4, random_state=42)      x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),                          np.arange(y_min, y_max, h))      # just plot the dataset first     cm = plt.cm.RdBu     cm_bright = ListedColormap(['#FF0000', '#0000FF'])     ax = plt.subplot(len(datasets), len(classifiers) + 1, i)     if ds_cnt == 0:         ax.set_title(\"Input data\")     # Plot the training points     ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                edgecolors='k')     # Plot the testing points     ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,                edgecolors='k')     ax.set_xlim(xx.min(), xx.max())     ax.set_ylim(yy.min(), yy.max())     ax.set_xticks(())     ax.set_yticks(())     i += 1      # iterate over classifiers     for name, clf in zip(names, classifiers):         ax = plt.subplot(len(datasets), len(classifiers) + 1, i)         clf.fit(X_train, y_train)         score = clf.score(X_test, y_test)          # Plot the decision boundary. For that, we will assign a color to each         # point in the mesh [x_min, x_max]x[y_min, y_max].         if hasattr(clf, \"decision_function\"):             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])         else:             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]          # Put the result into a color plot         Z = Z.reshape(xx.shape)         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)          # Plot the training points         ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,                    edgecolors='k')         # Plot the testing points         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,                    edgecolors='k', alpha=0.6)          ax.set_xlim(xx.min(), xx.max())         ax.set_ylim(yy.min(), yy.max())         ax.set_xticks(())         ax.set_yticks(())         if ds_cnt == 0:             ax.set_title(name)         ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),                 size=15, horizontalalignment='right')         i += 1  plt.tight_layout() plt.show() <pre>&lt;Figure size 1200x1200 with 0 Axes&gt;</pre> In\u00a0[22]: Copied! <pre>class SklearnClassificationModels:\n    def __init__(self,model,name_model):\n\n        self.model = model\n        self.name_model = name_model\n        \n    @staticmethod\n    def test_train_model(X,y,n_size):\n        X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    def fit_model(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        return self.model.fit(X_train, y_train) \n    \n    def df_testig(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        model_fit = self.model.fit(X_train, y_train)\n        preds = model_fit.predict(X_test)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test,\n                'yhat': model_fit.predict(X_test)\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,X,y,test_size):\n        df_temp = self.df_testig(X,y,test_size)\n        df_metrics = classification_metrics(df_temp)\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n</pre> class SklearnClassificationModels:     def __init__(self,model,name_model):          self.model = model         self.name_model = name_model              @staticmethod     def test_train_model(X,y,n_size):         X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)         return X_train, X_test, y_train, y_test          def fit_model(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         return self.model.fit(X_train, y_train)           def df_testig(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         model_fit = self.model.fit(X_train, y_train)         preds = model_fit.predict(X_test)         df_temp = pd.DataFrame(             {                 'y':y_test,                 'yhat': model_fit.predict(X_test)             }         )                  return df_temp          def metrics(self,X,y,test_size):         df_temp = self.df_testig(X,y,test_size)         df_metrics = classification_metrics(df_temp)         df_metrics['model'] = self.name_model                  return df_metrics  In\u00a0[23]: Copied! <pre># metrics \n\nimport itertools\n\n# nombre modelos\nnames_models = [\"Logistic\",\n         \"RBF SVM\", \n         \"Decision Tree\", \n         \"Random Forest\"\n]\n\n# modelos\nclassifiers = [\n    LogisticRegression(),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n]\n\n# datasets\nnames_dataset = ['make_moons',\n                 'make_circles',\n                 'linearly_separable'\n                ]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\n\n# juntar informacion\nlist_models = list(zip(names_models,classifiers))\nlist_dataset = list(zip(names_dataset,datasets))\n\nframes = []\nfor x in itertools.product(list_models, list_dataset):\n    \n    name_model = x[0][0]\n    classifier = x[0][1]\n    \n    name_dataset = x[1][0]\n    dataset = x[1][1]\n    \n    X = dataset[0]\n    Y =  dataset[1]\n    \n    fit_model =  SklearnClassificationModels( classifier,name_model)\n    df = fit_model.metrics(X,Y,0.2)\n    df['dataset'] = name_dataset\n    \n    frames.append(df)\n</pre> # metrics   import itertools  # nombre modelos names_models = [\"Logistic\",          \"RBF SVM\",           \"Decision Tree\",           \"Random Forest\" ]  # modelos classifiers = [     LogisticRegression(),     SVC(gamma=2, C=1),     DecisionTreeClassifier(max_depth=5),     RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), ]  # datasets names_dataset = ['make_moons',                  'make_circles',                  'linearly_separable'                 ]  X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,                            random_state=1, n_clusters_per_class=1) rng = np.random.RandomState(2) X += 2 * rng.uniform(size=X.shape) linearly_separable = (X, y)  datasets = [make_moons(noise=0.3, random_state=0),             make_circles(noise=0.2, factor=0.5, random_state=1),             linearly_separable             ]   # juntar informacion list_models = list(zip(names_models,classifiers)) list_dataset = list(zip(names_dataset,datasets))  frames = [] for x in itertools.product(list_models, list_dataset):          name_model = x[0][0]     classifier = x[0][1]          name_dataset = x[1][0]     dataset = x[1][1]          X = dataset[0]     Y =  dataset[1]          fit_model =  SklearnClassificationModels( classifier,name_model)     df = fit_model.metrics(X,Y,0.2)     df['dataset'] = name_dataset          frames.append(df) <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n</pre> In\u00a0[24]: Copied! <pre># juntar resultados\npd.concat(frames)\n</pre> # juntar resultados pd.concat(frames) Out[24]: accuracy recall precision fscore model dataset 0 0.90 0.9000 0.9000 0.9000 Logistic make_moons 0 0.35 0.5000 0.1750 0.2593 Logistic make_circles 0 0.95 0.9545 0.9500 0.9499 Logistic linearly_separable 0 0.95 0.9500 0.9545 0.9499 RBF SVM make_moons 0 0.80 0.8462 0.8182 0.7980 RBF SVM make_circles 0 0.95 0.9545 0.9500 0.9499 RBF SVM linearly_separable 0 0.95 0.9500 0.9545 0.9499 Decision Tree make_moons 0 0.75 0.8077 0.7917 0.7494 Decision Tree make_circles 0 0.85 0.8535 0.8500 0.8496 Decision Tree linearly_separable 0 0.95 0.9500 0.9545 0.9499 Random Forest make_moons 0 0.75 0.8077 0.7917 0.7494 Random Forest make_circles 0 0.80 0.8081 0.8081 0.8000 Random Forest linearly_separable"},{"location":"machine_learning/cla_01/#clasificacion-i","title":"Clasificaci\u00f3n I\u00b6","text":""},{"location":"machine_learning/cla_01/#regresion-logistica-y-otros-modelos","title":"Regresi\u00f3n Log\u00edstica y otros modelos\u00b6","text":"<p>A modo de recuerdo, el modelo de regresio\u0301n lineal general  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>Por otro lado, el modelo de regresi\u00f3n log\u00edstica  analiza datos distribuidos binomialmente de la forma: $Y_i \\sim B(p_i,n_i)$, para $i=1,...,m$ donde los n\u00fameros de ensayos Bernoulli $n_{i}$ son conocidos y las probabilidades de \u00e9xito $p_{i}$ son desconocidas. Un ejemplo de esta distribuci\u00f3n es el porcentaje de semillas $p_{i} $ que germinan despu\u00e9s de que $n_{i}$ son plantadas.</p>"},{"location":"machine_learning/cla_01/#interpretacion-para-el-caso-binario","title":"Interpretaci\u00f3n para el caso binario\u00b6","text":"<p>La idea es que la regresi\u00f3n log\u00edstica aproxime la probabilidad de obtener  0 (no ocurre cierto suceso) o  1 (ocurre el suceso) con el valor de la variable explicativa $x$.</p> <p>En esas condiciones, la probabilidad aproximada del suceso se aproximar\u00e1 mediante una funci\u00f3n log\u00edstica del tipo:</p> <p>$$\\pi(x) =\\dfrac{e^{\\beta_0+\\beta_1x}}{e^{\\beta_0+\\beta_1x}+1}=\\dfrac{1}{e^{-(\\beta_0+\\beta_1x)}+1}$$</p> <p>que puede reducirse al c\u00e1lculo de una regresi\u00f3n lineal para la funci\u00f3n logit de la probabilidad:</p> <p>$$g(x) = ln(\\dfrac{\\pi(x)}{1- \\pi(x)})=\\beta_0+\\beta_1 x$$</p> <p>El gr\u00e1fico de la funci\u00f3n log\u00edstica se muestra en la figura que encabeza esta secci\u00f3n, la variable independiente es la combinaci\u00f3n lineal $=\\beta_0+\\beta_1$ y la variable dependiente es la probabilidad estimada $ \\pi (x)$. Si se realiza la regresi\u00f3n lineal, la forma de la probabilidad estimada puede ser f\u00e1cilmente recuperada a partir de los coeficientes calculados.</p> <p>Para hacer la regresi\u00f3n deben tomarse los valores $X_i$ de las observaciones ordenados de mayor a menor y formar la siguiente tabla:</p>"},{"location":"machine_learning/cla_01/#error-de-prediccion","title":"Error de Predicci\u00f3n\u00b6","text":""},{"location":"machine_learning/cla_01/#matriz-de-confusion","title":"Matriz de confusi\u00f3n\u00b6","text":"<p>Los modelos de clasificacion son ocupadas para predecir valores categ\u00f3ricos, por ejemplo, determinar la especie de una flor basado en el largo (y ancho) de su p\u00e9talo (y s\u00e9palo).Para este caso, es necesario introducir el concepto de matriz de confusi\u00f3n.</p> <p>La matriz de confusi\u00f3n es una herramienta que permite la visualizaci\u00f3n del desempe\u00f1o de un algoritmo Para la clasificaci\u00f3n de dos clases (por ejemplo, 0 y 1), se tiene la siguiente matriz de confusi\u00f3n:</p> <p></p> <p>Ac\u00e1 se define:</p> <ul> <li>TP = Verdadero positivo: el modelo predijo la clase positiva correctamente, para ser una clase positiva.</li> <li>FP = Falso positivo: el modelo predijo la clase negativa incorrectamente, para ser una clase positiva.</li> <li>FN = Falso negativo: el modelo predijo incorrectamente que la clase positiva ser\u00eda la clase negativa.</li> <li>TN = Verdadero negativo: el modelo predijo la clase negativa correctamente, para ser la clase negativa.</li> </ul> <p>En este contexto, los valores TP Y TN muestran los valores correctos que tuve al momento de realizar la predicci\u00f3n, mientras que los valores de de FN Y FP denotan los valores que me equivoque de clase.</p> <p>Los conceptos de FN y FP se pueden interpretar con la siguiente imagen:</p> <p></p>"},{"location":"machine_learning/cla_01/#metricas-de-error","title":"M\u00e9tricas de Error\u00b6","text":"<p>En este contexto, se busca maximizar el n\u00famero al m\u00e1ximo la suma de los elementos TP Y TN, mientras que se busca disminuir la suma de los elementos de FN y FP. Para esto se definen las siguientes m\u00e9tricas:</p> <ol> <li>Accuracy</li> </ol> <p>$$accuracy(y,\\hat{y}) = \\dfrac{TP+TN}{TP+TN+FP+FN}$$</p> <ol> <li>Recall:</li> </ol> <p>$$recall(y,\\hat{y}) = \\dfrac{TP}{TP+FN}$$</p> <ol> <li>Precision:</li> </ol> <p>$$precision(y,\\hat{y}) = \\dfrac{TP}{TP+FP} $$</p> <ol> <li>F-score:</li> </ol> <p>$$fscore(y,\\hat{y}) = 2\\times \\dfrac{precision(y,\\hat{y})\\times recall(y,\\hat{y})}{precision(y,\\hat{y})+recall(y,\\hat{y})} $$</p>"},{"location":"machine_learning/cla_01/#curva-aucroc","title":"Curva  AUC\u2013ROC\u00b6","text":"<p>La curva AUC\u2013ROC es una representaci\u00f3n gr\u00e1fica de la sensibilidad frente a la especificidad para un sistema clasificador binario seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n. Otra interpretaci\u00f3n de este gr\u00e1fico es la representaci\u00f3n de la raz\u00f3n o proporci\u00f3n de verdaderos positivos (VPR = Raz\u00f3n de Verdaderos Positivos) frente a la raz\u00f3n o proporci\u00f3n de falsos positivos (FPR = Raz\u00f3n de Falsos Positivos) tambi\u00e9n seg\u00fan se var\u00eda el umbral de discriminaci\u00f3n (valor a partir del cual decidimos que un caso es un positivo). ROC tambi\u00e9n puede significar Relative Operating Characteristic (Caracter\u00edstica Operativa Relativa) porque es una comparaci\u00f3n de dos caracter\u00edsticas operativas (VPR y FPR) seg\u00fan cambiamos el umbral para la decisi\u00f3n.</p> <p>En espa\u00f1ol es preferible mantener el acr\u00f3nimo ingl\u00e9s, aunque es posible encontrar el equivalente espa\u00f1ol COR. No se suele utilizar ROC aislado, debemos decir \u201ccurva ROC\u201d o \u201can\u00e1lisis ROC\u201d.</p> <p></p> <p>El \u00e1rea cubierta por la curva es el \u00e1rea entre la l\u00ednea naranja (ROC) y el eje. Esta \u00e1rea cubierta es AUC. Cuanto m\u00e1s grande sea el \u00e1rea cubierta, mejores ser\u00e1n los modelos de aprendizaje autom\u00e1tico para distinguir las clases dadas. El valor ideal para AUC es 1.</p>"},{"location":"machine_learning/cla_01/#ejemplo-dataset-iris","title":"Ejemplo: Dataset Iris\u00b6","text":""},{"location":"machine_learning/cla_01/#otros-modelos-de-clasificacion","title":"Otros modelos de clasificaci\u00f3n\u00b6","text":"<p>Existen varios modelos de clasificaci\u00f3n que podemos ir comparando unos con otros, dentro de los cuales estacamos los siguientes:</p> <ul> <li>Regresi\u00f3n Log\u00edstica</li> <li>Arboles de Decision</li> <li>Random Forest</li> <li>SVM</li> </ul> <p>Nos basaremos en un ejemplo de sklearn que muestra los resultados de aplicar estos cuatro modelos sobre tres conjunto de datos distintos ( make_moons, make_circles, make_classification). Adem\u00e1s, se crea un rutina para comparar los resultados de las distintas m\u00e9tricas.</p>"},{"location":"machine_learning/cla_01/#graficos","title":"Gr\u00e1ficos\u00b6","text":"<p>Similar al gr\u00e1fico aplicado al conjunto de datos Iris, aca se realiza el mismo ejercicio pero para tres conjunto de datos sobre los distintos modelos.</p>"},{"location":"machine_learning/cla_01/#metricas","title":"M\u00e9tricas\u00b6","text":"<p>Dado que el sistema de calcular m\u00e9tricas sigue el mismo formato, solo cambiando el conjunto de datos y el modelo, se decide realizar una clase que automatice este proceso.</p>"},{"location":"machine_learning/cla_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Supervised learning</li> </ol>"},{"location":"machine_learning/ml_intro/","title":"Introducci\u00f3n","text":""},{"location":"machine_learning/ml_intro/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"machine_learning/ml_intro/#que-es-el-machine-learning","title":"\u00bfQu\u00e9 es el Machine learning?\u00b6","text":"<p>Podemos resumir Machine Learning en cuatro puntos:</p> <ul> <li><p>Estudia y construye sistemas que pueden aprender de los datos, m\u00e1s que seguir instrucciones expl\u00edcitamente programadas.</p> </li> <li><p>Conjunto de t\u00e9cnicas y modelos que permiten el modelamiento predictivo de datos, reunidas a partir de la intersecci\u00f3n de elementos de probabilidad, estad\u00edstica e inteligencia artificial.</p> </li> <li><p>T\u00edpicamente, alguien que trabaja en Machine Learning est\u00e1 en la Academia y busca realizar investigaci\u00f3n y publicar art\u00edculos.</p> </li> <li><p>Pregunta fundamental: \u00bfQu\u00e9 conocimiento emerge a partir de los datos? \u00bfQu\u00e9 modelo/t\u00e9cnica otorga la mejor predicci\u00f3n para estos datos?</p> </li> </ul> <p>Para poder avanzar en el estudio de machine learning es de vital importancia definir el concepto de modelo.</p>"},{"location":"machine_learning/ml_intro/#que-se-entiende-por-modelo","title":"\u00bfQu\u00e9 se entiende por modelo?\u00b6","text":"<p>Un modelo se entinde como:</p> <ul> <li>Una representaci\u00f3n abstracta y conveniente de un sistema.</li> <li>Una simplificaci\u00f3n del mundo real.</li> <li>Un medio de exploraci\u00f3n y de explicaci\u00f3n para nuestro entendimiento de la realidad.</li> </ul> <p>Por otro lado, un modelo no es:</p> <ul> <li>Igual al mundo real.</li> <li>Un sustituto para mediciones o experimentos.</li> </ul> <p>Los modelos nos permiten:</p> <ul> <li><p>reproducir experimentos donde factores no pueden ser f\u00e1cilmente controlados.</p> <ul> <li>Ejemplo: Comportamiento de c\u00e1psula lunar en gravedad cero y al atravesar atm\u00f3sfera a gran velocidad.</li> </ul> </li> <li><p>simplificar el entendimiento de sistemas complejos, al permitir el an\u00e1lisis y exploraci\u00f3n de cada componente del sistema por separado.</p> <ul> <li>Ejemplo: Adelgazamiento de la capa de hielo polar por calentamiento global.</li> </ul> </li> </ul>"},{"location":"machine_learning/ml_intro/#que-tipos-de-problemas-podemos-abordar-con-machine-learning","title":"\u00bf Qu\u00e9 tipos de problemas podemos abordar con machine learning?\u00b6","text":"<p>Los problemas que se pueden resolver con machine learning se pueden englobar en tres tipos: aprendeizaje supervisado, aprendizaje no supervisado y aprendizaje reforzado.</p> <p></p>"},{"location":"machine_learning/ml_intro/#aprendizaje-supervisado","title":"Aprendizaje supervisado\u00b6","text":"<ul> <li>El sistema aprende en base a datos estructurados o no estructurados.</li> <li>Clasificados previamente (se conoce la respuesta).</li> <li>El algoritmo produce una funci\u00f3n que establece una correspondencia entre las entradas y las salidas deseadas del sistema.</li> </ul>"},{"location":"machine_learning/ml_intro/#aprendizaje-no-supervisado","title":"Aprendizaje no supervisado\u00b6","text":"<ul> <li>Modelo se construye usando un conjunto de datos como entrada, los cuales no han sido clasificados previamente.</li> <li>El sistema tiene que ser capaz de reconocer patrones para poder etiquetar las nuevas entradas.</li> </ul>"},{"location":"machine_learning/ml_intro/#aprendizaje-por-refuerzo","title":"Aprendizaje por refuerzo\u00b6","text":"<p>Aprendizaje por refuerzo o Aprendizaje reforzado es un \u00e1rea del aprendizaje autom\u00e1tico inspirada en la psicolog\u00eda conductista, cuya ocupaci\u00f3n es determinar qu\u00e9 acciones debe escoger un agente de software en un entorno dado con el fin de maximizar alguna noci\u00f3n de \"recompensa\" o premio acumulado.</p> <p></p>"},{"location":"machine_learning/ml_intro/#algoritmos-mas-utilizados","title":"Algoritmos m\u00e1s utilizados\u00b6","text":"<p>Los algoritmos que m\u00e1s se suelen utilizar en los problemas de Machine Learning son los siguientes:</p> <ol> <li>Regresi\u00f3n Lineal</li> <li>Regresi\u00f3n Log\u00edstica</li> <li>Arboles de Decision</li> <li>Random Forest</li> <li>SVM</li> <li>KNN</li> <li>K-means</li> </ol> <p>\u00bf Qu\u00e9 se necesita para aprender machine learning?</p> <p>Se necesita tener conocimientos de los siguientes t\u00f3picos.</p> <ul> <li>Algebra Lineal</li> <li>Probabilidad y estad\u00edstica</li> <li>Optimizaci\u00f3n</li> </ul>"},{"location":"machine_learning/ml_intro/#librerias-de-machine-learning-en-python","title":"Librer\u00edas de machine learning en python\u00b6","text":"<p>Una de las grandes ventajas que ofrece python  sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning , las principales librer\u00edas que podemos utilizar son:</p>"},{"location":"machine_learning/ml_intro/#scikit-learn","title":"Scikit-Learn\u00b6","text":"<p>Scikit-learn es la principal librer\u00eda que existe para trabajar con Machine Learning, incluye la implementaci\u00f3n de un gran n\u00famero de algoritmos  de aprendizaje. La podemos utilizar para clasificaciones, extraccion de caracter\u00edsticas, regresiones, agrupaciones, reducci\u00f3n de dimensiones, selecci\u00f3n de modelos, o preprocesamiento.</p>"},{"location":"machine_learning/ml_intro/#statsmodels","title":"Statsmodels\u00b6","text":"<p>Statsmodels es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios.  Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos.</p>"},{"location":"machine_learning/ml_intro/#conceptos-claves-en-machine-learning","title":"Conceptos claves en machine learning\u00b6","text":""},{"location":"machine_learning/ml_intro/#esquema-machine-learning","title":"Esquema machine learning\u00b6","text":"<p>EL proceso de machine learning se puede resumir a grandes rasgo por el siguiente esquema.</p> <p></p> <ol> <li><p>Recolectar los datos. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos.</p> </li> <li><p>Preprocesar los datos. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.</p> </li> <li><p>Explorar los datos. Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo.</p> </li> <li><p>Entrenar el modelo. En esta etapa se entrenan los modelos con los datos que venimos procesando en las etapas anteriores. La idea es que los modelos puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones.</p> </li> <li><p>Evaluar el modelo. Evaluamos que tan preciso es el modelo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el modelo cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.</p> </li> <li><p>Utilizar el modelo. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.</p> </li> </ol> <p>Los pasos 1,2,3 son los pasos que ya se han visto con detalle en este curso. Por otro lado, la etapa de modelamiento (entrenar, evaluar y predecir) ser\u00e1 necesario introducir nuevos conceptos.</p>"},{"location":"machine_learning/ml_intro/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Basic Concepts in Machine Learning</li> <li>An Introduction to Machine Learning Theory and Its Applications: A Visual Tutorial with Examples</li> </ol>"},{"location":"machine_learning/ml_intro/","title":"Introducci\u00f3n","text":""},{"location":"machine_learning/ml_intro/#que-es-el-machine-learning","title":"\u00bfQu\u00e9 es el Machine learning?","text":"<p>Podemos resumir Machine Learning en cuatro puntos:</p> <ul> <li> <p>Estudia y construye sistemas que pueden aprender de los datos, m\u00e1s     que seguir instrucciones expl\u00edcitamente programadas.</p> </li> <li> <p>Conjunto de t\u00e9cnicas y modelos que permiten     el modelamiento predictivo de datos, reunidas a partir de la     intersecci\u00f3n de elementos de probabilidad, estad\u00edstica e     inteligencia artificial.</p> </li> <li> <p>T\u00edpicamente, alguien que trabaja en Machine Learning est\u00e1 en la Academia y busca realizar investigaci\u00f3n y publicar art\u00edculos.</p> </li> <li> <p>Pregunta fundamental:     \u00bfQu\u00e9 conocimiento emerge a partir de los datos? \u00bfQu\u00e9 modelo/t\u00e9cnica     otorga la mejor predicci\u00f3n para estos datos?</p> </li> </ul> <p>Para poder avanzar en el estudio de machine learning es de vital importancia definir el concepto de modelo.</p>"},{"location":"machine_learning/ml_intro/#que-se-entiende-por-modelo","title":"\u00bfQu\u00e9 se entiende por modelo?","text":"<p>Un modelo se entinde como: * Una representaci\u00f3n abstracta y conveniente de un sistema. * Una simplificaci\u00f3n del mundo real. * Un medio de exploraci\u00f3n y de explicaci\u00f3n para nuestro entendimiento de la realidad.</p> <p>Por otro lado, un modelo no es:</p> <ul> <li>Igual al mundo real.</li> <li>Un sustituto para mediciones o experimentos.</li> </ul> <p>Los modelos nos permiten: </p> <ul> <li> <p>reproducir experimentos donde factores no pueden ser f\u00e1cilmente controlados.</p> <ul> <li>Ejemplo: Comportamiento de c\u00e1psula lunar en gravedad cero y al atravesar atm\u00f3sfera a gran velocidad.</li> </ul> </li> <li> <p>simplificar el entendimiento de sistemas complejos, al permitir el an\u00e1lisis y exploraci\u00f3n de cada componente del sistema por separado.</p> <ul> <li>Ejemplo: Adelgazamiento de la capa de hielo polar por calentamiento global.</li> </ul> </li> </ul>"},{"location":"machine_learning/ml_intro/#que-tipos-de-problemas-podemos-abordar-con-machine-learning","title":"\u00bf Qu\u00e9 tipos de problemas podemos abordar con machine learning?","text":"<p>Los problemas que se pueden resolver con machine learning se pueden englobar en tres tipos: aprendeizaje supervisado, aprendizaje no supervisado y aprendizaje reforzado.</p> <p></p>"},{"location":"machine_learning/ml_intro/#aprendizaje-supervisado","title":"Aprendizaje supervisado","text":"<ul> <li>El sistema aprende en base a datos estructurados o no estructurados. </li> <li>Clasificados previamente (se conoce la respuesta).</li> <li>El algoritmo produce una funci\u00f3n que establece una correspondencia entre las entradas y las salidas deseadas del sistema.</li> </ul>"},{"location":"machine_learning/ml_intro/#aprendizaje-no-supervisado","title":"Aprendizaje no supervisado","text":"<ul> <li>Modelo se construye usando un conjunto de datos como entrada, los cuales no han sido clasificados previamente.</li> <li>El sistema tiene que ser capaz de reconocer patrones para poder etiquetar las nuevas entradas.</li> </ul>"},{"location":"machine_learning/ml_intro/#aprendizaje-por-refuerzo","title":"Aprendizaje por refuerzo","text":"<p>Aprendizaje por refuerzo o Aprendizaje reforzado es un \u00e1rea del aprendizaje autom\u00e1tico inspirada en la psicolog\u00eda conductista, cuya ocupaci\u00f3n es determinar qu\u00e9 acciones debe escoger un agente de software en un entorno dado con el fin de maximizar alguna noci\u00f3n de \"recompensa\" o premio acumulado. </p> <p></p>"},{"location":"machine_learning/ml_intro/#algoritmos-mas-utilizados","title":"Algoritmos m\u00e1s utilizados","text":"<p>Los algoritmos que m\u00e1s se suelen utilizar en los problemas de Machine Learning son los siguientes:</p> <ol> <li>Regresi\u00f3n Lineal</li> <li>Regresi\u00f3n Log\u00edstica</li> <li>Arboles de Decision</li> <li>Random Forest</li> <li>SVM </li> <li>KNN </li> <li>K-means</li> </ol> <p>\u00bf Qu\u00e9 se necesita para aprender machine learning?</p> <p>Se necesita tener conocimientos de los siguientes t\u00f3picos.</p> <ul> <li>Algebra Lineal</li> <li>Probabilidad y estad\u00edstica</li> <li>Optimizaci\u00f3n</li> </ul>"},{"location":"machine_learning/ml_intro/#librerias-de-machine-learning-en-python","title":"Librer\u00edas de machine learning en python","text":"<p>Una de las grandes ventajas que ofrece python  sobre otros lenguajes de programaci\u00f3n; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librer\u00edas de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning , las principales librer\u00edas que podemos utilizar son: </p>"},{"location":"machine_learning/ml_intro/#scikit-learn","title":"Scikit-Learn","text":"<p>Scikit-learn es la principal librer\u00eda que existe para trabajar con Machine Learning, incluye la implementaci\u00f3n de un gran n\u00famero de algoritmos  de aprendizaje. La podemos utilizar para clasificaciones, extraccion de caracter\u00edsticas, regresiones, agrupaciones, reducci\u00f3n de dimensiones, selecci\u00f3n de modelos, o preprocesamiento. </p>"},{"location":"machine_learning/ml_intro/#statsmodels","title":"Statsmodels","text":"<p>Statsmodels es otra gran librer\u00eda que hace foco en modelos estad\u00edsticos y se utiliza principalmente para an\u00e1lisis predictivos y exploratorios.  Las pruebas estad\u00edsticas que ofrece son bastante amplias y abarcan tareas de validaci\u00f3n para la mayor\u00eda de los casos. </p>"},{"location":"machine_learning/ml_intro/#conceptos-claves-en-machine-learning","title":"Conceptos claves en machine learning","text":""},{"location":"machine_learning/ml_intro/#esquema-machine-learning","title":"Esquema machine learning","text":"<p>EL proceso de machine learning se puede resumir a grandes rasgo por el siguiente esquema.</p> <p></p> <ol> <li> <p>Recolectar los datos. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos. </p> </li> <li> <p>Preprocesar los datos. Es pr\u00e1cticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho m\u00e1s sencillo que el paso anterior.</p> </li> <li> <p>Explorar los datos. Una vez que ya tenemos los datos y est\u00e1n con el formato correcto, podemos realizar un pre an\u00e1lisis para corregir los casos de valores faltantes o intentar encontrar a simple vista alg\u00fan patr\u00f3n en los mismos que nos facilite la construcci\u00f3n del modelo.</p> </li> <li> <p>Entrenar el modelo. En esta etapa se entrenan los modelos con los datos que venimos procesando en las etapas anteriores. La idea es que los modelos puedan extraer informaci\u00f3n \u00fatil de los datos que le pasamos para luego poder hacer predicciones. </p> </li> <li> <p>Evaluar el modelo. Evaluamos que tan preciso es el modelo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el modelo cambiando algunos par\u00e1metros hasta lograr un rendimiento aceptable.  </p> </li> <li> <p>Utilizar el modelo. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aqu\u00ed tambi\u00e9n podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores. </p> </li> </ol> <p>Los pasos 1,2,3 son los pasos que ya se han visto con detalle en este curso. Por otro lado, la etapa de modelamiento (entrenar, evaluar y predecir) ser\u00e1 necesario introducir nuevos conceptos.</p>"},{"location":"machine_learning/ns_01/","title":"No supervisado I","text":"<p>Consid\u00e9rense  $\ud835\udc36_1 ,...,  \ud835\udc36_k$  como los sets formados por los \u00edndices de las observaciones de cada uno de los clusters. Por ejemplo, el set  $\ud835\udc36_1$  contiene los \u00edndices de las observaciones agrupadas en el cluster 1. La nomenclatura empleada para indicar que la observaci\u00f3n  $i$ pertenece al cluster  $k$  es:  $i \\in C_k$ . Todos los sets satisfacen dos propiedades:</p> <ul> <li><p>$C_1 \\cup C_2 \\cup ... \\cup C_k = {1,...,n} $ . Significa que toda observaci\u00f3n pertenece a uno de los $k$ clusters.</p> </li> <li><p>$C_i \\cap C_{j} = \\emptyset $   para todo $i \\neq j$   . Implica que los clusters no solapan, ninguna observaci\u00f3n pertenece a m\u00e1s de un cluster a la vez.</p> </li> </ul> <p>El algoritmo consiste en reducir al m\u00ednimo la suma de las distancias cuadradas desde la media dentro del agrupamiento. Matem\u00e1ticamente: \\begin{align*} (P) \\ \\textrm{Minimizar } f(C_l,\\mu_l) = \\sum_{l=1}^k \\sum_{x_n \\in C_l} ||x_n - \\mu_l ||^2 \\textrm{, respecto a } C_l, \\mu_l, \\end{align*} donde $C_l$ es el cluster l-\u00e9simo y $\\mu_l$ es el centroide l-\u00e9simo.</p> <p></p> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.datasets import make_blobs\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns   from sklearn.datasets import make_blobs  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre>def init_blobs(N, k, seed=42):\n    X, y = make_blobs(n_samples=N, centers=k,\n                      random_state=seed, cluster_std=0.60)\n    return X\n\n# generar datos\ndata = init_blobs(10000, 6, seed=43)\ndf = pd.DataFrame(data, columns=[\"x\", \"y\"])\n\n\n\ndf.head()\n</pre> def init_blobs(N, k, seed=42):     X, y = make_blobs(n_samples=N, centers=k,                       random_state=seed, cluster_std=0.60)     return X  # generar datos data = init_blobs(10000, 6, seed=43) df = pd.DataFrame(data, columns=[\"x\", \"y\"])    df.head() Out[2]: x y 0 -6.953617 -4.989933 1 -2.681117 7.583914 2 -1.510161 4.933676 3 -9.748491 5.479457 4 -7.438017 -4.597754 <p>Debido a que trabajamos con el concepto de distancia, muchas veces las columnas del dataframe pueden estar en distintas escalas, lo cual puede complicar a los algoritmos ocupados (al menos con sklearn).</p> <p>En estos casos, se suele normalizar los atributos, es decir, dejar los valores en una escala acotada y/o con estimadores fijos. Por ejemplo, en ***sklearn** podemos encontrar las siguientes formas de normalizar:</p> <ul> <li>StandardScaler: se normaliza  restando la media y escalando por su desviaci\u00f3n estanda. $$x_{prep} = \\dfrac{x-u}{s}$$</li> </ul> <p>La ventaja es que la media del nuevo conjunto de datos cumple con la propiedad que su media $\\mu$ es igual a cero y su desviaci\u00f3n estandar $s$ es igual a 1.</p> <ul> <li>MinMaxScaler:  se normaliza ocupando los valores de los m\u00ednimos y m\u00e1ximo del conjunto de datos. $$x_{prep} = \\dfrac{x-x_{min}}{x_{min}-x_{max}}$$</li> </ul> <p>Esta forma de normalizar resulta \u00fatil cuando la desviaci\u00f3n estandar $s$ es muy peque\u00f1a (cercana) a cero, por lo que lo convierte en un estimador m\u00e1s roubusto que el StandardScaler.</p> In\u00a0[3]: Copied! <pre>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> from sklearn.preprocessing import StandardScaler  scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[3]: x y 0 -0.579033 -1.831435 1 0.408821 1.194578 2 0.679560 0.556774 3 -1.225241 0.688121 4 -0.691032 -1.737053 In\u00a0[4]: Copied! <pre># comprobar resultados del estimador\ndf.describe()\n</pre> # comprobar resultados del estimador df.describe() Out[4]: x y count 1.000000e+04 1.000000e+04 mean 2.060574e-16 -2.285105e-15 std 1.000050e+00 1.000050e+00 min -1.638247e+00 -2.410317e+00 25% -8.015576e-01 -4.418042e-01 50% -2.089351e-01 1.863259e-01 75% 5.480066e-01 8.159808e-01 max 2.243358e+00 1.639547e+00 <p>Con esta parametrizaci\u00f3n procedemos a graficar nuestros resultados:</p> In\u00a0[5]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") <p>Ahora ajustamos el algoritmo KMeans de sklearn. Primero, comprendamos los hiperpar\u00e1metros m\u00e1s importantes:</p> <ul> <li>n_clusters: El n\u00famero de clusters a crear, o sea K. Por defecto es 8</li> <li>init: M\u00e9todo de inicializaci\u00f3n. Un problema que tiene el algoritmo K-Medias es que la solucci\u00f3n alcanzada varia seg\u00fan la inicializaci\u00f3n de los centroides. <code>sklearn</code> empieza usando el m\u00e9todo <code>kmeans++</code> que es una versi\u00f3n m\u00e1s moderna y que proporciona mejores resultados que la inicializaci\u00f3n aleatoria (random)</li> <li>n_init: El n\u00famero de inicializaciones a probar. B\u00e1sicamente <code>KMeans</code> aplica el algoritmo <code>n_init</code> veces y elige los clusters que minimizan la inercia.</li> <li>max_iter: M\u00e1ximo n\u00famero de iteraciones para llegar al criterio de parada.</li> <li>random_state: semilla para garantizar la reproducibilidad de los resultados.</li> <li>tol: Tolerancia para declarar criterio de parada (cuanto m\u00e1s grande, antes parar\u00e1 el algoritmo).</li> </ul> In\u00a0[6]: Copied! <pre># ajustar modelo: k-means\n\nfrom sklearn.cluster import KMeans\n\nX = np.array(df)\nkmeans = KMeans(n_clusters=6,n_init=25, random_state=123)\nkmeans.fit(X)\n\n\ncentroids = kmeans.cluster_centers_ # centros \nclusters = kmeans.labels_ # clusters\n</pre> # ajustar modelo: k-means  from sklearn.cluster import KMeans  X = np.array(df) kmeans = KMeans(n_clusters=6,n_init=25, random_state=123) kmeans.fit(X)   centroids = kmeans.cluster_centers_ # centros  clusters = kmeans.labels_ # clusters In\u00a0[7]: Copied! <pre># etiquetar los datos con los clusters encontrados\ndf[\"cluster\"] = clusters\ndf[\"cluster\"] = df[\"cluster\"].astype('category')\ncentroids_df = pd.DataFrame(centroids, columns=[\"x\", \"y\"])\ncentroids_df[\"cluster\"] = [1,2,3,4,5,6]\n</pre> # etiquetar los datos con los clusters encontrados df[\"cluster\"] = clusters df[\"cluster\"] = df[\"cluster\"].astype('category') centroids_df = pd.DataFrame(centroids, columns=[\"x\", \"y\"]) centroids_df[\"cluster\"] = [1,2,3,4,5,6] In\u00a0[8]: Copied! <pre># graficar los datos etiquetados con k-means\nfig, ax = plt.subplots(figsize=(11, 8.5))\n\nsns.scatterplot( data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     hue=\"cluster\",\n                     legend='full',\n                     palette=\"Set2\")\n\nsns.scatterplot(x=\"x\", y=\"y\",\n                     s=100, color=\"black\", marker=\"x\",\n                     data=centroids_df)\nplt.show()\n</pre> # graficar los datos etiquetados con k-means fig, ax = plt.subplots(figsize=(11, 8.5))  sns.scatterplot( data=df,                      x=\"x\",                      y=\"y\",                      hue=\"cluster\",                      legend='full',                      palette=\"Set2\")  sns.scatterplot(x=\"x\", y=\"y\",                      s=100, color=\"black\", marker=\"x\",                      data=centroids_df) plt.show() <p>Ahora la pregunta que surge de manera natural es ... \u00bf c\u00f3mo escoger el mejor n\u00famero de clusters?.</p> <p>No existe un criterio objetivo ni ampliamente v\u00e1lido para la  elecci\u00f3n de un n\u00famero \u00f3ptimo de clusters. Aunque no exista un criterio objetivo para la selecci\u00f3n del n\u00famero de clusters, si que se han implementado diferentes m\u00e9todos que nos ayudan a elegir un n\u00famero apropiado de clusters para agrupar los datos; como son,</p> <ul> <li>m\u00e9todo del codo (elbow method)</li> <li>criterio de Calinsky</li> <li>Affinity Propagation (AP)</li> <li>Gap (tambi\u00e9n con su versi\u00f3n estad\u00edstica)</li> <li>Dendrogramas</li> <li>etc.</li> </ul> In\u00a0[9]: Copied! <pre># implementaci\u00f3n de la regla del codo\nNc = range(1, 15)\nkmeans = [KMeans(n_clusters=i, n_init=10) for i in Nc]  # Suppressing the warning here\nscore = [kmeans[i].fit(df).inertia_ for i in range(len(kmeans))]\n\ndf_Elbow = pd.DataFrame({'Number of Clusters': Nc, 'Score': score})\n\ndf_Elbow.head()\n</pre> # implementaci\u00f3n de la regla del codo Nc = range(1, 15) kmeans = [KMeans(n_clusters=i, n_init=10) for i in Nc]  # Suppressing the warning here score = [kmeans[i].fit(df).inertia_ for i in range(len(kmeans))]  df_Elbow = pd.DataFrame({'Number of Clusters': Nc, 'Score': score})  df_Elbow.head() Out[9]: Number of Clusters Score 0 1 49054.876400 1 2 23034.738275 2 3 11952.113532 3 4 6562.349711 4 5 1665.378832 In\u00a0[10]: Copied! <pre># graficar los datos etiquetados con k-means\nfig, ax = plt.subplots(figsize=(11, 8.5))\nplt.title('Elbow Curve')\nsns.lineplot(x=\"Number of Clusters\",\n             y=\"Score\",\n            data=df_Elbow)\nsns.scatterplot(x=\"Number of Clusters\",\n             y=\"Score\",\n             data=df_Elbow)\nplt.show()\n</pre> # graficar los datos etiquetados con k-means fig, ax = plt.subplots(figsize=(11, 8.5)) plt.title('Elbow Curve') sns.lineplot(x=\"Number of Clusters\",              y=\"Score\",             data=df_Elbow) sns.scatterplot(x=\"Number of Clusters\",              y=\"Score\",              data=df_Elbow) plt.show() <p>A partir de 4 clusters la reducci\u00f3n en la suma total de cuadrados internos parece estabilizarse, indicando que $k$ = 4 es una buena opci\u00f3n.</p> <p>En la base del dendrograma, cada observaci\u00f3n forma una terminaci\u00f3n individual conocida como hoja o leaf del \u00e1rbol. A medida que se asciende por la estructura, pares de hojas se fusionan formando las primeras ramas. Estas uniones se corresponden con los pares de observaciones m\u00e1s similares. Tambi\u00e9n ocurre que las ramas se fusionan con otras ramas o con hojas. Cuanto m\u00e1s temprana (m\u00e1s pr\u00f3xima a la base del dendrograma) ocurre una fusi\u00f3n, mayor es la similitud.</p> <p>Para cualquier par de observaciones, se puede identificar el punto del \u00e1rbol en el que las ramas que contienen dichas observaciones se fusionan. La altura a la que esto ocurre (eje vertical) indica c\u00f3mo de similares/diferentes son las dos observaciones. Los dendrogramas, por lo tanto, se deben interpretar \u00fanicamente en base al eje vertical y no por las posiciones que ocupan las observaciones en el eje horizontal, esto \u00faltimo es simplemente por est\u00e9tica y puede variar de un programa a otro.</p> <p>Por ejemplo, la observaci\u00f3n 8 es la m\u00e1s similar a la 10 ya que es la primera fusi\u00f3n que recibe la observaci\u00f3n 10 (y viceversa). Podr\u00eda resultar tentador decir que la observaci\u00f3n 14, situada inmediatamente a la derecha de la 10, es la siguiente m\u00e1s similar, sin embargo, las observaciones 28 y 44 son m\u00e1s similares a la 10 a pesar de que se encuentran m\u00e1s alejadas en el eje horizontal. Del mismo modo, no es correcto decir que la observaci\u00f3n 14 es m\u00e1s similar a la observaci\u00f3n 10 de lo que lo es la 36 por el hecho de que est\u00e1 m\u00e1s pr\u00f3xima en el eje horizontal. Prestando atenci\u00f3n a la altura en que las respectivas ramas se unen, la \u00fanica conclusi\u00f3n v\u00e1lida es que la similitud entre los pares 10-14 y 10-36 es la misma.</p> <p>Cortar el dendograma para generar los clusters</p> <p>Adem\u00e1s de representar en un dendrograma la similitud entre observaciones, se tiene que identificar el n\u00famero de clusters creados y qu\u00e9 observaciones forman parte de cada uno. Si se realiza un corte horizontal a una determinada altura del dendrograma, el n\u00famero de ramas que sobrepasan (en sentido ascendente) dicho corte se corresponde con el n\u00famero de clusters. La siguiente imagen muestra dos veces el mismo dendrograma. Si se realiza el corte a la altura de 5, se obtienen dos clusters, mientras que si se hace a la de 3.5 se obtienen 4. La altura de corte tiene por lo tanto la misma funci\u00f3n que el valor K en K-means-clustering: controla el n\u00famero de clusters obtenidos.</p> <p></p> <p></p> <p>Dos propiedades adicionales se derivan de la forma en que se generan los clusters en el m\u00e9todo de hierarchical clustering:</p> <ul> <li><p>Dada la longitud variable de las ramas, siempre existe un intervalo de altura para el que cualquier corte da lugar al mismo n\u00famero de clusters. En el ejemplo anterior, todos los cortes entre las alturas 5 y 6 tienen como resultado los mismos 2 clusters.</p> </li> <li><p>Con un solo dendrograma se dispone de la flexibilidad para generar cualquier n\u00famero de clusters desde 1 a n. La selecci\u00f3n del n\u00famero \u00f3ptimo puede valorarse de forma visual, tratando de identificar las ramas principales en base a la altura a la que ocurren las uniones. En el ejemplo expuesto es razonable elegir entre 2 o 4 clusters.</p> </li> </ul> In\u00a0[11]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.cluster import AgglomerativeClustering from scipy.cluster.hierarchy import dendrogram from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[12]: Copied! <pre># generar datos\nX, y = make_blobs(\n        n_samples    = 200, \n        n_features   = 2, \n        centers      = 4, \n        cluster_std  = 0.60, \n        shuffle      = True, \n        random_state = 0\n       )\n\n\ndf = pd.DataFrame({\n    'x':X[:,0],\n    'y':X[:,1]\n})\n\n\n# Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> # generar datos X, y = make_blobs(         n_samples    = 200,          n_features   = 2,          centers      = 4,          cluster_std  = 0.60,          shuffle      = True,          random_state = 0        )   df = pd.DataFrame({     'x':X[:,0],     'y':X[:,1] })   # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[12]: x y 0 1.348818 -0.908114 1 -0.638621 -0.534950 2 0.653079 0.027910 3 -1.573023 1.276049 4 0.970706 -1.418431 In\u00a0[13]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[14]: Copied! <pre># Modelos\n\nX = np.array(df[['x','y']])\n</pre> # Modelos  X = np.array(df[['x','y']]) In\u00a0[15]: Copied! <pre># primer modelo\nmodelo_hclust_complete = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'complete',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                        )\nmodelo_hclust_complete.fit(X)\n</pre> # primer modelo modelo_hclust_complete = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'complete',                             distance_threshold = 0,                             n_clusters         = None                         ) modelo_hclust_complete.fit(X) Out[15]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='complete', n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='complete', n_clusters=None)</pre> In\u00a0[16]: Copied! <pre># segundo modelo\nmodelo_hclust_average = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'average',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                        )\nmodelo_hclust_average.fit(X)\n</pre> # segundo modelo modelo_hclust_average = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'average',                             distance_threshold = 0,                             n_clusters         = None                         ) modelo_hclust_average.fit(X) Out[16]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='average', n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        linkage='average', n_clusters=None)</pre> In\u00a0[17]: Copied! <pre># tercer modelo\nmodelo_hclust_ward = AgglomerativeClustering(\n                            affinity = 'euclidean',\n                            linkage  = 'ward',\n                            distance_threshold = 0,\n                            n_clusters         = None\n                     )\nmodelo_hclust_ward.fit(X)\n</pre> # tercer modelo modelo_hclust_ward = AgglomerativeClustering(                             affinity = 'euclidean',                             linkage  = 'ward',                             distance_threshold = 0,                             n_clusters         = None                      ) modelo_hclust_ward.fit(X) Out[17]: <pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        n_clusters=None)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AgglomerativeClustering<pre>AgglomerativeClustering(affinity='euclidean', distance_threshold=0,\n                        n_clusters=None)</pre> In\u00a0[18]: Copied! <pre>def plot_dendrogram(model, **kwargs):\n    '''\n    Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering\n    y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy\n    '''\n    \n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx &lt; n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot\n    dendrogram(linkage_matrix, **kwargs)\n</pre> def plot_dendrogram(model, **kwargs):     '''     Esta funci\u00f3n extrae la informaci\u00f3n de un modelo AgglomerativeClustering     y representa su dendograma con la funci\u00f3n dendogram de scipy.cluster.hierarchy     '''          counts = np.zeros(model.children_.shape[0])     n_samples = len(model.labels_)     for i, merge in enumerate(model.children_):         current_count = 0         for child_idx in merge:             if child_idx &lt; n_samples:                 current_count += 1  # leaf node             else:                 current_count += counts[child_idx - n_samples]         counts[i] = current_count      linkage_matrix = np.column_stack([model.children_, model.distances_,                                       counts]).astype(float)      # Plot     dendrogram(linkage_matrix, **kwargs) In\u00a0[19]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_average, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage average\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_average, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage average\") plt.show() In\u00a0[20]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_complete, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage complete\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_complete, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage complete\") plt.show() In\u00a0[21]: Copied! <pre># Dendrogramas\nplt.figure(figsize=(20,10)) \nplot_dendrogram(modelo_hclust_ward, color_threshold=0)\nplt.title(\"Distancia eucl\u00eddea, Linkage ward\")\nplt.show()\n</pre> # Dendrogramas plt.figure(figsize=(20,10))  plot_dendrogram(modelo_hclust_ward, color_threshold=0) plt.title(\"Distancia eucl\u00eddea, Linkage ward\") plt.show() <p>En este caso, los tres tipos de linkage identifican claramente 4 clusters, si bien esto no significa que en los 3 dendrogramas los clusters est\u00e9n formados por exactamente las mismas observaciones.</p> <p>N\u00famero de clusters</p> <p>Una forma de identificar el n\u00famero de clusters, es inspeccionar visualmente el dendograma y decidir a qu\u00e9 altura se corta para generar los clusters. Por ejemplo, para los resultados generados mediante distancia eucl\u00eddea y linkage ward, parece sensato cortar el dendograma a una altura de entre 5 y 10, de forma que se creen 4 clusters.</p> In\u00a0[22]: Copied! <pre>plt.figure(figsize=(20,10)) \naltura_corte = 6\nplot_dendrogram(modelo_hclust_ward, color_threshold=altura_corte)\nplt.title(\"Distancia eucl\u00eddea, Linkage ward\")\nplt.axhline(y=altura_corte, c = 'black', linestyle='--', label='altura corte')\nplt.legend()\nplt.show()\n</pre> plt.figure(figsize=(20,10))  altura_corte = 6 plot_dendrogram(modelo_hclust_ward, color_threshold=altura_corte) plt.title(\"Distancia eucl\u00eddea, Linkage ward\") plt.axhline(y=altura_corte, c = 'black', linestyle='--', label='altura corte') plt.legend() plt.show() <p>Una vez identificado el n\u00famero \u00f3ptimo de clusters, se reentrena el modelo indicando este valor.</p> <p>El cerebro humano identifica f\u00e1cilmente 5 agrupaciones y algunas observaciones aisladas (ruido). V\u00e9anse ahora los clusters que se obtienen si se aplica, por ejemplo, K-means clustering.</p> <p></p> <p>Los clusters generados distan mucho de representar las verdaderas agrupaciones. Esto es as\u00ed porque los m\u00e9todos de partitioning clustering como k-means, hierarchical, k-medoids, ... son buenos encontrando agrupaciones con forma esf\u00e9rica o convexa que no contengan un exceso de outliers o ruido, pero fallan al tratar de identificar formas arbitrarias. De ah\u00ed que el \u00fanico cluster que se corresponde con un grupo real sea el amarillo.</p> <p>DBSCAN evita este problema siguiendo la idea de que, para que una observaci\u00f3n forme parte de un cluster, tiene que haber un m\u00ednimo de observaciones vecinas dentro de un radio de proximidad y de que los clusters est\u00e1n separados por regiones vac\u00edas o con pocas observaciones.</p> <p>El algoritmo DBSCAN necesita dos par\u00e1metros:</p> <ul> <li>Epsilon  ($\\epsilon$) : radio que define la regi\u00f3n vecina a una observaci\u00f3n, tambi\u00e9n llamada  \ud835\udf16 -neighborhood.</li> <li>Minimum points ($min_samples$): n\u00famero m\u00ednimo de observaciones dentro de la regi\u00f3n epsilon.</li> </ul> <p>Empleando estos dos par\u00e1metros, cada observaci\u00f3n del set de datos se puede clasificar en una de las siguientes tres categor\u00edas:</p> <ul> <li><p>Core point: observaci\u00f3n que tiene en su  \ud835\udf16 -neighborhood un n\u00famero de observaciones vecinas igual o mayor a min_samples.</p> </li> <li><p>Border point: observaci\u00f3n no satisface el m\u00ednimo de observaciones vecinas para ser core point pero que pertenece al  $\\epsilon$-neighborhood de otra observaci\u00f3n que s\u00ed es core point.</p> </li> <li><p>Noise-outlier: observaci\u00f3n que no es core point ni border point.</p> </li> </ul> <p>Por \u00faltimo, empleando las tres categor\u00edas anteriores se pueden definir tres niveles de conectividad entre observaciones:</p> <ul> <li><p>Directamente alcanzable (direct density reachable): una observaci\u00f3n  $A$  es directamente alcanzable desde otra observaci\u00f3n  $B$  si  $A$  forma parte del  $\\epsilon$ -neighborhood de  $B$  y  $B$  es un core point. Por definici\u00f3n, las observaciones solo pueden ser directamente alcanzables desde un core point.</p> </li> <li><p>Alcanzable (density reachable): una observaci\u00f3n  $A$  es alcanzable desde otra observaci\u00f3n  $\ud835\udc35$  si existe una secuencia de core points que van desde  $B$  a  $A$ .</p> </li> <li><p>Densamente conectadas (density conected): dos observaciones $A$  y  $B$  est\u00e1n densamente conectadas si existe una observaci\u00f3n core point  $C$  tal que  $A$  y  $B$  son alcanzables desde  $C$ .</p> </li> </ul> <p>La siguiente imagen muestra las conexiones existentes entre un conjunto de observaciones si se emplea  $min_samples = 4$ . La observaci\u00f3n  $A$  y el resto de observaciones marcadas en rojo son core points, ya que todas ellas contienen al menos 4 observaciones vecinas (incluy\u00e9ndose a ellas mismas) en su  $\\epsilon$-neighborhood. Como todas son alcanzables entre ellas, forman un cluster. Las observaciones  $B$  y  $C$  no son core points pero son alcanzables desde  $A$  a trav\u00e9s de otros core points, por lo tanto, pertenecen al mismo cluster que  $A$ . La observaci\u00f3n  $N$  no es ni un core point ni es directamente alcanzable, por lo que se considera como ruido.</p> <p></p> In\u00a0[23]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.cluster import DBSCAN from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[24]: Copied! <pre># leer datos\nurl='https://drive.google.com/file/d/1sLxYBCZJawCHyxEjHnn-hnBDInXx4hby/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\",\")\ndf.head()\n</pre> # leer datos url='https://drive.google.com/file/d/1sLxYBCZJawCHyxEjHnn-hnBDInXx4hby/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\",\") df.head() Out[24]: x y shape 0 -0.803739 -0.853053 1 1 0.852851 0.367618 1 2 0.927180 -0.274902 1 3 -0.752626 -0.511565 1 4 0.706846 0.810679 1 In\u00a0[25]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[26]: Copied! <pre># Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\ndf.head()\n</pre> # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns]) df.head() Out[26]: x y shape 0 -1.120749 -0.193616 1 1 1.448907 0.844692 1 2 1.564203 0.298161 1 3 -1.041463 0.096855 1 4 1.222429 1.221561 1 In\u00a0[27]: Copied! <pre># Modelo\nX = np.array(df[['x','y']])\n\nmodelo_dbscan = DBSCAN(\n                    eps          = 0.2,\n                    min_samples  = 5,\n                    metric       = 'euclidean',\n                )\n\n\nmodelo_dbscan.fit(X)\n</pre> # Modelo X = np.array(df[['x','y']])  modelo_dbscan = DBSCAN(                     eps          = 0.2,                     min_samples  = 5,                     metric       = 'euclidean',                 )   modelo_dbscan.fit(X) Out[27]: <pre>DBSCAN(eps=0.2)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DBSCAN<pre>DBSCAN(eps=0.2)</pre> In\u00a0[28]: Copied! <pre># agregar labels\ndf['labels'] = modelo_dbscan.labels_\n</pre> # agregar labels df['labels'] = modelo_dbscan.labels_ In\u00a0[29]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\",hue = \"labels\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\",hue = \"labels\") In\u00a0[30]: Copied! <pre># N\u00famero de clusters y observaciones \"outliers\"\nn_clusters = len(set(df['labels'])) - (1 if -1 in df['labels'] else 0)\nn_noise    = list(df['labels']).count(-1)\n\nprint(f'N\u00famero de clusters encontrados: {n_clusters}')\nprint(f'N\u00famero de outliers encontrados: {n_noise}')\n</pre> # N\u00famero de clusters y observaciones \"outliers\" n_clusters = len(set(df['labels'])) - (1 if -1 in df['labels'] else 0) n_noise    = list(df['labels']).count(-1)  print(f'N\u00famero de clusters encontrados: {n_clusters}') print(f'N\u00famero de outliers encontrados: {n_noise}') <pre>N\u00famero de clusters encontrados: 6\nN\u00famero de outliers encontrados: 25\n</pre> In\u00a0[31]: Copied! <pre># Tratamiento de datos\n# ==============================================================================\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\n\n# Gr\u00e1ficos\n# ==============================================================================\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.patches import Ellipse\nfrom matplotlib import style\nstyle.use('ggplot') or plt.style.use('ggplot')\n\n# Preprocesado y modelado\n# ==============================================================================\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import silhouette_score\n\n# Configuraci\u00f3n warnings\n# ==============================================================================\nimport warnings\nwarnings.filterwarnings('ignore')\n</pre> # Tratamiento de datos # ============================================================================== import numpy as np import pandas as pd from sklearn.datasets import make_blobs  # Gr\u00e1ficos # ============================================================================== import matplotlib.pyplot as plt import matplotlib as mpl from matplotlib.patches import Ellipse from matplotlib import style style.use('ggplot') or plt.style.use('ggplot')  # Preprocesado y modelado # ============================================================================== from sklearn.mixture import GaussianMixture from sklearn.preprocessing import scale from sklearn.metrics import silhouette_score  # Configuraci\u00f3n warnings # ============================================================================== import warnings warnings.filterwarnings('ignore') In\u00a0[32]: Copied! <pre># generar datos\nX, y = make_blobs(\n        n_samples    = 300, \n        n_features   = 2, \n        centers      = 4, \n        cluster_std  = 0.60, \n        shuffle      = True, \n        random_state = 0\n       )\n\n\ndf = pd.DataFrame({\n    'x':X[:,0],\n    'y':X[:,1]\n})\n\n\n# Escalado de datos\nscaler = StandardScaler()\ncolumns = ['x', 'y']\ndf[columns] = scaler.fit_transform(df[columns])\n\ndf.head()\n</pre> # generar datos X, y = make_blobs(         n_samples    = 300,          n_features   = 2,          centers      = 4,          cluster_std  = 0.60,          shuffle      = True,          random_state = 0        )   df = pd.DataFrame({     'x':X[:,0],     'y':X[:,1] })   # Escalado de datos scaler = StandardScaler() columns = ['x', 'y'] df[columns] = scaler.fit_transform(df[columns])  df.head() Out[32]: x y 0 0.516255 -0.707227 1 -0.861664 1.329068 2 0.711174 0.437049 3 -0.619792 1.485573 4 0.782282 -0.801378 In\u00a0[33]: Copied! <pre># graficar \nsns.set(rc={'figure.figsize':(11.7,8.27)})\nax = sns.scatterplot( data=df,x=\"x\", y=\"y\")\n</pre> # graficar  sns.set(rc={'figure.figsize':(11.7,8.27)}) ax = sns.scatterplot( data=df,x=\"x\", y=\"y\") In\u00a0[34]: Copied! <pre># Modelo\n\nX = np.array(df[['x','y']])\nmodelo_gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=123)\nmodelo_gmm.fit(X=X)\n</pre> # Modelo  X = np.array(df[['x','y']]) modelo_gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=123) modelo_gmm.fit(X=X) Out[34]: <pre>GaussianMixture(n_components=4, random_state=123)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianMixture<pre>GaussianMixture(n_components=4, random_state=123)</pre> In\u00a0[35]: Copied! <pre># Media de cada componente\nmodelo_gmm.means_\n</pre> # Media de cada componente modelo_gmm.means_ Out[35]: <pre>array([[ 0.57844185,  0.17292982],\n       [ 1.2180002 , -1.19725866],\n       [-0.96910551, -0.44143927],\n       [-0.83710796,  1.46219241]])</pre> In\u00a0[36]: Copied! <pre># Matriz de covarianza de cada componente\nmodelo_gmm.covariances_\n</pre> # Matriz de covarianza de cada componente modelo_gmm.covariances_ Out[36]: <pre>array([[[ 0.14277634, -0.00527707],\n        [-0.00527707,  0.05201453]],\n\n       [[ 0.12745218, -0.00619666],\n        [-0.00619666,  0.05157763]],\n\n       [[ 0.12131004,  0.00243031],\n        [ 0.00243031,  0.04602115]],\n\n       [[ 0.15451151,  0.0068188 ],\n        [ 0.0068188 ,  0.05660383]]])</pre> <p>Predicci\u00f3n y clasificaci\u00f3n</p> <p>Una vez entrenado el modelo GMMs, se puede predecir la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada una de las componentes (clusters). Para obtener la clasificaci\u00f3n final, se asigna a la componente con mayor probabilidad</p> In\u00a0[37]: Copied! <pre># Probabilidades\n# ==============================================================================\n# Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a\n# cada una de las componentes.\nprobabilidades = modelo_gmm.predict_proba(X)\nprobabilidades\n</pre> # Probabilidades # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. probabilidades = modelo_gmm.predict_proba(X) probabilidades Out[37]: <pre>array([[2.59319058e-02, 9.71686641e-01, 2.38145325e-03, 8.05199375e-21],\n       [7.16006205e-09, 7.13831446e-33, 2.34989165e-15, 9.99999993e-01],\n       [9.99999970e-01, 8.78380305e-12, 9.13663813e-09, 2.04805600e-08],\n       ...,\n       [9.99965889e-01, 4.92493619e-10, 3.41016281e-05, 8.75675460e-09],\n       [3.01319652e-06, 6.45628361e-30, 1.52897049e-18, 9.99996987e-01],\n       [4.39337172e-07, 1.99785604e-11, 9.99999561e-01, 4.05381245e-15]])</pre> In\u00a0[38]: Copied! <pre># Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad)\n# ==============================================================================\n# Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a\n# cada una de las componentes.\nclasificacion = modelo_gmm.predict(X)\nclasificacion\n</pre> # Clasificaci\u00f3n (asignaci\u00f3n a la componente de mayor probabilidad) # ============================================================================== # Cada fila es una observaci\u00f3n y cada columna la probabilidad de pertenecer a # cada una de las componentes. clasificacion = modelo_gmm.predict(X) clasificacion Out[38]: <pre>array([1, 3, 0, 3, 1, 1, 2, 0, 3, 3, 2, 3, 0, 3, 1, 0, 0, 1, 2, 2, 1, 1,\n       0, 2, 2, 0, 1, 0, 2, 0, 3, 3, 0, 3, 3, 3, 3, 3, 2, 1, 0, 2, 0, 0,\n       2, 2, 3, 2, 3, 1, 2, 1, 3, 1, 1, 2, 3, 2, 3, 1, 3, 0, 3, 2, 2, 2,\n       3, 1, 3, 2, 0, 2, 3, 2, 2, 3, 2, 0, 1, 3, 1, 0, 1, 1, 3, 0, 1, 0,\n       3, 3, 0, 1, 3, 2, 2, 0, 1, 1, 0, 2, 3, 1, 3, 1, 0, 1, 1, 0, 3, 0,\n       2, 2, 1, 3, 1, 0, 3, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 3, 2,\n       2, 1, 3, 2, 2, 3, 0, 3, 3, 2, 0, 2, 0, 2, 3, 0, 3, 3, 3, 0, 3, 0,\n       1, 2, 3, 2, 1, 0, 3, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 3, 1, 0, 2, 3,\n       1, 1, 0, 2, 1, 0, 2, 2, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 2, 2, 2, 0,\n       2, 3, 0, 2, 1, 2, 0, 3, 2, 3, 0, 3, 0, 2, 0, 0, 3, 2, 2, 1, 1, 0,\n       3, 1, 1, 2, 1, 2, 0, 3, 3, 0, 0, 3, 0, 1, 2, 0, 1, 2, 3, 2, 1, 0,\n       1, 3, 3, 3, 3, 2, 2, 3, 0, 2, 1, 0, 2, 2, 2, 1, 1, 3, 0, 0, 2, 1,\n       3, 2, 0, 3, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0, 3, 3, 1, 1, 0, 1, 1, 1,\n       3, 2, 3, 0, 1, 1, 3, 3, 3, 1, 1, 0, 3, 2], dtype=int64)</pre> In\u00a0[39]: Copied! <pre># Representaci\u00f3n gr\u00e1fica\n# ==============================================================================\n# Codigo obtenido de:\n# https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models \ndef make_ellipses(gmm, ax):\n    for n in range(gmm.n_components):\n        if gmm.covariance_type == 'full':\n            covariances = gmm.covariances_[n]\n        elif gmm.covariance_type == 'tied':\n            covariances = gmm.covariances_\n        elif gmm.covariance_type == 'diag':\n            covariances = np.diag(gmm.covariances_[n])\n        elif gmm.covariance_type == 'spherical':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        \n        for i in range(1,3):\n            ell = mpl.patches.Ellipse(gmm.means_[n], i*v[0], i*v[1],\n                                      180 + angle, color=\"blue\")\n            ell.set_clip_box(ax.bbox)\n            ell.set_alpha(0.1)\n            ax.add_artist(ell)\n</pre> # Representaci\u00f3n gr\u00e1fica # ============================================================================== # Codigo obtenido de: # https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models  def make_ellipses(gmm, ax):     for n in range(gmm.n_components):         if gmm.covariance_type == 'full':             covariances = gmm.covariances_[n]         elif gmm.covariance_type == 'tied':             covariances = gmm.covariances_         elif gmm.covariance_type == 'diag':             covariances = np.diag(gmm.covariances_[n])         elif gmm.covariance_type == 'spherical':             covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]         v, w = np.linalg.eigh(covariances)         u = w[0] / np.linalg.norm(w[0])         angle = np.arctan2(u[1], u[0])         angle = 180 * angle / np.pi  # convert to degrees         v = 2. * np.sqrt(2.) * np.sqrt(v)                  for i in range(1,3):             ell = mpl.patches.Ellipse(gmm.means_[n], i*v[0], i*v[1],                                       180 + angle, color=\"blue\")             ell.set_clip_box(ax.bbox)             ell.set_alpha(0.1)             ax.add_artist(ell)          In\u00a0[40]: Copied! <pre>fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\n# Distribuci\u00f3n de probabilidad de cada componente\nfor i in np.unique(clasificacion):\n    axs[0].scatter(\n        x = X[clasificacion == i, 0],\n        y = X[clasificacion == i, 1], \n        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n        marker    = 'o',\n        edgecolor = 'black', \n        label= f\"Componente {i}\"\n    )\n\nmake_ellipses(modelo_gmm, ax = axs[0])\naxs[0].set_title('Distribuci\u00f3n de prob. de cada componente')\naxs[0].legend()\n\n\n# Distribuci\u00f3n de probabilidad del modelo completo\nxs = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000)\nys = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000)\nxx, yy = np.meshgrid(xs, ys)\nscores = modelo_gmm.score_samples(np.c_[xx.ravel(), yy.ravel()], )\naxs[1].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.tab10(clasificacion))\nscores = np.exp(scores) # Las probabilidades est\u00e1n en log\naxs[1].contour(\n    xx, yy, scores.reshape(xx.shape),\n    levels=np.percentile(scores, np.linspace(0, 100, 10))[1:-1]\n)\naxs[1].set_title('Distribuci\u00f3n de prob. del modelo completo');\n</pre> fig, axs = plt.subplots(1, 2, figsize=(15, 5))  # Distribuci\u00f3n de probabilidad de cada componente for i in np.unique(clasificacion):     axs[0].scatter(         x = X[clasificacion == i, 0],         y = X[clasificacion == i, 1],          c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],         marker    = 'o',         edgecolor = 'black',          label= f\"Componente {i}\"     )  make_ellipses(modelo_gmm, ax = axs[0]) axs[0].set_title('Distribuci\u00f3n de prob. de cada componente') axs[0].legend()   # Distribuci\u00f3n de probabilidad del modelo completo xs = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000) ys = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000) xx, yy = np.meshgrid(xs, ys) scores = modelo_gmm.score_samples(np.c_[xx.ravel(), yy.ravel()], ) axs[1].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.tab10(clasificacion)) scores = np.exp(scores) # Las probabilidades est\u00e1n en log axs[1].contour(     xx, yy, scores.reshape(xx.shape),     levels=np.percentile(scores, np.linspace(0, 100, 10))[1:-1] ) axs[1].set_title('Distribuci\u00f3n de prob. del modelo completo'); <p>N\u00famero de clusters</p> <p>Dado que los modelos GMM son modelos probabil\u00edsticos, se puede recurrir a m\u00e9tricas como el Akaike information criterion (AIC) o Bayesian information criterion (BIC) para identificar c\u00f3mo de bien se ajustan los datos observados a modelo creado.</p> In\u00a0[41]: Copied! <pre>n_components = range(1, 21)\nvalores_bic = []\nvalores_aic = []\n\nfor i in n_components:\n    modelo = GaussianMixture(n_components=i, covariance_type=\"full\")\n    modelo = modelo.fit(X)\n    valores_bic.append(modelo.bic(X))\n    valores_aic.append(modelo.aic(X))\n\nfig, ax = plt.subplots(1, 1, figsize=(12,6))\nax.plot(n_components, valores_bic, label='BIC')\nax.plot(n_components, valores_aic, label='AIC')\nax.set_title(\"Valores BIC y AIC\")\nax.set_xlabel(\"N\u00famero componentes\")\nax.legend();\n</pre> n_components = range(1, 21) valores_bic = [] valores_aic = []  for i in n_components:     modelo = GaussianMixture(n_components=i, covariance_type=\"full\")     modelo = modelo.fit(X)     valores_bic.append(modelo.bic(X))     valores_aic.append(modelo.aic(X))  fig, ax = plt.subplots(1, 1, figsize=(12,6)) ax.plot(n_components, valores_bic, label='BIC') ax.plot(n_components, valores_aic, label='AIC') ax.set_title(\"Valores BIC y AIC\") ax.set_xlabel(\"N\u00famero componentes\") ax.legend(); In\u00a0[42]: Copied! <pre>print(f\"N\u00famero \u00f3ptimo acorde al BIC: {range(1, 21)[np.argmin(valores_bic)]}\")\nprint(f\"N\u00famero \u00f3ptimo acorde al AIC: {range(1, 21)[np.argmin(valores_aic)]}\")\n</pre> print(f\"N\u00famero \u00f3ptimo acorde al BIC: {range(1, 21)[np.argmin(valores_bic)]}\") print(f\"N\u00famero \u00f3ptimo acorde al AIC: {range(1, 21)[np.argmin(valores_aic)]}\") <pre>N\u00famero \u00f3ptimo acorde al BIC: 4\nN\u00famero \u00f3ptimo acorde al AIC: 4\n</pre> <p>Ambas m\u00e9tricas identifican el 4 como n\u00famero \u00f3ptimo de clusters (componentes).</p>"},{"location":"machine_learning/ns_01/#no-supervisado-i","title":"No supervisado I\u00b6","text":""},{"location":"machine_learning/ns_01/#clustering","title":"Clustering\u00b6","text":"<p>El Clustering es la tarea de agrupar objetos por similitud, en grupos o conjuntos de manera que los miembros del mismo grupo tengan caracter\u00edsticas similares. Es la tarea principal de la miner\u00eda de datos exploratoria y es una t\u00e9cnica com\u00fan en el an\u00e1lisis de datos estad\u00edsticos.</p>"},{"location":"machine_learning/ns_01/#k-means","title":"K-means\u00b6","text":""},{"location":"machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>El algoritmo K-means  (MacQueen, 1967) agrupa las observaciones en un n\u00famero predefinido de $k$ clusters de forma que, la suma de las varianzas internas de los clusters, sea lo menor posible.</p> <p>Existen varias implementaciones de este algoritmo, la m\u00e1s com\u00fan de ellas se conoce como Lloyd\u2019s. En la bibliograf\u00eda es com\u00fan encontrar los t\u00e9rminos inertia, within-cluster sum-of-squares o varianza intra-cluster para referirse a la varianza interna de los clusters.</p>"},{"location":"machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<ol> <li>Especificar el n\u00famero $k$ de clusters que se quieren crear.</li> <li>Seleccionar de forma aleatoria $k$ observaciones del set de datos como centroides iniciales.</li> <li>Asignar cada una de las observaciones al centroide m\u00e1s cercano.</li> <li>Para cada uno de los $k$ clusters generados en el paso 3, recalcular su centroide.</li> </ol> <p>Repetir los pasos 3 y 4 hasta que las asignaciones no cambien o se alcance el n\u00famero m\u00e1ximo de iteraciones establecido.</p> <p>El problema anterior es NP-hard (imposible de resolver en tiempo polinomial, del tipo m\u00e1s dif\u00edcil de los probleams NP).</p>"},{"location":"machine_learning/ns_01/#ventajas-y-desventajas","title":"Ventajas y desventajas\u00b6","text":"<p>K-means es uno de los m\u00e9todos de clustering m\u00e1s utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta.</p> <ul> <li><p>Requiere que se indique de antemano el n\u00famero de clusters que se van a crear. Esto puede ser complicado si no se dispone de informaci\u00f3n adicional sobre los datos con los que se trabaja. Se han desarrollado varias estrategias para ayudar a identificar potenciales valores \u00f3ptimos de $k$ (elbow, shilouette), pero todas ellas son orientativas.</p> </li> <li><p>Dificultad para detectar clusters alargados o con formas irregulares.</p> </li> <li><p>Las agrupaciones resultantes pueden variar dependiendo de la asignaci\u00f3n aleatoria inicial de los centroides. Para minimizar este problema, se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna. Aun as\u00ed, solo se puede garantizar la reproducibilidad de los resultados si se emplean semillas.</p> </li> <li><p>Presenta problemas de robustez frente a outliers.</p> </li> </ul>"},{"location":"machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo k-means.</p>"},{"location":"machine_learning/ns_01/#regla-del-codo","title":"Regla del codo\u00b6","text":"<p>Este m\u00e9todo utiliza los valores de la funci\u00f3n de perdida, $f(C_l,\\mu_l)$, obtenidos tras aplicar el $K$-means a diferente n\u00famero de Clusters (desde 1 a $N$ clusters).</p> <p>Una vez obtenidos los valores de la funci\u00f3n de p\u00e9rdida  tras aplicar el K-means de 1 a $N$ clusters, representamos en una gr\u00e1fica lineal la funci\u00f3n de p\u00e9rdida  respecto del n\u00famero de clusters.</p> <p>En esta gr\u00e1fica se deber\u00eda de apreciar un cambio brusco en la evoluci\u00f3n de la funci\u00f3n de p\u00e9rdida, teniendo la l\u00ednea representada una forma similar a la de un brazo y su codo.</p> <p>El punto en el que se observa ese cambio brusco en la funci\u00f3n de p\u00e9rdida nos dir\u00e1 el n\u00famero \u00f3ptimo de clusters a seleccionar para ese data set; o dicho de otra manera: el punto que representar\u00eda al codo del brazo ser\u00e1 el n\u00famero \u00f3ptimo de clusters para ese data set .</p>"},{"location":"machine_learning/ns_01/#hierarchical-clustering","title":"Hierarchical clustering\u00b6","text":""},{"location":"machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Hierarchical clustering es una alternativa a los m\u00e9todos de partitioning clustering que no requiere que se pre-especifique el n\u00famero de clusters. Los m\u00e9todos que engloba el hierarchical clustering se subdividen en dos tipos dependiendo de la estrategia seguida para crear los grupos:</p> <ul> <li><p>Aglomerativo (agglomerative clustering o bottom-up): el agrupamiento se inicia con todas las observaciones separadas, cada una formando un cluster individual. Los clusters se van combinado a medida que la estructura crece hasta converger en uno solo.</p> </li> <li><p>Divisivo (divisive clustering o top-down): es la estrategia opuesta al aglomerativo. Se inicia con todas las observaciones contenidas en un mismo cluster y se suceden divisiones hasta que cada observaci\u00f3n forma un cluster* individual.</p> </li> </ul> <p>En ambos casos, los resultados pueden representarse de forma muy intuitiva en una estructura de \u00e1rbol llamada dendrograma.</p>"},{"location":"machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":""},{"location":"machine_learning/ns_01/#aglomerativo","title":"Aglomerativo\u00b6","text":"<p>El algoritmo seguido para por el clustering aglomerativo es:</p> <ol> <li><p>Considerar cada una de las n observaciones como un cluster individual, formando as\u00ed la base del dendrograma (hojas).</p> </li> <li><p>Proceso iterativo hasta que todas las observaciones pertenecen a un \u00fanico cluster:</p> <ul> <li><p>Calcular la distancia entre cada posible par de los n clusters. El investigador debe determinar el tipo de medida empleada para cuantificar la similitud entre observaciones o grupos (distancia y linkage).</p> </li> <li><p>Los dos clusters m\u00e1s similares se fusionan, de forma que quedan n-1 clusters.</p> </li> </ul> </li> <li><p>Cortar la estructura de \u00e1rbol generada (dendrograma) a una determinada altura para crear los clusters finales.</p> </li> </ol> <p>Para que el proceso de agrupamiento pueda llevarse a cabo tal como indica el algoritmo anterior, es necesario definir c\u00f3mo se cuantifica la similitud entre dos clusters. Es decir, se tiene que extender el concepto de distancia entre pares de observaciones para que sea aplicable a pares de grupos, cada uno formado por varias observaciones. A este proceso se le conoce como linkage. A continuaci\u00f3n, se describen los 5 tipos de linkage m\u00e1s empleados y sus definiciones.</p> <ul> <li><p>Complete or Maximum: se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida m\u00e1s conservadora (maximal intercluster dissimilarity).</p> </li> <li><p>Single or Minimum: se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora (minimal intercluster dissimilarity).</p> </li> <li><p>Average: Se calcula la distancia entre todos los posibles pares formados por una observaci\u00f3n del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters (mean intercluster dissimilarity).</p> </li> <li><p>Centroid: Se calcula el centroide de cada uno de los clusters y se selecciona la distancia entre ellos como la distancia entre los dos clusters.</p> </li> <li><p>Ward: Se trata de un m\u00e9todo general. La selecci\u00f3n del par de clusters que se combinan en cada paso del agglomerative hierarchical clustering se basa en el valor \u00f3ptimo de una funci\u00f3n objetivo, pudiendo ser esta \u00faltima cualquier funci\u00f3n definida por el analista. El m\u00e9todo Ward's minimum variance es un caso particular en el que el objetivo es minimizar la suma total de varianza intra-cluster. En cada paso, se identifican aquellos 2 clusters cuya fusi\u00f3n conlleva menor incremento de la varianza total intra-cluster. Esta es la misma m\u00e9trica que se minimiza en K-means.</p> </li> </ul> <p>Los m\u00e9todos de complete, average y Ward's minimum variance suelen ser los preferidos por los analistas debido a que generan dendrogramas m\u00e1s compensados. Sin embargo, no se puede determinar que uno sea mejor que otro, ya que depende del caso de estudio en cuesti\u00f3n. Por ejemplo, en gen\u00f3mica, se emplea con frecuencia el m\u00e9todo de centroides. Junto con los resultados de un proceso de hierarchical clustering siempre hay que indicar qu\u00e9 distancia se ha empleado, as\u00ed como el tipo de linkage, ya que, dependiendo de estos, los resultados pueden variar en gran medida.</p>"},{"location":"machine_learning/ns_01/#divisivo","title":"Divisivo\u00b6","text":"<p>El algoritmo m\u00e1s conocido de divisive hierarchical clustering es DIANA (DIvisive ANAlysis Clustering). Este algoritmo se inicia con un \u00fanico cluster que contiene todas las observaciones. A continuaci\u00f3n, se van sucediendo divisiones hasta que cada observaci\u00f3n forma un cluster independiente. En cada iteraci\u00f3n, se selecciona el cluster con mayor di\u00e1metro, entendiendo por di\u00e1metro de un cluster la mayor de las diferencias entre dos de sus observaciones. Una vez seleccionado el cluster, se identifica la observaci\u00f3n m\u00e1s dispar, que es aquella con mayor distancia promedio respecto al resto de observaciones que forman el cluster. Esta observaci\u00f3n inicia el nuevo cluster. Se reasignan las observaciones en funci\u00f3n de si est\u00e1n m\u00e1s pr\u00f3ximas al nuevo cluster o al resto de la partici\u00f3n, dividiendo as\u00ed el cluster seleccionado en dos nuevos clusters.</p> <ol> <li>Todas las $n$ observaciones forman un \u00fanico cluster.</li> <li>Repetir hasta que haya $n$ clusters:<ul> <li>Calcular para cada cluster la mayor de las distancias entre pares de observaciones (di\u00e1metro del cluster).</li> <li>Seleccionar el cluster con mayor di\u00e1metro.</li> <li>Calcular la distancia media de cada observaci\u00f3n respecto a las dem\u00e1s.</li> <li>La observaci\u00f3n m\u00e1s distante inicia un nuevo cluster.</li> <li>Se reasignan las observaciones restantes al nuevo cluster o al viejo dependiendo de cu\u00e1l est\u00e1 m\u00e1s pr\u00f3ximo.</li> </ul> </li> </ol> <p>A diferencia del clustering aglomerativo, en el que hay que elegir un tipo de distancia y un m\u00e9todo de linkage, en el clustering divisivo solo hay que elegir la distancia, no hay linkage.</p>"},{"location":"machine_learning/ns_01/#dendograma","title":"Dendograma\u00b6","text":"<p>Los resultados del hierarchical clustering pueden representarse como un \u00e1rbol en el que las ramas representan la jerarqu\u00eda con la que se van sucediendo las uniones de clusters.</p> <p>Sup\u00f3ngase que se dispone de 45 observaciones en un espacio de dos dimensiones, a los que se les aplica hierarchical clustering para intentar identificar grupos. El siguiente dendrograma representa los resultados obtenidos.</p> <p></p>"},{"location":"machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":""},{"location":"machine_learning/ns_01/#density-based-clustering-dbscan","title":"Density based clustering (DBSCAN)\u00b6","text":""},{"location":"machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Density-based spatial clustering of applications with noise (DBSCAN) fue presentado en 1996 por Ester et al. como una forma de identificar clusters siguiendo el modo intuitivo en el que lo hace el cerebro humano, identificando regiones con alta densidad de observaciones separadas por regiones de baja densidad.</p> <p></p>"},{"location":"machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<ol> <li><p>Para cada observaci\u00f3n $x_i$ calcular la distancia entre ella y el resto de observaciones. Si en su  $epsilon$ -neighborhood hay un n\u00famero de observaciones $\\geq min_samples$  marcar la observaci\u00f3n como core point, de lo contrario marcarla como visitada.</p> </li> <li><p>Para cada observaci\u00f3n  $x_i$   marcada como core point, si todav\u00eda no ha sido asignada a ning\u00fan cluster, crear uno nuevo y asignarla a \u00e9l. Encontrar recursivamente todas las observaciones densamente conectadas a ella y asignarlas al mismo cluster.</p> </li> <li><p>Iterar el mismo proceso para todas las observaciones que no hayan sido visitadas.</p> </li> <li><p>Aquellas observaciones que tras haber sido visitadas no pertenecen a ning\u00fan cluster se marcan como outliers.</p> </li> </ol> <p>Como resultado, todo cluster cumple dos propiedades: todos los puntos que forman parte de un mismo cluster est\u00e1n densamente conectados entre ellos y, si una observaci\u00f3n $A$  es densamente alcanzable desde cualquier otra observaci\u00f3n de un cluster, entonces $A$  tambi\u00e9n pertenece al cluster.</p>"},{"location":"machine_learning/ns_01/#hiperparametros","title":"Hiperpar\u00e1metros\u00b6","text":"<p>Como ocurre en muchas otras t\u00e9cnicas estad\u00edsticas, en DBSCAN no existe una forma \u00fanica y exacta de encontrar el valor adecuado de epsilon  ($\\epsilon$))  y  $min_samples$) . A modo orientativo se pueden seguir las siguientes premisas:</p> <ul> <li><p>$min_samples$ : cuanto mayor sea el tama\u00f1o del set de datos, mayor debe ser el valor m\u00ednimo de observaciones vecinas. En el libro Practical Guide to Cluster Analysis in R recomiendan no bajar nunca de 3. Si los datos contienen niveles altos de ruido, aumentar  $min_samples$  favorecer\u00e1 la creaci\u00f3n de clusters significativos menos influenciados por outliers.</p> </li> <li><p>epsilon ($\\epsilon$): una buena forma de escoger el valor de $\\epsilon$  es estudiar las distancias promedio entre las  $k = minsamples\ud835\udc60$  observaciones m\u00e1s pr\u00f3ximas. Al representar estas distancias en funci\u00f3n de  $\\epsilon$ , el punto de inflexi\u00f3n de la curva suele ser un valor \u00f3ptimo. Si el valor de  $\\epsilon$ escogido es muy peque\u00f1o, una proporci\u00f3n alta de las observaciones no se asignar\u00e1n a ning\u00fan cluster, por el contrario, si el valor es demasiado grande, la mayor\u00eda de observaciones se agrupar\u00e1n en un \u00fanico cluster.</p> </li> </ul>"},{"location":"machine_learning/ns_01/#ventajas-y-desventajas","title":"Ventajas y desventajas\u00b6","text":"<ul> <li><p>Ventajas</p> <ul> <li>No requiere que el usuario especifique el n\u00famero de clusters.</li> <li>Es independiente de la forma que tengan los clusters.</li> <li>Puede identificar outliers, por lo que los clusters generados no se ven influenciados por ellos.</li> </ul> </li> <li><p>Desventajas</p> <ul> <li><p>Es un m\u00e9todo determin\u00edstico siempre y cuando el orden de los datos sea el mismo. Los border points que son alcanzables desde m\u00e1s de un cluster pueden asignarse a uno u otro dependiendo del orden en el que se procesen los datos.</p> </li> <li><p>No genera buenos resultados cuando la densidad de los grupos es muy distinta, ya que no es posible encontrar los par\u00e1metros  \ud835\udf16  y min_samples que sirvan para todos a la vez.</p> </li> </ul> </li> </ul>"},{"location":"machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Veamos un ejemplo de an\u00e1lisis no supervisado ocupando el algoritmo DBSCAN.</p>"},{"location":"machine_learning/ns_01/#gaussian-mixture-models-gmms","title":"Gaussian mixture models (GMMs)\u00b6","text":""},{"location":"machine_learning/ns_01/#teoria","title":"Teor\u00eda\u00b6","text":"<p>Un Gaussian Mixture model es un modelo probabil\u00edstico en el que se considera que las observaciones siguen una distribuci\u00f3n probabil\u00edstica formada por la combinaci\u00f3n de m\u00faltiples distribuciones normales (componentes). En su aplicaci\u00f3n al clustering, puede entenderse como una generalizaci\u00f3n de K-means con la que, en lugar de asignar cada observaci\u00f3n a un \u00fanico cluster, se obtiene una probabilidad de pertenencia a cada uno.</p> <p>Para estimar los par\u00e1metros que definen la funci\u00f3n de distribuci\u00f3n de cada cluster (media y matriz de covarianza) se recurre al algoritmo de Expectation-Maximization (EM). Una vez aprendidos los par\u00e1metros, se puede calcular la probabilidad que tiene cada observaci\u00f3n de pertenecer a cada cluster y asignarla a aquel con mayor probabilidad.</p> <p></p>"},{"location":"machine_learning/ns_01/#algoritmo","title":"Algoritmo\u00b6","text":"<p>Junto con el n\u00famero de clusters (componentes), hay que determinar el tipo de matriz de covarianza que pueden tener los clusters. Dependiendo del tipo de matriz, la forma de los clusters puede ser:</p> <ul> <li><p>tied: todos los clusters comparten la misma matriz de covarianza.</p> </li> <li><p>diagonal: las dimensiones de cada cluster a lo largo de cada dimensi\u00f3n puede ser distinto, pero las elipses generadas siempre quedan alineadas con los ejes, es decir, su orientaciones son limitadas.</p> </li> <li><p>spherical: las dimensiones de cada cluster son las mismas en todas las dimensiones. Esto permite generar clusters de distinto tama\u00f1o pero todos esf\u00e9ricos.</p> </li> <li><p>full: cada cluster puede puede ser modelado como una elipse cualquier orientaci\u00f3n y dimensiones.</p> </li> </ul>"},{"location":"machine_learning/ns_01/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":""},{"location":"machine_learning/ns_01/#limitaciones-del-clustering","title":"Limitaciones del clustering\u00b6","text":"<p>El clustering puede ser una herramienta muy \u00fatil para encontrar agrupaciones en los datos, sobre todo a medida que el volumen de los mismos aumenta. Sin embargo, es importante recordar sus limitaciones o problemas que pueden surgir al aplicarlo. Algunas de ellas son:</p> <ul> <li><p>Peque\u00f1as decisiones pueden tener grandes consecuencias:a la hora de utilizar los m\u00e9todos de clustering se tienen que tomar decisiones que influyen en gran medida en los resultados obtenidos. No existe una \u00fanica respuesta correcta, por lo que en la pr\u00e1ctica se prueban diferentes opciones.</p> <ul> <li>Escalado y centrado de las variables</li> <li>Qu\u00e9 medida de distancia/similitud emplear</li> <li>N\u00famero de clusters</li> <li>Tipo de linkage empleado en hierarchical clustering</li> <li>A que altura establecer el corte de un dendrograma</li> </ul> </li> <li><p>Validaci\u00f3n de los clusters obtenidos: no es f\u00e1cil comprobar la validez de los resultados ya que en la mayor\u00eda de escenarios se desconoce la verdadera agrupaci\u00f3n.</p> </li> <li><p>Falta de robustez: los m\u00e9todos de K-means-clustering e hierarchical clustering asignan obligatoriamente cada observaci\u00f3n a un grupo. Si existe en la muestra alg\u00fan outlier, a pesar de que realmente no pertenezca a ning\u00fan grupo, el algoritmo lo asignar\u00e1 a uno de ellos provocando una distorsi\u00f3n significativa del cluster en cuesti\u00f3n. Algunas alternativas son k-medoids y DBSCAN.</p> </li> <li><p>La naturaleza del algoritmo de hierarchical clustering conlleva que, si se realiza una mala divisi\u00f3n en los pasos iniciales, no se pueda corregir en los pasos siguientes.</p> </li> </ul>"},{"location":"machine_learning/ns_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Unsupervised learning</li> <li>Clustering con Python (Joaqu\u00edn Amat Rodrigo)</li> </ol>"},{"location":"machine_learning/ns_02/","title":"No supervisado II","text":"In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n\npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns    pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre>from sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import scale\n</pre> from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.pipeline import Pipeline from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import scale In\u00a0[3]: Copied! <pre># Datos\n\nurl='https://drive.google.com/file/d/1ckxtKR1U_ySdtMy1uWLo_KYGbUYrKfmI/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\n\ndatos = pd.read_csv(url, sep=\",\")\ndatos = datos.rename(columns = {datos.columns[0]:'index'}).set_index('index')\ndatos.head()\n</pre> # Datos  url='https://drive.google.com/file/d/1ckxtKR1U_ySdtMy1uWLo_KYGbUYrKfmI/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]   datos = pd.read_csv(url, sep=\",\") datos = datos.rename(columns = {datos.columns[0]:'index'}).set_index('index') datos.head() Out[3]: Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 <p>Veamos una exploraci\u00f3n inicial de los datos:</p> In\u00a0[4]: Copied! <pre>print('----------------------')\nprint('Media de cada variable')\nprint('----------------------')\ndatos.mean(axis=0)\n</pre> print('----------------------') print('Media de cada variable') print('----------------------') datos.mean(axis=0) <pre>----------------------\nMedia de cada variable\n----------------------\n</pre> Out[4]: <pre>Murder        7.788\nAssault     170.760\nUrbanPop     65.540\nRape         21.232\ndtype: float64</pre> <p>La media de las variables muestra que hay tres veces m\u00e1s secuestros que asesinatos y 8 veces m\u00e1s asaltos que secuestros.</p> In\u00a0[5]: Copied! <pre>print('-------------------------')\nprint('Varianza de cada variable')\nprint('-------------------------')\ndatos.var(axis=0)\n</pre> print('-------------------------') print('Varianza de cada variable') print('-------------------------') datos.var(axis=0) <pre>-------------------------\nVarianza de cada variable\n-------------------------\n</pre> Out[5]: <pre>Murder        18.970465\nAssault     6945.165714\nUrbanPop     209.518776\nRape          87.729159\ndtype: float64</pre> <p>La varianza es muy distinta entre las variables, en el caso de Assault, la varianza es varios \u00f3rdenes de magnitud superior al resto.</p> <p>Si no se estandarizan las variables para que tengan media cero y desviaci\u00f3n est\u00e1ndar de uno antes de realizar el estudio PCA, la variable Assault, que tiene una media y dispersi\u00f3n muy superior al resto, dominar\u00e1 la mayor\u00eda de las componentes principales.</p> <p>Modelo PCA</p> <p>La clase <code>sklearn.decomposition.PCA</code> incorpora las principales funcionalidades que se necesitan a la hora de trabajar con modelos PCA. El argumento <code>n_components</code> determina el n\u00famero de componentes calculados. Si se indica None, se calculan todas las posibles (min(filas, columnas) - 1).</p> <p>Por defecto, <code>PCA()</code> centra los valores pero no los escala. Esto es importante ya que, si las variables tienen distinta dispersi\u00f3n, como en este caso, es necesario escalarlas. Una forma de hacerlo es combinar un <code>StandardScaler()</code> y un <code>PCA()</code> dentro de un <code>pipeline</code>.</p> In\u00a0[6]: Copied! <pre># Entrenamiento modelo PCA con escalado de los datos\n# ==============================================================================\npca_pipe = make_pipeline(StandardScaler(), PCA())\npca_pipe.fit(datos)\n\n# Se extrae el modelo entrenado del pipeline\nmodelo_pca = pca_pipe.named_steps['pca']\n</pre> # Entrenamiento modelo PCA con escalado de los datos # ============================================================================== pca_pipe = make_pipeline(StandardScaler(), PCA()) pca_pipe.fit(datos)  # Se extrae el modelo entrenado del pipeline modelo_pca = pca_pipe.named_steps['pca'] <p>Una vez entrenado el objeto <code>PCA</code>, pude accederse a toda la informaci\u00f3n de las componentes creadas.</p> <p><code>components_</code> contiene el valor de los loadings  \ud835\udf19  que definen cada componente (eigenvector). Las filas se corresponden con las componentes principals (ordenadas de mayor a menor varianza explicada). Las filas se corresponden con las variables de entrada.</p> In\u00a0[7]: Copied! <pre># Se combierte el array a dataframe para a\u00f1adir nombres a los ejes.\npd.DataFrame(\n    data    = modelo_pca.components_,\n    columns = datos.columns,\n    index   = ['PC1', 'PC2', 'PC3', 'PC4']\n)\n</pre> # Se combierte el array a dataframe para a\u00f1adir nombres a los ejes. pd.DataFrame(     data    = modelo_pca.components_,     columns = datos.columns,     index   = ['PC1', 'PC2', 'PC3', 'PC4'] ) Out[7]: Murder Assault UrbanPop Rape PC1 0.535899 0.583184 0.278191 0.543432 PC2 0.418181 0.187986 -0.872806 -0.167319 PC3 -0.341233 -0.268148 -0.378016 0.817778 PC4 0.649228 -0.743407 0.133878 0.089024 <p>Analizar con detalle el vector de loadings que forma cada componente puede ayudar a interpretar qu\u00e9 tipo de informaci\u00f3n recoge cada una de ellas. Por ejemplo, la primera componente es el resultado de la siguiente combinaci\u00f3n lineal de las variables originales:</p> <p>$$PC1=0.535899 Murder+0.583184 Assault+0.278191 UrbanPop+0.543432 Rape$$</p> <p>Los pesos asignados en la primera componente a las variables Assault, Murder y Rape son aproximadamente iguales entre ellos y superiores al asignado a UrbanPoP. Esto significa que la primera componente recoge mayoritariamente la informaci\u00f3n correspondiente a los delitos. En la segunda componente, es la variable UrbanPoP es la que tiene con diferencia mayor peso, por lo que se corresponde principalmente con el nivel de urbanizaci\u00f3n del estado. Si bien en este ejemplo la interpretaci\u00f3n de las componentes es bastante clara, no en todos los casos ocurre lo mismo, sobre todo a medida que aumenta el n\u00famero de variables.</p> <p>La influencia de las variables en cada componente analizarse visualmente con un gr\u00e1fico de tipo heatmap.</p> In\u00a0[8]: Copied! <pre># Heatmap componentes\n# ==============================================================================\nplt.figure(figsize=(12,4))\ncomponentes = modelo_pca.components_\nplt.imshow(componentes.T, cmap='viridis', aspect='auto')\nplt.yticks(range(len(datos.columns)), datos.columns)\nplt.xticks(range(len(datos.columns)), np.arange(modelo_pca.n_components_) + 1)\nplt.grid(False)\nplt.colorbar();\n</pre> # Heatmap componentes # ============================================================================== plt.figure(figsize=(12,4)) componentes = modelo_pca.components_ plt.imshow(componentes.T, cmap='viridis', aspect='auto') plt.yticks(range(len(datos.columns)), datos.columns) plt.xticks(range(len(datos.columns)), np.arange(modelo_pca.n_components_) + 1) plt.grid(False) plt.colorbar(); <p>Una vez calculadas las componentes principales, se puede conocer la varianza explicada por cada una de ellas, la proporci\u00f3n respecto al total y la proporci\u00f3n de varianza acumulada. Esta informaci\u00f3n est\u00e1 almacenada en los atributos <code>explained_variance_</code> y <code>explained_variance_ratio_</code> del modelo.</p> In\u00a0[9]: Copied! <pre># graficar varianza por componente\npercent_variance = np.round(modelo_pca.explained_variance_ratio_* 100, decimals =2)\ncolumns = ['PC1', 'PC2', 'PC3', 'PC4']\n\nplt.figure(figsize=(12,4))\nplt.bar(x= range(1,5), height=percent_variance, tick_label=columns)\nplt.xticks(np.arange(modelo_pca.n_components_) + 1)\n\nplt.ylabel('Componente principal')\nplt.xlabel('Por. varianza explicada')\nplt.title('Porcentaje de varianza explicada por cada componente')\nplt.show()\n</pre> # graficar varianza por componente percent_variance = np.round(modelo_pca.explained_variance_ratio_* 100, decimals =2) columns = ['PC1', 'PC2', 'PC3', 'PC4']  plt.figure(figsize=(12,4)) plt.bar(x= range(1,5), height=percent_variance, tick_label=columns) plt.xticks(np.arange(modelo_pca.n_components_) + 1)  plt.ylabel('Componente principal') plt.xlabel('Por. varianza explicada') plt.title('Porcentaje de varianza explicada por cada componente') plt.show() <p>Ahora realizamos el gr\u00e1fico pero respecto a la suma acumulada.</p> In\u00a0[10]: Copied! <pre># graficar varianza por la suma acumulada de los componente\npercent_variance_cum = np.cumsum(percent_variance)\ncolumns = ['PC1', 'PC1+PC2', 'PC1+PC2+PC3', 'PC1+PC2+PC3+PC4']\n\nplt.figure(figsize=(12,4))\nplt.bar(x= range(1,5), height=percent_variance_cum, tick_label=columns)\nplt.ylabel('Percentate of Variance Explained')\nplt.xlabel('Principal Component Cumsum')\nplt.title('PCA Scree Plot')\nplt.show()\n</pre> # graficar varianza por la suma acumulada de los componente percent_variance_cum = np.cumsum(percent_variance) columns = ['PC1', 'PC1+PC2', 'PC1+PC2+PC3', 'PC1+PC2+PC3+PC4']  plt.figure(figsize=(12,4)) plt.bar(x= range(1,5), height=percent_variance_cum, tick_label=columns) plt.ylabel('Percentate of Variance Explained') plt.xlabel('Principal Component Cumsum') plt.title('PCA Scree Plot') plt.show() <p>Si se empleasen \u00fanicamente las dos primeras componentes se conseguir\u00eda explicar el 87% de la varianza observada.</p> <p>Transformaci\u00f3n</p> <p>Una vez entrenado el modelo, con el m\u00e9todo <code>transform()</code> se puede reducir la dimensionalidad de nuevas observaciones proyect\u00e1ndolas en el espacio definido por las componentes.</p> In\u00a0[11]: Copied! <pre># Proyecci\u00f3n de las observaciones de entrenamiento\n# ==============================================================================\nproyecciones = pca_pipe.transform(X=datos)\nproyecciones = pd.DataFrame(\n    proyecciones,\n    columns = ['PC1', 'PC2', 'PC3', 'PC4'],\n    index   = datos.index\n)\nproyecciones.head()\n</pre> # Proyecci\u00f3n de las observaciones de entrenamiento # ============================================================================== proyecciones = pca_pipe.transform(X=datos) proyecciones = pd.DataFrame(     proyecciones,     columns = ['PC1', 'PC2', 'PC3', 'PC4'],     index   = datos.index ) proyecciones.head() Out[11]: PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 <p>La transformaci\u00f3n es el resultado de multiplicar los vectores que definen cada componente con el valor de las variables. Puede calcularse de forma manual:</p> In\u00a0[12]: Copied! <pre>proyecciones = np.dot(modelo_pca.components_, scale(datos).T)\nproyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4'])\nproyecciones = proyecciones.transpose().set_index(datos.index)\nproyecciones.head()\n</pre> proyecciones = np.dot(modelo_pca.components_, scale(datos).T) proyecciones = pd.DataFrame(proyecciones, index = ['PC1', 'PC2', 'PC3', 'PC4']) proyecciones = proyecciones.transpose().set_index(datos.index) proyecciones.head() Out[12]: PC1 PC2 PC3 PC4 index Alabama 0.985566 1.133392 -0.444269 0.156267 Alaska 1.950138 1.073213 2.040003 -0.438583 Arizona 1.763164 -0.745957 0.054781 -0.834653 Arkansas -0.141420 1.119797 0.114574 -0.182811 California 2.523980 -1.542934 0.598557 -0.341996 <p>Reconstrucci\u00f3n</p> <p>Puede revertirse la transformaci\u00f3n y reconstruir el valor inicial con el m\u00e9todo inverse_transform(). Es importante tener en cuenta que, la reconstrucci\u00f3n, solo ser\u00e1 completa si se han incluido todas las componentes.</p> In\u00a0[13]: Copied! <pre># Recostruccion de las proyecciones\n# ==============================================================================\nrecostruccion = pca_pipe.inverse_transform(proyecciones)\nrecostruccion = pd.DataFrame(\n                    recostruccion,\n                    columns = datos.columns,\n                    index   = datos.index\n)\nprint('------------------')\nprint('Valores originales')\nprint('------------------')\ndisplay(recostruccion.head())\n\nprint('---------------------')\nprint('Valores reconstruidos')\nprint('---------------------')\ndisplay(datos.head())\n</pre> # Recostruccion de las proyecciones # ============================================================================== recostruccion = pca_pipe.inverse_transform(proyecciones) recostruccion = pd.DataFrame(                     recostruccion,                     columns = datos.columns,                     index   = datos.index ) print('------------------') print('Valores originales') print('------------------') display(recostruccion.head())  print('---------------------') print('Valores reconstruidos') print('---------------------') display(datos.head()) <pre>------------------\nValores originales\n------------------\n</pre> Murder Assault UrbanPop Rape index Alabama 13.2 236.0 58.0 21.2 Alaska 10.0 263.0 48.0 44.5 Arizona 8.1 294.0 80.0 31.0 Arkansas 8.8 190.0 50.0 19.5 California 9.0 276.0 91.0 40.6 <pre>---------------------\nValores reconstruidos\n---------------------\n</pre> Murder Assault UrbanPop Rape index Alabama 13.2 236 58 21.2 Alaska 10.0 263 48 44.5 Arizona 8.1 294 80 31.0 Arkansas 8.8 190 50 19.5 California 9.0 276 91 40.6 <p>Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n 2. El paso 2 es similar al paso 1, pero en lugar de usar una distribuci\u00f3n gaussiana se usa una distribuci\u00f3n t de Student con un grado de libertad, que tambi\u00e9n se conoce como la distribuci\u00f3n de Cauchy (Figura 3). Esto nos da un segundo conjunto de probabilidades ($Q_{ij}$) en el espacio de baja dimensi\u00f3n.</p> <p>Como puede ver, la distribuci\u00f3n t de Student tiene colas m\u00e1s pesadas que la distribuci\u00f3n normal. Las colas pesadas permiten un mejor modelado de distancias muy separadas.</p> <p></p> <p>Figura 3 \u2013 Distribuci\u00f3n noraml vs t-student</p> <ol> <li>El \u00faltimo paso es que queremos que este conjunto de probabilidades del espacio de baja dimensi\u00f3n ($Q_{ij}$) refleje las del espacio de alta dimensi\u00f3n ($P_{ij}$) de la mejor manera posible.</li> </ol> <p>Queremos que las dos estructuras de mapa sean similares. Medimos la diferencia entre las distribuciones de probabilidad de los espacios bidimensionales utilizando la divergencia de Kullback-Liebler (KL).</p> <p>No incluir\u00e9 mucho en KL, excepto que es un enfoque asim\u00e9trico que compara de manera eficiente los grandes valores $P_{ij}$ y $Q_{ij}$. Finalmente, utilizamos el descenso de gradiente para minimizar nuestra funci\u00f3n de costo KL.</p> In\u00a0[14]: Copied! <pre># Load Python Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\n%matplotlib inline\n</pre> # Load Python Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from time import time %matplotlib inline In\u00a0[15]: Copied! <pre>from sklearn.datasets import load_digits\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n</pre> from sklearn.datasets import load_digits from sklearn.manifold import TSNE from sklearn.decomposition import PCA In\u00a0[16]: Copied! <pre>digits = load_digits()\n\ndf = pd.DataFrame(digits['data'])\ndf['label'] = digits['target']\ndf.head()\n</pre> digits = load_digits()  df = pd.DataFrame(digits['data']) df['label'] = digits['target'] df.head() Out[16]: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 label 0 0.0 0.0 5.0 13.0 9.0 1.0 0.0 0.0 0.0 0.0 13.0 15.0 10.0 15.0 5.0 0.0 0.0 3.0 15.0 2.0 0.0 11.0 8.0 0.0 0.0 4.0 12.0 0.0 0.0 8.0 8.0 0.0 0.0 5.0 8.0 0.0 0.0 9.0 8.0 0.0 0.0 4.0 11.0 0.0 1.0 12.0 7.0 0.0 0.0 2.0 14.0 5.0 10.0 12.0 0.0 0.0 0.0 0.0 6.0 13.0 10.0 0.0 0.0 0.0 0 1 0.0 0.0 0.0 12.0 13.0 5.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 9.0 0.0 0.0 0.0 0.0 3.0 15.0 16.0 6.0 0.0 0.0 0.0 7.0 15.0 16.0 16.0 2.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 3.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 1.0 16.0 16.0 6.0 0.0 0.0 0.0 0.0 0.0 11.0 16.0 10.0 0.0 0.0 1 2 0.0 0.0 0.0 4.0 15.0 12.0 0.0 0.0 0.0 0.0 3.0 16.0 15.0 14.0 0.0 0.0 0.0 0.0 8.0 13.0 8.0 16.0 0.0 0.0 0.0 0.0 1.0 6.0 15.0 11.0 0.0 0.0 0.0 1.0 8.0 13.0 15.0 1.0 0.0 0.0 0.0 9.0 16.0 16.0 5.0 0.0 0.0 0.0 0.0 3.0 13.0 16.0 16.0 11.0 5.0 0.0 0.0 0.0 0.0 3.0 11.0 16.0 9.0 0.0 2 3 0.0 0.0 7.0 15.0 13.0 1.0 0.0 0.0 0.0 8.0 13.0 6.0 15.0 4.0 0.0 0.0 0.0 2.0 1.0 13.0 13.0 0.0 0.0 0.0 0.0 0.0 2.0 15.0 11.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 12.0 12.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 10.0 8.0 0.0 0.0 0.0 8.0 4.0 5.0 14.0 9.0 0.0 0.0 0.0 7.0 13.0 13.0 9.0 0.0 0.0 3 4 0.0 0.0 0.0 1.0 11.0 0.0 0.0 0.0 0.0 0.0 0.0 7.0 8.0 0.0 0.0 0.0 0.0 0.0 1.0 13.0 6.0 2.0 2.0 0.0 0.0 0.0 7.0 15.0 0.0 9.0 8.0 0.0 0.0 5.0 16.0 10.0 0.0 16.0 6.0 0.0 0.0 4.0 15.0 16.0 13.0 16.0 1.0 0.0 0.0 0.0 0.0 3.0 15.0 10.0 0.0 0.0 0.0 0.0 0.0 2.0 16.0 4.0 0.0 0.0 4 In\u00a0[17]: Copied! <pre># PCA\nscaler = StandardScaler()\n\nX = df.drop(columns='label')\ny = df['label']\n    \nembedding = PCA(n_components=2)\nX_transform = embedding.fit_transform(X)\n    \ndf_pca = pd.DataFrame(X_transform,columns = ['Score1','Score2'])\ndf_pca['label'] = y\n</pre> # PCA scaler = StandardScaler()  X = df.drop(columns='label') y = df['label']      embedding = PCA(n_components=2) X_transform = embedding.fit_transform(X)      df_pca = pd.DataFrame(X_transform,columns = ['Score1','Score2']) df_pca['label'] = y In\u00a0[18]: Copied! <pre># Plot Digits PCA\n\n\n# Set style of scatterplot\nsns.set_context(\"notebook\", font_scale=1.1)\nsns.set_style(\"ticks\")\n\n# Create scatterplot of dataframe\nsns.lmplot(x='Score1',\n           y='Score2',\n           data=df_pca,\n           fit_reg=False,\n           legend=True,\n           height=9,\n           hue='label',\n           scatter_kws={\"s\":200, \"alpha\":0.3})\n\nplt.title('PCA Results: Digits', weight='bold').set_fontsize('14')\nplt.xlabel('Prin Comp 1', weight='bold').set_fontsize('10')\nplt.ylabel('Prin Comp 2', weight='bold').set_fontsize('10')\n</pre> # Plot Digits PCA   # Set style of scatterplot sns.set_context(\"notebook\", font_scale=1.1) sns.set_style(\"ticks\")  # Create scatterplot of dataframe sns.lmplot(x='Score1',            y='Score2',            data=df_pca,            fit_reg=False,            legend=True,            height=9,            hue='label',            scatter_kws={\"s\":200, \"alpha\":0.3})  plt.title('PCA Results: Digits', weight='bold').set_fontsize('14') plt.xlabel('Prin Comp 1', weight='bold').set_fontsize('10') plt.ylabel('Prin Comp 2', weight='bold').set_fontsize('10') <p>Al graficar las dos componentes principales del m\u00e9todo PCA, se observa que no existe una clara distinci\u00f3n entre la distintas clases (solo se ve un gran c\u00famulo de puntos mezclados).</p> In\u00a0[19]: Copied! <pre># tsne\nscaler = StandardScaler()\n\nX = df.drop(columns='label')\ny = df['label']\n    \nembedding = TSNE(n_components=2)\nX_transform = embedding.fit_transform(X)\n    \ndf_tsne = pd.DataFrame(X_transform,columns = ['_DIM_1_','_DIM_2_'])\ndf_tsne['label'] = y\n</pre> # tsne scaler = StandardScaler()  X = df.drop(columns='label') y = df['label']      embedding = TSNE(n_components=2) X_transform = embedding.fit_transform(X)      df_tsne = pd.DataFrame(X_transform,columns = ['_DIM_1_','_DIM_2_']) df_tsne['label'] = y In\u00a0[20]: Copied! <pre># Plot Digits t-SNE\nsns.set_context(\"notebook\", font_scale=1.1)\nsns.set_style(\"ticks\")\n\nsns.lmplot(x='_DIM_1_',\n           y='_DIM_2_',\n           data=df_tsne,\n           fit_reg=False,\n           legend=True,\n           height=9,\n           hue='label',\n           scatter_kws={\"s\":200, \"alpha\":0.3})\n\nplt.title('t-SNE Results: Digits', weight='bold').set_fontsize('14')\nplt.xlabel('Dimension 1', weight='bold').set_fontsize('10')\nplt.ylabel('Dimension 2', weight='bold').set_fontsize('10')\n</pre> # Plot Digits t-SNE sns.set_context(\"notebook\", font_scale=1.1) sns.set_style(\"ticks\")  sns.lmplot(x='_DIM_1_',            y='_DIM_2_',            data=df_tsne,            fit_reg=False,            legend=True,            height=9,            hue='label',            scatter_kws={\"s\":200, \"alpha\":0.3})  plt.title('t-SNE Results: Digits', weight='bold').set_fontsize('14') plt.xlabel('Dimension 1', weight='bold').set_fontsize('10') plt.ylabel('Dimension 2', weight='bold').set_fontsize('10') <p>Para el caso del m\u00e9todo TSNE, se observa una diferenciaci\u00f3n entre los grupos de estudios (aspecto que fue muy distinto al momento de analizar el m\u00e9todo del PCA).</p> <p>Observaci\u00f3n: Si bien se muestra donde el m\u00e9todo TSNE logra ser superior en aspecto de reducci\u00f3n de dimensionalidad que el m\u00e9todo PCA, no significa que para distintos experimientos se tengan los mismo resultados.</p> In\u00a0[21]: Copied! <pre># Librerias\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA,TruncatedSVD,NMF\nfrom sklearn.manifold import Isomap,LocallyLinearEmbedding,MDS,SpectralEmbedding,TSNE\n</pre> # Librerias from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  from sklearn.decomposition import PCA,TruncatedSVD,NMF from sklearn.manifold import Isomap,LocallyLinearEmbedding,MDS,SpectralEmbedding,TSNE In\u00a0[22]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[23]: Copied! <pre># Datos\nurl='https://drive.google.com/file/d/1zqoLNbuysK5Idrb9--DFtCoxkapruSDc/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndatos = pd.read_csv(url, sep=\",\")\ndatos = datos.drop(columns = datos.columns[0])\ndatos['fat'] = datos['fat'].astype(float)\ndatos.head()\n</pre> # Datos url='https://drive.google.com/file/d/1zqoLNbuysK5Idrb9--DFtCoxkapruSDc/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  datos = pd.read_csv(url, sep=\",\") datos = datos.drop(columns = datos.columns[0]) datos['fat'] = datos['fat'].astype(float) datos.head() Out[23]: V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 V29 V30 V31 V32 V33 V34 V35 V36 V37 V38 V39 V40 V41 V42 V43 V44 V45 V46 V47 V48 V49 V50 V51 V52 V53 V54 V55 V56 V57 V58 V59 V60 V61 V62 V63 V64 V65 V66 V67 V68 V69 V70 V71 V72 V73 V74 V75 V76 V77 V78 V79 V80 V81 V82 V83 V84 V85 V86 V87 V88 V89 V90 V91 V92 V93 V94 V95 V96 V97 V98 V99 V100 fat 0 2.61776 2.61814 2.61859 2.61912 2.61981 2.62071 2.62186 2.62334 2.62511 2.62722 2.62964 2.63245 2.63565 2.63933 2.64353 2.64825 2.65350 2.65937 2.66585 2.67281 2.68008 2.68733 2.69427 2.70073 2.70684 2.71281 2.71914 2.72628 2.73462 2.74416 2.75466 2.76568 2.77679 2.78790 2.79949 2.81225 2.82706 2.84356 2.86106 2.87857 2.89497 2.90924 2.92085 2.93015 2.93846 2.94771 2.96019 2.97831 3.00306 3.03506 3.07428 3.11963 3.16868 3.21771 3.26254 3.29988 3.32847 3.34899 3.36342 3.37379 3.38152 3.38741 3.39164 3.39418 3.39490 3.39366 3.39045 3.38541 3.37869 3.37041 3.36073 3.34979 3.33769 3.32443 3.31013 3.29487 3.27891 3.26232 3.24542 3.22828 3.21080 3.19287 3.17433 3.15503 3.13475 3.11339 3.09116 3.06850 3.04596 3.02393 3.00247 2.98145 2.96072 2.94013 2.91978 2.89966 2.87964 2.85960 2.83940 2.81920 22.5 1 2.83454 2.83871 2.84283 2.84705 2.85138 2.85587 2.86060 2.86566 2.87093 2.87661 2.88264 2.88898 2.89577 2.90308 2.91097 2.91953 2.92873 2.93863 2.94929 2.96072 2.97272 2.98493 2.99690 3.00833 3.01920 3.02990 3.04101 3.05345 3.06777 3.08416 3.10221 3.12106 3.13983 3.15810 3.17623 3.19519 3.21584 3.23747 3.25889 3.27835 3.29384 3.30362 3.30681 3.30393 3.29700 3.28925 3.28409 3.28505 3.29326 3.30923 3.33267 3.36251 3.39661 3.43188 3.46492 3.49295 3.51458 3.53004 3.54067 3.54797 3.55306 3.55675 3.55921 3.56045 3.56034 3.55876 3.55571 3.55132 3.54585 3.53950 3.53235 3.52442 3.51583 3.50668 3.49700 3.48683 3.47626 3.46552 3.45501 3.44481 3.43477 3.42465 3.41419 3.40303 3.39082 3.37731 3.36265 3.34745 3.33245 3.31818 3.30473 3.29186 3.27921 3.26655 3.25369 3.24045 3.22659 3.21181 3.19600 3.17942 40.1 2 2.58284 2.58458 2.58629 2.58808 2.58996 2.59192 2.59401 2.59627 2.59873 2.60131 2.60414 2.60714 2.61029 2.61361 2.61714 2.62089 2.62486 2.62909 2.63361 2.63835 2.64330 2.64838 2.65354 2.65870 2.66375 2.66880 2.67383 2.67892 2.68411 2.68937 2.69470 2.70012 2.70563 2.71141 2.71775 2.72490 2.73344 2.74327 2.75433 2.76642 2.77931 2.79272 2.80649 2.82064 2.83541 2.85121 2.86872 2.88905 2.91289 2.94088 2.97325 3.00946 3.04780 3.08554 3.11947 3.14696 3.16677 3.17938 3.18631 3.18924 3.18950 3.18801 3.18498 3.18039 3.17411 3.16611 3.15641 3.14512 3.13241 3.11843 3.10329 3.08714 3.07014 3.05237 3.03393 3.01504 2.99569 2.97612 2.95642 2.93660 2.91667 2.89655 2.87622 2.85563 2.83474 2.81361 2.79235 2.77113 2.75015 2.72956 2.70934 2.68951 2.67009 2.65112 2.63262 2.61461 2.59718 2.58034 2.56404 2.54816 8.4 3 2.82286 2.82460 2.82630 2.82814 2.83001 2.83192 2.83392 2.83606 2.83842 2.84097 2.84374 2.84664 2.84975 2.85307 2.85661 2.86038 2.86437 2.86860 2.87308 2.87789 2.88301 2.88832 2.89374 2.89917 2.90457 2.90991 2.91521 2.92043 2.92565 2.93082 2.93604 2.94128 2.94658 2.95202 2.95777 2.96419 2.97159 2.98045 2.99090 3.00284 3.01611 3.03048 3.04579 3.06194 3.07889 3.09686 3.11629 3.13775 3.16217 3.19068 3.22376 3.26172 3.30379 3.34793 3.39093 3.42920 3.45998 3.48227 3.49687 3.50558 3.51026 3.51221 3.51215 3.51036 3.50682 3.50140 3.49398 3.48457 3.47333 3.46041 3.44595 3.43005 3.41285 3.39450 3.37511 3.35482 3.33376 3.31204 3.28986 3.26730 3.24442 3.22117 3.19757 3.17357 3.14915 3.12429 3.09908 3.07366 3.04825 3.02308 2.99820 2.97367 2.94951 2.92576 2.90251 2.87988 2.85794 2.83672 2.81617 2.79622 5.9 4 2.78813 2.78989 2.79167 2.79350 2.79538 2.79746 2.79984 2.80254 2.80553 2.80890 2.81272 2.81704 2.82184 2.82710 2.83294 2.83945 2.84664 2.85458 2.86331 2.87280 2.88291 2.89335 2.90374 2.91371 2.92305 2.93187 2.94060 2.94986 2.96035 2.97241 2.98606 3.00097 3.01652 3.03220 3.04793 3.06413 3.08153 3.10078 3.12185 3.14371 3.16510 3.18470 3.20140 3.21477 3.22544 3.23505 3.24586 3.26027 3.28063 3.30889 3.34543 3.39019 3.44198 3.49800 3.55407 3.60534 3.64789 3.68011 3.70272 3.71815 3.72863 3.73574 3.74059 3.74357 3.74453 3.74336 3.73991 3.73418 3.72638 3.71676 3.70553 3.69289 3.67900 3.66396 3.64785 3.63085 3.61305 3.59463 3.57582 3.55695 3.53796 3.51880 3.49936 3.47938 3.45869 3.43711 3.41458 3.39129 3.36772 3.34450 3.32201 3.30025 3.27907 3.25831 3.23784 3.21765 3.19766 3.17770 3.15770 3.13753 25.5 <p>El set de datos contiene 101 columnas. Las 100 primeras, nombradas como  $V_1,...,V_{100}$  recogen el valor de absorbancia para cada una de las 100 longitudes de onda analizadas (predictores), y la columna fat el contenido en grasa medido por t\u00e9cnicas qu\u00edmicas (variable respuesta).</p> <p>Muchas de las variables est\u00e1n altamente correlacionadas (correlaci\u00f3n absoluta &gt; 0.8), lo que supone un problema a la hora de emplear modelos de regresi\u00f3n lineal.</p> In\u00a0[24]: Copied! <pre># Correlaci\u00f3n entre columnas num\u00e9ricas\ndef tidy_corr_matrix(corr_mat):\n    '''\n    Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy\n    '''\n    corr_mat = corr_mat.stack().reset_index()\n    corr_mat.columns = ['variable_1','variable_2','r']\n    corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]\n    corr_mat['abs_r'] = np.abs(corr_mat['r'])\n    corr_mat = corr_mat.sort_values('abs_r', ascending=False)\n    \n    return(corr_mat)\n\ncorr_matrix = datos.select_dtypes(include=['float64', 'int']) \\\n              .corr(method='pearson')\ndisplay(tidy_corr_matrix(corr_matrix).head(5))\n</pre> # Correlaci\u00f3n entre columnas num\u00e9ricas def tidy_corr_matrix(corr_mat):     '''     Funci\u00f3n para convertir una matriz de correlaci\u00f3n de pandas en formato tidy     '''     corr_mat = corr_mat.stack().reset_index()     corr_mat.columns = ['variable_1','variable_2','r']     corr_mat = corr_mat.loc[corr_mat['variable_1'] != corr_mat['variable_2'], :]     corr_mat['abs_r'] = np.abs(corr_mat['r'])     corr_mat = corr_mat.sort_values('abs_r', ascending=False)          return(corr_mat)  corr_matrix = datos.select_dtypes(include=['float64', 'int']) \\               .corr(method='pearson') display(tidy_corr_matrix(corr_matrix).head(5)) variable_1 variable_2 r abs_r 1019 V11 V10 0.999996 0.999996 919 V10 V11 0.999996 0.999996 1021 V11 V12 0.999996 0.999996 1121 V12 V11 0.999996 0.999996 917 V10 V9 0.999996 0.999996 <p>Se procede aplicar el modelo de regresi\u00f3n lineal .</p> In\u00a0[25]: Copied! <pre># Divisi\u00f3n de los datos en train y test\nX = datos.drop(columns='fat')\ny = datos['fat']\n\nX_train, X_test, y_train, y_test = train_test_split(\n                                        X,\n                                        y.values,\n                                        train_size   = 0.7,\n                                        random_state = 1234,\n                                        shuffle      = True\n                                    )\n</pre> # Divisi\u00f3n de los datos en train y test X = datos.drop(columns='fat') y = datos['fat']  X_train, X_test, y_train, y_test = train_test_split(                                         X,                                         y.values,                                         train_size   = 0.7,                                         random_state = 1234,                                         shuffle      = True                                     ) In\u00a0[26]: Copied! <pre># Creaci\u00f3n y entrenamiento del modelo\nmodelo = LinearRegression()\nmodelo.fit(X = X_train, y = y_train)\n</pre> # Creaci\u00f3n y entrenamiento del modelo modelo = LinearRegression() modelo.fit(X = X_train, y = y_train) Out[26]: <pre>LinearRegression()</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression()</pre> In\u00a0[27]: Copied! <pre># Predicciones test\npredicciones = modelo.predict(X=X_test)\npredicciones = predicciones.flatten()\n\n# Error de test del modelo \ndf_pred = pd.DataFrame({\n    'y':y_test,\n    'yhat':predicciones\n})\n\ndf_summary = regression_metrics(df_pred)\ndf_summary\n</pre> # Predicciones test predicciones = modelo.predict(X=X_test) predicciones = predicciones.flatten()  # Error de test del modelo  df_pred = pd.DataFrame({     'y':y_test,     'yhat':predicciones })  df_summary = regression_metrics(df_pred) df_summary Out[27]: mae mse rmse mape smape 0 2.0904 14.743 3.8397 16.1573 0.2782 <p>Ahora se ocuparan los modelos de reducci\u00f3n de dimensionalidad para entrenar el modelo de regresi\u00f3n lineal. Para ello se ocuparan los distintos algoritmos mencionados. Lo primero es crear una funci\u00f3n que pueda realizar esta tarea de manera autom\u00e1tica.</p> In\u00a0[28]: Copied! <pre># funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal\n\ndef dr_pipeline(df, model_dr):\n    \n    # datos\n    X = df.drop(columns='fat')\n    y = df['fat']\n    \n    # reduccion de la dimensionalidad\n    embedding = model_dr\n    X = embedding.fit_transform(X)\n    \n\n    X_train, X_test, y_train, y_test = train_test_split(\n                                        X,\n                                        y,\n                                        train_size   = 0.7,\n                                        random_state = 1234,\n                                        shuffle      = True\n                                    )\n    \n    # Creaci\u00f3n y entrenamiento del modelo\n    modelo = LinearRegression()\n    modelo.fit(X = X_train, y = y_train)\n\n    \n    # Predicciones test\n    predicciones = modelo.predict(X=X_test)\n    predicciones = predicciones.flatten()\n\n    # Error de test del modelo \n    df_pred = pd.DataFrame({\n        'y':y_test,\n        'yhat':predicciones\n    })\n\n    df_summary = regression_metrics(df_pred)\n    \n    return df_summary\n</pre> # funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal  def dr_pipeline(df, model_dr):          # datos     X = df.drop(columns='fat')     y = df['fat']          # reduccion de la dimensionalidad     embedding = model_dr     X = embedding.fit_transform(X)           X_train, X_test, y_train, y_test = train_test_split(                                         X,                                         y,                                         train_size   = 0.7,                                         random_state = 1234,                                         shuffle      = True                                     )          # Creaci\u00f3n y entrenamiento del modelo     modelo = LinearRegression()     modelo.fit(X = X_train, y = y_train)           # Predicciones test     predicciones = modelo.predict(X=X_test)     predicciones = predicciones.flatten()      # Error de test del modelo      df_pred = pd.DataFrame({         'y':y_test,         'yhat':predicciones     })      df_summary = regression_metrics(df_pred)          return df_summary <p>Enfoque: Algebra lineal</p> In\u00a0[29]: Copied! <pre>modelos_algebra_lineal = [\n    ('PCA',PCA(n_components=5)),\n    ('SVD',TruncatedSVD(n_components=5)),\n    ('NMF',NMF(n_components=5))\n]\n\nnames = [x[0] for x in modelos_algebra_lineal]\nresults = [dr_pipeline(datos,x[1]) for x in modelos_algebra_lineal]\n</pre> modelos_algebra_lineal = [     ('PCA',PCA(n_components=5)),     ('SVD',TruncatedSVD(n_components=5)),     ('NMF',NMF(n_components=5)) ]  names = [x[0] for x in modelos_algebra_lineal] results = [dr_pipeline(datos,x[1]) for x in modelos_algebra_lineal] In\u00a0[30]: Copied! <pre>df_algebra_lineal = pd.concat(results).reset_index(drop=True)\ndf_algebra_lineal['metodo'] =names\ndf_algebra_lineal\n</pre> df_algebra_lineal = pd.concat(results).reset_index(drop=True) df_algebra_lineal['metodo'] =names df_algebra_lineal Out[30]: mae mse rmse mape smape metodo 0 2.8050 12.2999 3.5071 28.6888 0.4459 PCA 1 2.6344 11.3195 3.3645 26.0475 0.4133 SVD 2 6.3295 75.1477 8.6688 68.0810 0.8101 NMF <p>Enfoque: Manifold Learning</p> In\u00a0[31]: Copied! <pre>modelos_manifold= [\n    ('Isomap',Isomap(n_components=5)),\n    ('LocallyLinearEmbedding', LocallyLinearEmbedding(n_components=5)),\n    ('MDS',  MDS(n_components=5)),\n    ('SpectralEmbedding', SpectralEmbedding(n_components=5)),\n    ('TSNE', TSNE(n_components=2)),\n]\n\nnames = [x[0] for x in modelos_manifold]\nresults = [dr_pipeline(datos,x[1]) for x in modelos_manifold]\n\n\ndf_manifold = pd.concat(results).reset_index(drop=True)\ndf_manifold['metodo'] =names\ndf_manifold\n</pre> modelos_manifold= [     ('Isomap',Isomap(n_components=5)),     ('LocallyLinearEmbedding', LocallyLinearEmbedding(n_components=5)),     ('MDS',  MDS(n_components=5)),     ('SpectralEmbedding', SpectralEmbedding(n_components=5)),     ('TSNE', TSNE(n_components=2)), ]  names = [x[0] for x in modelos_manifold] results = [dr_pipeline(datos,x[1]) for x in modelos_manifold]   df_manifold = pd.concat(results).reset_index(drop=True) df_manifold['metodo'] =names df_manifold <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\manifold\\_mds.py:299: FutureWarning: The default value of `normalized_stress` will change to `'auto'` in version 1.4. To suppress this warning, manually set the value of `normalized_stress`.\n  warnings.warn(\n</pre> Out[31]: mae mse rmse mape smape metodo 0 9.4021 133.7433 11.5647 99.1430 0.9957 Isomap 1 9.1331 130.6859 11.4318 88.6049 0.9396 LocallyLinearEmbedding 2 7.1867 76.1355 8.7256 83.9630 0.9128 MDS 3 8.9529 130.9545 11.4435 83.3004 0.9089 SpectralEmbedding 4 8.8753 128.9746 11.3567 88.1109 0.9368 TSNE In\u00a0[32]: Copied! <pre>df_manifold.sort_values([\"mae\",\"mape\"])\n</pre> df_manifold.sort_values([\"mae\",\"mape\"]) Out[32]: mae mse rmse mape smape metodo 2 7.1867 76.1355 8.7256 83.9630 0.9128 MDS 4 8.8753 128.9746 11.3567 88.1109 0.9368 TSNE 3 8.9529 130.9545 11.4435 83.3004 0.9089 SpectralEmbedding 1 9.1331 130.6859 11.4318 88.6049 0.9396 LocallyLinearEmbedding 0 9.4021 133.7433 11.5647 99.1430 0.9957 Isomap <p>En este caso en particular, funciona de mejor forma aplicar los m\u00e9todos de descomposici\u00f3n del Algebra Lineal en relaci\u00f3n de los m\u00e9todos de Manifold Learning. La ense\u00f1anza que se lleva de esto que, dependiendo del volumen de datos que se trabaje, la capidad de c\u00f3mputo y las habilidades de programaci\u00f3n suficiente, se pueden probar y automatizar varios de estos m\u00e9todos. Por supuesto, quedar\u00e1 como responsabilidad del programador buscar el criterio para poder seleccionar el mejor m\u00e9todo (dependiendo del caso en estudio).</p>"},{"location":"machine_learning/ns_02/#no-supervisado-ii","title":"No supervisado II\u00b6","text":""},{"location":"machine_learning/ns_02/#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de dimensionalidad\u00b6","text":"<p>En aprendizaje autom\u00e1tico y estad\u00edsticas reducci\u00f3n de dimensionalidad  es el proceso de reducci\u00f3n del n\u00famero de variables aleatorias que se trate, y se puede dividir en selecci\u00f3n de funci\u00f3n y extracci\u00f3n de funci\u00f3n</p> <p>Sin embargo, se puede utilizar como un paso de preprocesamiento de transformaci\u00f3n de datos para algoritmos de aprendizaje autom\u00e1tico en conjuntos de datos de modelado predictivo de clasificaci\u00f3n y regresi\u00f3n con algoritmos de aprendizaje supervisado.</p> <p>Hay muchos algoritmos de reducci\u00f3n de dimensionalidad entre los que elegir y no existe el mejor algoritmo para todos los casos. En cambio, es una buena idea explorar una variedad de algoritmos de reducci\u00f3n de dimensionalidad y diferentes configuraciones para cada algoritmo.</p>"},{"location":"machine_learning/ns_02/#algoritmos-de-reduccion-de-la-dimensionalidad","title":"Algoritmos de reducci\u00f3n de la dimensionalidad\u00b6","text":"<p>Hay muchos algoritmos que pueden ser usados para la reducci\u00f3n de la dimensionalidad.</p> <p>Dos clases principales de m\u00e9todos son los que se extraen del \u00e1lgebra lineal y los que se extraen del aprendizaje m\u00faltiple.</p>"},{"location":"machine_learning/ns_02/#metodos-de-algebra-lineal","title":"M\u00e9todos de \u00e1lgebra lineal\u00b6","text":"<p>Los m\u00e9todos de factorizaci\u00f3n matricial extra\u00eddos del campo del \u00e1lgebra lineal pueden utilizarse para la dimensionalidad. Algunos de los m\u00e9todos m\u00e1s populares incluyen:</p> <ul> <li>An\u00e1lisis de los componentes principales</li> <li>Descomposici\u00f3n del valor singular</li> <li>Factorizaci\u00f3n de matriz no negativa</li> </ul>"},{"location":"machine_learning/ns_02/#multiples-metodos-de-aprendizaje","title":"M\u00faltiples m\u00e9todos de aprendizaje\u00b6","text":"<p>Los m\u00faltiples m\u00e9todos de aprendizaje buscan una proyecci\u00f3n de dimensiones inferiores de alta entrada dimensional que capte las propiedades salientes de los datos de entrada.</p> <p>Algunos de los m\u00e9todos m\u00e1s populares incluyen:</p> <ul> <li>Isomap Embedding</li> <li>Locally Linear Embedding</li> <li>Multidimensional Scaling</li> <li>Spectral Embedding</li> <li>t-distributed Stochastic Neighbor Embedding (t-sne)</li> </ul> <p>Cada algoritmo ofrece un enfoque diferente para el desaf\u00edo de descubrir las relaciones naturales en los datos de dimensiones inferiores.</p> <p>No hay un mejor algoritmo de reducci\u00f3n de la dimensionalidad, y no hay una manera f\u00e1cil de encontrar el mejor algoritmo para sus datos sin usar experimentos controlados.</p> <p>Debido a la importancia que se tiene en el mundo del machine lerning, se dar\u00e1 un explicaci\u00f3n formal del m\u00e9todo de PCA y luego se dar\u00e1 una breve rese\u00f1a de los dem\u00e1s m\u00e9todos.</p>"},{"location":"machine_learning/ns_02/#pca","title":"PCA\u00b6","text":"<p>El an\u00e1lisis de componentes principales (Principal Component Analysis PCA) es un m\u00e9todo de reducci\u00f3n de dimensionalidad que permite simplificar la complejidad de espacios con m\u00faltiples dimensiones a la vez que conserva su informaci\u00f3n.</p> <p>Sup\u00f3ngase que existe una muestra con  $n$  individuos cada uno con  $p$  variables ( $X_1$,...,$X_p$), es decir, el espacio muestral tiene  $p$  dimensiones. PCA permite encontrar un n\u00famero de factores subyacentes  ($z&lt;p$)  que explican aproximadamente lo mismo que las  $p$  variables originales. Donde antes se necesitaban  $p$  valores para caracterizar a cada individuo, ahora bastan  $z$  valores. Cada una de estas  $z$  nuevas variables recibe el nombre de componente principal.</p> <p></p> <p>El m\u00e9todo de PCA permite por lo tanto \"condensar\" la informaci\u00f3n aportada por m\u00faltiples variables en solo unas pocas componentes. Aun as\u00ed, no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes. Dos de las principales aplicaciones del PCA son la visualizaci\u00f3n y el preprocesado de predictores previo ajuste de modelos supervisados.</p>"},{"location":"machine_learning/ns_02/#interpretacion-geometrica-de-las-componentes-principales","title":"Interpretaci\u00f3n geom\u00e9trica de las componentes principales\u00b6","text":"<p>Una forma intuitiva de entender el proceso de PCA es interpretar las componentes principales desde un punto de vista geom\u00e9trico. Sup\u00f3ngase un conjunto de observaciones para las que se dispone de dos variables ( $X_1$, $X_2$ ). El vector que define la primera componente principal ($Z_1$ ) sigue la direcci\u00f3n en la que las observaciones tienen m\u00e1s varianza (l\u00ednea roja). La proyecci\u00f3n de cada observaci\u00f3n sobre esa direcci\u00f3n equivale al valor de la primera componente para dicha observaci\u00f3n (principal component score,  $z_{i1}$ ).</p> <p></p> <p>La segunda componente ( $Z_2$ ) sigue la segunda direcci\u00f3n en la que los datos muestran mayor varianza y que no est\u00e1 correlacionada con la primera componente. La condici\u00f3n de no correlaci\u00f3n entre componentes principales equivale a decir que sus direcciones son perpendiculares/ortogonales.</p> <p></p>"},{"location":"machine_learning/ns_02/#calculo-de-las-componentes-principales","title":"C\u00e1lculo de las componentes principales\u00b6","text":"<p>Cada componente principal ( $Z_i$ ) se obtiene por combinaci\u00f3n lineal de las variables originales. Se pueden entender como nuevas variables obtenidas al combinar de una determinada forma las variables originales. La primera componente principal de un grupo de variables ( $X_1,...,X_p$ ) es la combinaci\u00f3n lineal normalizada de dichas variables que tiene mayor varianza:</p> <p>$$ Z_1 = \\phi_{11}X_1 + ... + \\phi_{p1}X_p$$</p> <p>Que la combinaci\u00f3n lineal sea normalizada implica que:</p> <p>$$\\sum_{j=1}^p \\phi^2_{j1} = 1$$</p> <p>Los t\u00e9rminos  $\\phi_{11},...,\\phi_{p1}$  reciben en el nombre de loadings y son los que definen las componentes. Por ejemplo,  $\\phi_{11}$  es el loading de la variable  $X_1$  de la primera componente principal. Los loadings pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer que tipo de informaci\u00f3n recoge cada una de las componentes.</p> <p>Dado un set de datos  $X$  con $n$ observaciones y $p$ variables, el proceso a seguir para calcular la primera componente principal es:</p> <ul> <li><p>Centrar las variables: se resta a cada valor la media de la variable a la que pertenece. Con esto se consigue que todas las variables tengan media cero.</p> </li> <li><p>Se resuelve un problema de optimizaci\u00f3n para encontrar el valor de los loadings con los que se maximiza la varianza. Una forma de resolver esta optimizaci\u00f3n es mediante el c\u00e1lculo de eigenvector-eigenvalue de la matriz de covarianzas.</p> </li> </ul> <p>Una vez calculada la primera componente ( $Z_1$ ), se calcula la segunda ( $Z_2$ ) repitiendo el mismo proceso pero a\u00f1adiendo la condici\u00f3n de que la combinaci\u00f3n lineal no pude estar correlacionada con la primera componente. Esto equivale a decir que  $Z_1$  y  $Z_2$  tienen que ser perpendiculares. EL proceso se repite de forma iterativa hasta calcular todas las posibles componentes (min($n-1, p$)) o hasta que se decida detener el proceso. El orden de importancia de las componentes viene dado por la magnitud del eigenvalue asociado a cada eigenvector.</p>"},{"location":"machine_learning/ns_02/#caracteristicas-del-pca","title":"Caracter\u00edsticas del PCA\u00b6","text":"<ul> <li>Escalado de las variables: El proceso de PCA identifica las direcciones con mayor varianza.</li> <li>Reproducibilidad de las componentes: El proceso de PCA est\u00e1ndar es determinista, genera siempre las mismas componentes principales, es decir, el valor de los loadings resultantes es el mismo.</li> <li>Influencia de outliers: Al trabajar con varianzas, el m\u00e9todo PCA es muy sensible a outliers, por lo que es recomendable estudiar si los hay. La detecci\u00f3n de valores at\u00edpicos con respecto a una determinada dimensi\u00f3n es algo relativamente sencillo de hacer mediante comprobaciones gr\u00e1ficas.</li> </ul>"},{"location":"machine_learning/ns_02/#proporcion-de-varianza-explicada","title":"Proporci\u00f3n de varianza explicada\u00b6","text":"<p>Una de las preguntas m\u00e1s frecuentes que surge tras realizar un PCA es: \u00bfCu\u00e1nta informaci\u00f3n presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensi\u00f3n? o lo que es lo mismo \u00bfCuanta informaci\u00f3n es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporci\u00f3n de varianza explicada por cada componente principal.</p> <p>Asumiendo que las variables se han normalizado para tener media cero, la varianza total presente en el set de datos se define como</p> <p>$$\\sum_{j=1}^p Var(X_j) = \\dfrac{1}{n}\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2$$</p> <p>y la varianza explicada por la componente m es</p> <p>$$\\dfrac{1}{n}\\sum_{i=1}^n z_{im}^2 = \\dfrac{1}{n}\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2$$</p> <p>Por lo tanto, la proporci\u00f3n de varianza explicada por la componente m viene dada por el ratio</p> <p>$$ \\dfrac{\\sum_{i=1}^n (\\sum_{j=1}^p \\phi_{jm}x_{ij})^2}{\\sum_{j=1}^p\\sum_{i=1}^nx_{ij}^2}$$</p> <p>Tanto la proporci\u00f3n de varianza explicada, como la proporci\u00f3n de varianza explicada acumulada, son dos valores de gran utilidad a la hora de decidir el n\u00famero de componentes principales a utilizar en los an\u00e1lisis posteriores. Si se calculan todas las componentes principales de un set de datos, entonces, aunque transformada, se est\u00e1 almacenando toda la informaci\u00f3n presente en los datos originales. El sumatorio de la proporci\u00f3n de varianza explicada acumulada de todas las componentes es siempre 1.</p>"},{"location":"machine_learning/ns_02/#numero-optimo-de-componentes-principales","title":"N\u00famero \u00f3ptimo de componentes principales\u00b6","text":"<p>Por lo general, dada una matriz de datos de dimensiones $n \\times p$, el n\u00famero de componentes principales que se pueden calcular es como m\u00e1ximo de $n-1$ o $p$ (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de inter\u00e9s utilizar el n\u00famero m\u00ednimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o m\u00e9todo \u00fanico que permita identificar cual es el n\u00famero \u00f3ptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporci\u00f3n de varianza explicada acumulada y seleccionar el n\u00famero de componentes m\u00ednimo a partir del cual el incremento deja de ser sustancial.</p> <p></p>"},{"location":"machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>El m\u00e9todo Principal Components Regression PCR consiste en ajustar un modelo de regresi\u00f3n lineal por m\u00ednimos cuadrados empleando como predictores las componentes generadas a partir de un Principal Component Analysis (PCA). De esta forma, con un n\u00famero reducido de componentes se puede explicar la mayor parte de la varianza de los datos.</p> <p>En los estudios observacionales, es frecuente disponer de un n\u00famero elevado de variables que se pueden emplear como predictores, sin embargo, esto no implica necesariamente que se disponga de mucha informaci\u00f3n. Si las variables est\u00e1n correlacionadas entre ellas, la informaci\u00f3n que aportan es redundante y adem\u00e1s, se incumple la condici\u00f3n de no colinealidad necesaria en la regresi\u00f3n por m\u00ednimos cuadrados. Dado que el PCA es \u00fatil eliminando informaci\u00f3n redundante, si se emplean como predictores las componentes principales, se puede mejorar el modelo de regresi\u00f3n. Es importante tener en cuenta que, si bien el Principal Components Regression reduce el n\u00famero de predictores del modelo, no se puede considerar como un m\u00e9todo de selecci\u00f3n de variables ya que todas ellas se necesitan para el c\u00e1lculo de las componentes. La identificaci\u00f3n del n\u00famero \u00f3ptimo de componentes principales que se emplean como predictores en PCR puede identificarse por validaci\u00f3n cruzada.</p> <p>Datos: El set de datos <code>USArrests</code> contiene el porcentaje de asaltos (Assault), asesinatos (Murder) y secuestros (Rape) por cada 100,000 habitantes para cada uno de los 50 estados de USA (1973). Adem\u00e1s, tambi\u00e9n incluye el porcentaje de la poblaci\u00f3n de cada estado que vive en zonas rurales (UrbanPoP).</p>"},{"location":"machine_learning/ns_02/#t-distributed-stochastic-neighbor-embedding-t-sne","title":"t-Distributed Stochastic Neighbor Embedding (t-SNE)\u00b6","text":"<p>t-Distributed Stochastic Neighbor Embedding (t-SNE)  es una t\u00e9cnica no lineal no supervisada utilizada principalmente para la exploraci\u00f3n de datos y la visualizaci\u00f3n de datos de alta dimensi\u00f3n.</p> <p>En t\u00e9rminos m\u00e1s simples, tSNE le da una sensaci\u00f3n o intuici\u00f3n de c\u00f3mo se organizan los datos en un espacio de alta dimensi\u00f3n. Fue desarrollado por Laurens van der Maatens y Geoffrey Hinton en 2008.</p>"},{"location":"machine_learning/ns_02/#comparando-con-pca","title":"Comparando con PCA\u00b6","text":"<p>Si est\u00e1 familiarizado con An\u00e1lisis de componentes principales (PCA), entonces como yo , probablemente se est\u00e9 preguntando la diferencia entre PCA y tSNE.</p> <p>Lo primero a tener en cuenta es que PCA se desarroll\u00f3 en 1933, mientras que tSNE se desarroll\u00f3 en 2008. Mucho ha cambiado en el mundo de la ciencia de datos desde 1933, principalmente en el \u00e1mbito del c\u00e1lculo y el tama\u00f1o de los datos.</p> <p>En segundo lugar, PCA es una t\u00e9cnica de reducci\u00f3n de dimensi\u00f3n lineal que busca maximizar la varianza y preserva las distancias pares grandes. En otras palabras, las cosas que son diferentes terminan muy separadas. Esto puede conducir a una visualizaci\u00f3n deficiente, especialmente cuando se trata de estructuras distribuidoras no lineales. Piense en una estructura m\u00faltiple como cualquier forma geom\u00e9trica como: cilindro, bola, curva, etc.</p> <p>tSNE difiere de PCA al preservar solo peque\u00f1as distancias por pares o similitudes locales, mientras que PCA se preocupa por preservar distancias pares grandes para maximizar la varianza.</p> <p>Laurens ilustra bastante bien el enfoque PCA y tSNE utilizando el conjunto de datos Swiss Roll en la Figura 1 [1].</p> <p>Puede ver que debido a la no linealidad de este conjunto de datos de juguete (m\u00faltiple) y la preservaci\u00f3n de grandes distancias, PCA conservar\u00eda incorrectamente la estructura de los datos.</p> <p></p> <p>Figura 1 \u2013 Dataset de rollo suizo. Conservar la distancia peque\u00f1a con tSNE (l\u00ednea continua) frente a la maximizaci\u00f3n de la variaci\u00f3n PCA [1]</p>"},{"location":"machine_learning/ns_02/#explicacion","title":"Explicaci\u00f3n\u00b6","text":"<p>Ahora que sabemos por qu\u00e9 podr\u00edamos usar tSNE sobre PCA, analicemos c\u00f3mo funciona tSNE. El algoritmo tSNE calcula una medida de similitud entre pares de instancias en el espacio de alta dimensi\u00f3n y en el espacio de baja dimensi\u00f3n. Luego trata de optimizar estas dos medidas de similitud usando una funci\u00f3n de costo. Vamos a dividirlo en 3 pasos b\u00e1sicos.</p> <ol> <li>Paso 1, mide similitudes entre puntos en el espacio de alta dimensi\u00f3n. Piense en un conjunto de puntos de datos dispersos en un espacio 2D (Figura 2).</li> </ol> <p>Para cada punto de datos (xi) centraremos una distribuci\u00f3n Gaussiana sobre ese punto. Luego medimos la densidad de todos los puntos (xj) bajo esa distribuci\u00f3n Gaussiana. Luego renormalize para todos los puntos.</p> <p>Esto nos da un conjunto de probabilidades (Pij) para todos los puntos. Esas probabilidades son proporcionales a las similitudes.</p> <p>Todo lo que eso significa es que si los puntos de datos x1 y x2 tienen valores iguales bajo este c\u00edrculo gaussiano, entonces sus proporciones y similitudes son iguales y, por lo tanto, tienes similitudes locales en la estructura de este espacio de alta dimensi\u00f3n.</p> <p>La distribuci\u00f3n gaussiana o el c\u00edrculo se pueden manipular usando lo que se llama perplejidad, que influye en la varianza de la distribuci\u00f3n (tama\u00f1o del c\u00edrculo) y esencialmente en el n\u00famero de vecinos m\u00e1s cercanos. El rango normal para la perplejidad est\u00e1 entre 5 y 50 [2].</p> <p></p> <p>Figura 2 \u2013 Medici\u00f3n de similitudes por pares en el espacio de alta dimensi\u00f3n</p>"},{"location":"machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>Laurens van der Maaten menciona el uso de tSNE en \u00e1reas como investigaci\u00f3n del clima, seguridad inform\u00e1tica, bioinform\u00e1tica, investigaci\u00f3n del c\u00e1ncer, etc. tSNE podr\u00eda usarse en datos de alta dimensi\u00f3n y luego el resultado de esas dimensiones se convierte en insumos para alg\u00fan otro modelo de clasificaci\u00f3n .</p> <p>Adem\u00e1s, tSNE podr\u00eda usarse para investigar, aprender o evaluar la segmentaci\u00f3n. Muchas veces seleccionamos la cantidad de segmentos antes del modelado o iteramos despu\u00e9s de los resultados. tSNE a menudo puede mostrar una separaci\u00f3n clara en los datos.</p> <p>Esto se puede usar antes de usar su modelo de segmentaci\u00f3n para seleccionar un n\u00famero de cl\u00faster o despu\u00e9s para evaluar si sus segmentos realmente se mantienen. tSNE, sin embargo, no es un enfoque de agrupamiento, ya que no conserva las entradas como PCA y los valores a menudo pueden cambiar entre ejecuciones, por lo que es pura exploraci\u00f3n.</p> <p>A continuaci\u00f3n se procede a comparar de manera  visual los algoritmos de PCA y tSNE en el conjunto de datos <code>Digits</code> .</p> <p>Datos: El conjunto de datos contiene im\u00e1genes de d\u00edgitos escritos a mano: 10 clases donde cada clase se refiere a un d\u00edgito. Los programas de preprocesamiento puestos a disposici\u00f3n por NIST se utilizaron para extraer mapas de bits normalizados de d\u00edgitos escritos a mano de un formulario preimpreso. De un total de 43 personas, 30 contribuyeron al conjunto de entrenamiento y diferentes 13 al conjunto de prueba. Los mapas de bits de 32x32 se dividen en bloques no superpuestos de 4x4 y se cuenta el n\u00famero de p\u00edxeles en cada bloque. Esto genera una matriz de entrada de 8x8 donde cada elemento es un n\u00famero entero en el rango 0..16. Esto reduce la dimensionalidad y da invariancia a peque\u00f1as distorsiones.</p>"},{"location":"machine_learning/ns_02/#otros-metodos-de-reduccion-de-dimensionalidad","title":"Otros m\u00e9todos de reducci\u00f3n de dimensionalidad\u00b6","text":"<p>Existen otro m\u00e9todos de reducci\u00f3n de dimencionalidad, a continuaci\u00f3n se deja una referencia con la descripci\u00f3n de cada uno de estos algoritmos.</p> <ul> <li>Descomposici\u00f3n del valor singular </li> <li>Non-Negative Matrix Factorization </li> <li>Isomap Embedding </li> <li>Locally Linear Embedding </li> <li>Multidimensional Scaling </li> <li>Spectral Embedding </li> </ul>"},{"location":"machine_learning/ns_02/#aplicacion","title":"Aplicaci\u00f3n\u00b6","text":"<p>En este ejemplo se quiere aprovechar las bondades de aplicar la reducci\u00f3n de dimensionalidad para ocupar un modelo de clasificaci\u00f3n (en este caso, el modelo de regresi\u00f3n log\u00edstica). Para ello se ocupar\u00e1 el conjunto de datos <code>meatspec.csv</code></p> <p>Datos: El departamento de calidad de una empresa de alimentaci\u00f3n se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante t\u00e9cnicas de anal\u00edtica qu\u00edmica, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitir\u00eda reducir costes y optimizar tiempo es emplear un espectrofot\u00f3metro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en funci\u00f3n de sus caracter\u00edsticas) e inferir el contenido en grasa a partir de sus medidas.</p> <p>Antes de dar por v\u00e1lida esta nueva t\u00e9cnica, la empresa necesita comprobar qu\u00e9 margen de error tiene respecto al an\u00e1lisis qu\u00edmico. Para ello, se mide el espectro de absorbancia a 100 longitudes de onda en 215 muestras de carne, cuyo contenido en grasa se obtiene tambi\u00e9n por an\u00e1lisis qu\u00edmico, y se entrena un modelo con el objetivo de predecir el contenido en grasa a partir de los valores dados por el espectrofot\u00f3metro.</p>"},{"location":"machine_learning/ns_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>In Depth: Principal Component Analysis</li> <li>Unsupervised dimensionality reduction</li> </ol>"},{"location":"machine_learning/over_01/","title":"Overfitting I","text":"<p>El overfitting ocurre cuando el algoritmo de machine learning captura el ruido de los datos. Intuitivamente, el overfitting ocurre cuando el modelo o el algoritmo se ajusta demasiado bien a los datos. Espec\u00edficamente, el sobreajuste ocurre si el modelo o algoritmo muestra un sesgo bajo pero una varianza alta.</p> <p>El overfitting a menudo es el resultado de un modelo excesivamente complicado, y puede evitarse ajustando m\u00faltiples modelos y utilizando validaci\u00f3n o validaci\u00f3n cruzada para comparar sus precisiones predictivas en los datos de prueba.</p> <p></p> <p>El underfitting ocurre cuando un modelo estad\u00edstico o un algoritmo de machine learning no pueden capturar la tendencia subyacente de los datos. Intuitivamente, el underfitting ocurre cuando el modelo o el algoritmo no se ajustan suficientemente a los datos. Espec\u00edficamente, el underfitting ocurre si el modelo o algoritmo muestra una varianza baja pero un sesgo alto.</p> <p>El underfitting suele ser el resultado de un modelo excesivamente simple.</p> <p></p> <p>\u00bfC\u00f3mo escoger el mejor modelo?</p> <p></p> <ul> <li><p>El sobreajuste va a estar relacionado con la complejidad del modelo, mientras m\u00e1s complejidad le agreguemos, mayor va a ser la tendencia a sobreajuste a los datos.</p> </li> <li><p>No existe una regla general para establecer cual es el nivel ideal de complejidad que le podemos otorgar a nuestro modelo sin caer en el sobreajuste; pero podemos valernos de algunas herramientas anal\u00edticas para intentar entender como el modelo se ajusta a los datos y reconocer el sobreajuste.</p> </li> </ul> <p>Para entender esto, veamos un ejemplo con el m\u00e9todo de \u00e1rboles de decisiones. Los \u00e1rboles de decisi\u00f3n (DT) son un m\u00e9todo de aprendizaje supervisado no param\u00e9trico utilizado para la clasificaci\u00f3n y la regresi\u00f3n.</p> In\u00a0[1]: Copied! <pre># librerias \n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nimport random\n\nrandom.seed(1982) # semilla\n\n# graficos incrustados\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor import random  random.seed(1982) # semilla  # graficos incrustados %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n</pre> # Create a random dataset rng = np.random.RandomState(1) X = np.sort(5 * rng.rand(80, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - rng.rand(16))  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982) In\u00a0[3]: Copied! <pre># Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=10)\n\nregr_1.fit(x_train,y_train)\nregr_2.fit(x_train,y_train)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\ny_3 = regr_1.predict(X_test)\n\n# Plot the results\nfig, ax = plt.subplots(figsize=(11, 8.5))\nplt.scatter(X, y, s=20, edgecolor=\"black\",\n            c=\"darkorange\", label=\"data\")\nplt.plot(X_test, y_1, color=\"cornflowerblue\",\n         label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=10\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n</pre> # Fit regression model regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=10)  regr_1.fit(x_train,y_train) regr_2.fit(x_train,y_train)  # Predict X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test) y_3 = regr_1.predict(X_test)  # Plot the results fig, ax = plt.subplots(figsize=(11, 8.5)) plt.scatter(X, y, s=20, edgecolor=\"black\",             c=\"darkorange\", label=\"data\") plt.plot(X_test, y_1, color=\"cornflowerblue\",          label=\"max_depth=2\", linewidth=2) plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=10\", linewidth=2) plt.xlabel(\"data\") plt.ylabel(\"target\") plt.title(\"Decision Tree Regression\") plt.legend() plt.show() <p>Basado en los gr\u00e1ficos, el modelo de DT con profundidad 2, no se ajuste muy bien a los datos, mientras que el modelo DT con profundidad 10 se ajuste excesivamente demasiado a ellos.</p> <p>Para ver el ajuste de cada modelo, estudiaremos su precisi\u00f3n (score) sobre los conjunto de entrenamiento y de testeo.</p> In\u00a0[4]: Copied! <pre>result  = pd.DataFrame({\n    \n    'model': ['dt_depth_2','dt_depth_10'],\n    'train_score': [ regr_1.score(x_train, y_train), regr_2.score(x_train, y_train)],\n    'test_score': [ regr_1.score(x_eval, y_eval), regr_2.score(x_eval, y_eval)]\n})\nresult\n</pre> result  = pd.DataFrame({          'model': ['dt_depth_2','dt_depth_10'],     'train_score': [ regr_1.score(x_train, y_train), regr_2.score(x_train, y_train)],     'test_score': [ regr_1.score(x_eval, y_eval), regr_2.score(x_eval, y_eval)] }) result Out[4]: model train_score test_score 0 dt_depth_2 0.766363 0.719137 1 dt_depth_10 1.000000 0.661186 <p>Como es de esperar, para el modelo DT con profundidad 10, la precisi\u00f3n sobre el conjunto de entrenamiento es perfecta (igual a 1), no obstante, esta disminuye considerablemente al obtener la presici\u00f3n sobre los datos de testeo (igual a 0.66), por lo que esto es una evidencia para decir que el modelo tiene overfitting.</p> <p>Caso contrario es el modelo  DT con profundidad 2, puesto que es un caso t\u00edpico de underfitting. Cabe destacar que el modelo de underfitting tiene una presici\u00f3n similar tanto para el conjunto de entrenamiento como para el conjunto de testo.</p> <p>Conclusiones del caso</p> <p>Ambos modelos no ajuste de la mejor manera, pero lo hacen de distintas perspectivas. Se debe poner mucho \u00e9nfasis al momento de separar el conjunto de entrenamiento y de testeo, puesto que los resultados se pueden ver altamente sesgado (caso del overfitting). Particularmente para este caso, el ajuste era complejo de realizar puesto que eliminabamos un monto de datos \"significativos\", que hacian que los modelos no captar\u00e1n la continuidad de la funci\u00f3n sinusoidal.</p> In\u00a0[5]: Copied! <pre># Ejemplo en python - \u00e1rboles de decisi\u00f3n\n# dummy data con 100 atributos y 2 clases\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n# Grafico de ajuste del \u00e1rbol de decisi\u00f3n\ntrain_prec =  []\neval_prec = []\nmax_deep_list = list(range(2, 20))\n\n# Entrenar con arboles de distinta profundidad\nfor deep in max_deep_list:\n    model = DecisionTreeClassifier( max_depth=deep)\n    model.fit(x_train, y_train)\n    train_prec.append(model.score(x_train, y_train))\n    eval_prec.append(model.score(x_eval, y_eval))\n</pre> # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)  # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec =  [] eval_prec = [] max_deep_list = list(range(2, 20))  # Entrenar con arboles de distinta profundidad for deep in max_deep_list:     model = DecisionTreeClassifier( max_depth=deep)     model.fit(x_train, y_train)     train_prec.append(model.score(x_train, y_train))     eval_prec.append(model.score(x_eval, y_eval)) In\u00a0[6]: Copied! <pre># graficar los resultados.\n\nsns.set(rc={'figure.figsize':(12,9)})\n\ndf1 = pd.DataFrame({'numero_nodos':max_deep_list,\n                   'precision':train_prec,\n                   'datos':'entrenamiento'})\n\ndf2 = pd.DataFrame({'numero_nodos':max_deep_list,\n                   'precision':eval_prec,\n                   'datos':'evaluacion'})\n\ndf_graph = pd.concat([df1,df2])\n\nsns.lineplot(data=df_graph,\n             x='numero_nodos',\n             y='precision',\n             hue='datos',\n             palette=\"Set1\")\n</pre> # graficar los resultados.  sns.set(rc={'figure.figsize':(12,9)})  df1 = pd.DataFrame({'numero_nodos':max_deep_list,                    'precision':train_prec,                    'datos':'entrenamiento'})  df2 = pd.DataFrame({'numero_nodos':max_deep_list,                    'precision':eval_prec,                    'datos':'evaluacion'})  df_graph = pd.concat([df1,df2])  sns.lineplot(data=df_graph,              x='numero_nodos',              y='precision',              hue='datos',              palette=\"Set1\") Out[6]: <pre>&lt;Axes: xlabel='numero_nodos', ylabel='precision'&gt;</pre> <p>El gr\u00e1fico que acabamos de construir se llama gr\u00e1fico de ajuste y muestra la precisi\u00f3n del modelo en funci\u00f3n de su complejidad.</p> <p>El punto con mayor precisi\u00f3n, en los datos de evaluaci\u00f3n, lo obtenemos con un nivel de profundidad de aproximadamente 6 nodos; a partir de all\u00ed el modelo pierde en generalizaci\u00f3n y comienza a estar sobreajustado.</p> <p>Tambi\u00e9n podemos crear un gr\u00e1fico similar con la ayuda de Scikit-learn, utilizando <code>validation_curve</code>.</p> In\u00a0[7]: Copied! <pre># utilizando validation curve de sklearn\nfrom sklearn.model_selection import validation_curve\n\ntrain_prec, eval_prec = validation_curve(estimator=model, X=x_train,\n                                        y=y_train, param_name='max_depth',\n                                        param_range=max_deep_list, cv=5)\n\ntrain_mean = np.mean(train_prec, axis=1)\ntrain_std = np.std(train_prec, axis=1)\ntest_mean = np.mean(eval_prec, axis=1)\ntest_std = np.std(eval_prec, axis=1)\n</pre> # utilizando validation curve de sklearn from sklearn.model_selection import validation_curve  train_prec, eval_prec = validation_curve(estimator=model, X=x_train,                                         y=y_train, param_name='max_depth',                                         param_range=max_deep_list, cv=5)  train_mean = np.mean(train_prec, axis=1) train_std = np.std(train_prec, axis=1) test_mean = np.mean(eval_prec, axis=1) test_std = np.std(eval_prec, axis=1) In\u00a0[8]: Copied! <pre># graficando las curvas\nplt.plot(max_deep_list, train_mean, color='r', marker='o', markersize=5,\n         label='entrenamiento')\nplt.fill_between(max_deep_list, train_mean + train_std, \n                 train_mean - train_std, alpha=0.15, color='r')\nplt.plot(max_deep_list, test_mean, color='b', linestyle='--', \n         marker='s', markersize=5, label='evaluacion')\nplt.fill_between(max_deep_list, test_mean + test_std, \n                 test_mean - test_std, alpha=0.15, color='b')\nplt.legend(loc='center right')\nplt.xlabel('numero_nodos')\nplt.ylabel('precision')\nplt.show()\n</pre> # graficando las curvas plt.plot(max_deep_list, train_mean, color='r', marker='o', markersize=5,          label='entrenamiento') plt.fill_between(max_deep_list, train_mean + train_std,                   train_mean - train_std, alpha=0.15, color='r') plt.plot(max_deep_list, test_mean, color='b', linestyle='--',           marker='s', markersize=5, label='evaluacion') plt.fill_between(max_deep_list, test_mean + test_std,                   test_mean - test_std, alpha=0.15, color='b') plt.legend(loc='center right') plt.xlabel('numero_nodos') plt.ylabel('precision') plt.show()"},{"location":"machine_learning/over_01/#overfitting-i","title":"Overfitting I\u00b6","text":""},{"location":"machine_learning/over_01/#ejemplo-con-arboles-de-decision","title":"Ejemplo con  \u00c1rboles de Decisi\u00f3n\u00b6","text":"<p>Los \u00c1rboles de Decisi\u00f3n pueden ser muchas veces una herramienta muy precisa, pero tambi\u00e9n con mucha tendencia al sobreajuste. Para construir estos modelos aplicamos un procedimiento recursivo para encontrar los atributos que nos proporcionan m\u00e1s informaci\u00f3n sobre distintos subconjuntos de datos, cada vez m\u00e1s peque\u00f1os.</p> <p>Si aplicamos este procedimiento en forma reiterada, eventualmente podemos llegar a un \u00e1rbol en el que cada hoja tenga una sola instancia de nuestra variable objetivo a clasificar.</p> <p>En este caso extremo, el \u00c1rbol de Decisi\u00f3n va a tener una pobre generalizaci\u00f3n y estar bastante sobreajustado; ya que cada instancia de los datos de entrenamiento va a encontrar el camino que lo lleve eventualmente a la hoja que lo contiene, alcanzando as\u00ed una precisi\u00f3n del 100% con los datos de entrenamiento.</p>"},{"location":"machine_learning/over_01/#ejemplo-funcion-sinusoidal","title":"Ejemplo funci\u00f3n sinusoidal\u00b6","text":"<p>Veamos un ejemplo sencillo con la ayuda de python, tratemos de ajustar un modelo de DT sobre una funci\u00f3n senusoidal.</p>"},{"location":"machine_learning/over_01/#equilibrio-en-el-ajuste-de-modelos","title":"Equilibrio en el ajuste de modelos\u00b6","text":"<p>A continuaci\u00f3n ocuparemos otro conjunto de entrenamientos (make_classification) para mostrar una forma de encoentrar un un equilibrio en la complejidad del modelo y su ajuste a los datos.</p> <p>Siguiendo con el ejemplo de los modelos de \u00e1rbol de decisi\u00f3n, analizaremos la presici\u00f3n (score) para distintas profundidades sobre los distintos conjuntos (entrenamiento y testeo).</p>"},{"location":"machine_learning/over_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Underfitting vs Underfitting</li> <li>Overfitting and Underfitting With Machine Learning Algorithms</li> </ol>"},{"location":"machine_learning/over_02/","title":"Overfitting II","text":"<p>Veamos un ejemplo en python, ocupando el conjunto de datos make_classification.</p> In\u00a0[1]: Copied! <pre># librerias \n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nimport random\n\nrandom.seed(1982) # semilla\n\n# graficos incrustados\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns  from sklearn.model_selection import train_test_split from sklearn.datasets import make_classification from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor import random  random.seed(1982) # semilla  # graficos incrustados %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># Ejemplo en python - \u00e1rboles de decisi\u00f3n\n# dummy data con 100 atributos y 2 clases\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n# Grafico de ajuste del \u00e1rbol de decisi\u00f3n\ntrain_prec =  []\neval_prec = []\nmax_deep_list = list(range(2, 20))\n</pre> # Ejemplo en python - \u00e1rboles de decisi\u00f3n # dummy data con 100 atributos y 2 clases X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)  # Grafico de ajuste del \u00e1rbol de decisi\u00f3n train_prec =  [] eval_prec = [] max_deep_list = list(range(2, 20)) In\u00a0[3]: Copied! <pre># Ejemplo cross-validation\nfrom sklearn.model_selection import cross_validate,StratifiedKFold\n\n# creando pliegues\n\nskf = StratifiedKFold(n_splits=20)\nprecision = []\nmodel =  DecisionTreeClassifier(criterion='entropy', max_depth=5)\n\nskf.get_n_splits(x_train, y_train)\nfor k, (train_index, test_index) in enumerate(skf.split(X, y)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model.fit(X_train,y_train) \n    score = model.score(X_test,y_test)\n    precision.append(score)\n    print('Pliegue: {0:}, Dist Clase: {1:}, Prec: {2:.3f}'.format(k+1,\n                        np.bincount(y_train), score))\n</pre> # Ejemplo cross-validation from sklearn.model_selection import cross_validate,StratifiedKFold  # creando pliegues  skf = StratifiedKFold(n_splits=20) precision = [] model =  DecisionTreeClassifier(criterion='entropy', max_depth=5)  skf.get_n_splits(x_train, y_train) for k, (train_index, test_index) in enumerate(skf.split(X, y)):     X_train, X_test = X[train_index], X[test_index]     y_train, y_test = y[train_index], y[test_index]     model.fit(X_train,y_train)      score = model.score(X_test,y_test)     precision.append(score)     print('Pliegue: {0:}, Dist Clase: {1:}, Prec: {2:.3f}'.format(k+1,                         np.bincount(y_train), score))      <pre>Pliegue: 1, Dist Clase: [4763 4737], Prec: 0.928\nPliegue: 2, Dist Clase: [4763 4737], Prec: 0.914\nPliegue: 3, Dist Clase: [4763 4737], Prec: 0.916\nPliegue: 4, Dist Clase: [4763 4737], Prec: 0.938\nPliegue: 5, Dist Clase: [4763 4737], Prec: 0.924\nPliegue: 6, Dist Clase: [4763 4737], Prec: 0.938\nPliegue: 7, Dist Clase: [4763 4737], Prec: 0.924\nPliegue: 8, Dist Clase: [4762 4738], Prec: 0.938\nPliegue: 9, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 10, Dist Clase: [4762 4738], Prec: 0.908\nPliegue: 11, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 12, Dist Clase: [4762 4738], Prec: 0.938\nPliegue: 13, Dist Clase: [4762 4738], Prec: 0.934\nPliegue: 14, Dist Clase: [4762 4738], Prec: 0.922\nPliegue: 15, Dist Clase: [4762 4738], Prec: 0.930\nPliegue: 16, Dist Clase: [4762 4738], Prec: 0.928\nPliegue: 17, Dist Clase: [4762 4738], Prec: 0.924\nPliegue: 18, Dist Clase: [4762 4738], Prec: 0.926\nPliegue: 19, Dist Clase: [4762 4738], Prec: 0.936\nPliegue: 20, Dist Clase: [4762 4738], Prec: 0.920\n</pre> <p>En este ejemplo, utilizamos el iterador <code>StratifiedKFold</code> que nos proporciona Scikit-learn. Este iterador es una versi\u00f3n mejorada de la validaci\u00f3n cruzada, ya que cada pliegue va a estar estratificado para mantener las proporciones entre las clases del conjunto de datos original, lo que suele dar mejores estimaciones del sesgo y la varianza del modelo.</p> <p>Tambi\u00e9n podr\u00edamos utilizar <code>cross_val_score</code> que ya nos proporciona los resultados de la precisi\u00f3n que tuvo el modelo en cada pliegue.</p> In\u00a0[4]: Copied! <pre># Ejemplo con cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# separ los datos en train y eval\nx_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35, \n                                                    train_size=0.65,\n                                                    random_state=1982)\n\n\nmodel = DecisionTreeClassifier(criterion='entropy',\n                               max_depth=5)\n\n\nprecision = cross_val_score(estimator=model,\n                            X=x_train,\n                            y=y_train,\n                            cv=20)\n</pre> # Ejemplo con cross_val_score from sklearn.model_selection import cross_val_score  # separ los datos en train y eval x_train, x_eval, y_train, y_eval = train_test_split(X, y, test_size=0.35,                                                      train_size=0.65,                                                     random_state=1982)   model = DecisionTreeClassifier(criterion='entropy',                                max_depth=5)   precision = cross_val_score(estimator=model,                             X=x_train,                             y=y_train,                             cv=20) In\u00a0[5]: Copied! <pre>precision = [round(x,2) for x in precision]\nprint('Precisiones: {} '.format(precision))\nprint('Precision promedio: {0: .3f} +/- {1: .3f}'.format(np.mean(precision),\n                                          np.std(precision)))\n</pre> precision = [round(x,2) for x in precision] print('Precisiones: {} '.format(precision)) print('Precision promedio: {0: .3f} +/- {1: .3f}'.format(np.mean(precision),                                           np.std(precision))) <pre>Precisiones: [0.93, 0.94, 0.92, 0.94, 0.93, 0.9, 0.92, 0.94, 0.94, 0.93, 0.94, 0.92, 0.91, 0.91, 0.93, 0.94, 0.93, 0.93, 0.92, 0.93] \nPrecision promedio:  0.927 +/-  0.011\n</pre> <p>Para graficar las curvas de aprendizaje es necesario ocupar el comando de sklearn llamado <code>learning_curve</code>.</p> In\u00a0[6]: Copied! <pre># Ejemplo Curvas de aprendizaje\nfrom sklearn.model_selection import  learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n                        estimator=model,\n                        X=x_train,\n                        y=y_train, \n                        train_sizes=np.linspace(0.1, 1.0, 20),\n                        cv=10,\n                        n_jobs=-1\n                        )\n\n# calculo de metricas\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n</pre> # Ejemplo Curvas de aprendizaje from sklearn.model_selection import  learning_curve  train_sizes, train_scores, test_scores = learning_curve(                         estimator=model,                         X=x_train,                         y=y_train,                          train_sizes=np.linspace(0.1, 1.0, 20),                         cv=10,                         n_jobs=-1                         )  # calculo de metricas train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) <p>Veamos que el comando <code>learning_curve</code> va creando conjunto de datos, pero de distintos tama\u00f1os.</p> In\u00a0[7]: Copied! <pre># tamano conjunto de entrenamiento\nfor k in range(len(train_sizes)):\n    print('Tama\u00f1o Conjunto {}: {}'.format(k+1,train_sizes[k]))\n</pre> # tamano conjunto de entrenamiento for k in range(len(train_sizes)):     print('Tama\u00f1o Conjunto {}: {}'.format(k+1,train_sizes[k])) <pre>Tama\u00f1o Conjunto 1: 585\nTama\u00f1o Conjunto 2: 862\nTama\u00f1o Conjunto 3: 1139\nTama\u00f1o Conjunto 4: 1416\nTama\u00f1o Conjunto 5: 1693\nTama\u00f1o Conjunto 6: 1970\nTama\u00f1o Conjunto 7: 2247\nTama\u00f1o Conjunto 8: 2524\nTama\u00f1o Conjunto 9: 2801\nTama\u00f1o Conjunto 10: 3078\nTama\u00f1o Conjunto 11: 3356\nTama\u00f1o Conjunto 12: 3633\nTama\u00f1o Conjunto 13: 3910\nTama\u00f1o Conjunto 14: 4187\nTama\u00f1o Conjunto 15: 4464\nTama\u00f1o Conjunto 16: 4741\nTama\u00f1o Conjunto 17: 5018\nTama\u00f1o Conjunto 18: 5295\nTama\u00f1o Conjunto 19: 5572\nTama\u00f1o Conjunto 20: 5850\n</pre> <p>Finalmente, graficamos las precisiones tanto para el conjunto de entranamiento como de evaluaci\u00f3n para los distintos conjuntos de datos generados.</p> In\u00a0[8]: Copied! <pre># graficando las curvas\nplt.figure(figsize=(12,8))\n\nplt.plot(train_sizes, train_mean, color='r', marker='o', markersize=5,\n         label='entrenamiento')\nplt.fill_between(train_sizes, train_mean + train_std, \n                 train_mean - train_std, alpha=0.15, color='r')\nplt.plot(train_sizes, test_mean, color='b', linestyle='--', \n         marker='s', markersize=5, label='evaluacion')\nplt.fill_between(train_sizes, test_mean + test_std, \n                 test_mean - test_std, alpha=0.15, color='b')\nplt.grid()\nplt.title('Curva de aprendizaje')\nplt.legend(loc='upper right')\nplt.xlabel('Cant de ejemplos de entrenamiento')\nplt.ylabel('Precision')\nplt.show()\n</pre> # graficando las curvas plt.figure(figsize=(12,8))  plt.plot(train_sizes, train_mean, color='r', marker='o', markersize=5,          label='entrenamiento') plt.fill_between(train_sizes, train_mean + train_std,                   train_mean - train_std, alpha=0.15, color='r') plt.plot(train_sizes, test_mean, color='b', linestyle='--',           marker='s', markersize=5, label='evaluacion') plt.fill_between(train_sizes, test_mean + test_std,                   test_mean - test_std, alpha=0.15, color='b') plt.grid() plt.title('Curva de aprendizaje') plt.legend(loc='upper right') plt.xlabel('Cant de ejemplos de entrenamiento') plt.ylabel('Precision') plt.show() <p>En este gr\u00e1fico podemos concluir que:</p> <ul> <li><p>Con pocos datos la precisi\u00f3n entre los datos de entrenamiento y los de evaluaci\u00f3n son muy distintas y luego a medida que la cantidad de datos va aumentando, el modelo puede generalizar mucho mejor y las precisiones se comienzan a emparejar.</p> </li> <li><p>Este gr\u00e1fico tambi\u00e9n puede ser importante a la hora de decidir invertir en la obtenci\u00f3n de m\u00e1s datos, ya que por ejemplo nos indica que a partir las 2500 muestras, el modelo ya no gana mucha m\u00e1s precisi\u00f3n a pesar de obtener m\u00e1s datos.</p> </li> </ul> In\u00a0[9]: Copied! <pre># Ejemplo de grid search con SVM.\nfrom sklearn.model_selection import GridSearchCV\n\n# creaci\u00f3n del modelo\nmodel = DecisionTreeClassifier()\n\n# rango de parametros\nrango_criterion = ['gini','entropy']\nrango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150])\nparam_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth)\nparam_grid\n</pre> # Ejemplo de grid search con SVM. from sklearn.model_selection import GridSearchCV  # creaci\u00f3n del modelo model = DecisionTreeClassifier()  # rango de parametros rango_criterion = ['gini','entropy'] rango_max_depth =np.array( [4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]) param_grid = dict(criterion=rango_criterion, max_depth=rango_max_depth) param_grid Out[9]: <pre>{'criterion': ['gini', 'entropy'],\n 'max_depth': array([  4,   5,   6,   7,   8,   9,  10,  11,  12,  15,  20,  30,  40,\n         50,  70,  90, 120, 150])}</pre> In\u00a0[10]: Copied! <pre># aplicar greed search\n\ngs = GridSearchCV(estimator=model, \n                  param_grid=param_grid, \n                  scoring='accuracy',\n                  cv=5,\n                  n_jobs=-1)\n\ngs = gs.fit(x_train, y_train)\n</pre> # aplicar greed search  gs = GridSearchCV(estimator=model,                    param_grid=param_grid,                    scoring='accuracy',                   cv=5,                   n_jobs=-1)  gs = gs.fit(x_train, y_train) In\u00a0[11]: Copied! <pre># imprimir resultados\nprint(gs.best_score_)\nprint(gs.best_params_)\n</pre> # imprimir resultados print(gs.best_score_) print(gs.best_params_) <pre>0.9332307692307692\n{'criterion': 'entropy', 'max_depth': 6}\n</pre> In\u00a0[12]: Copied! <pre># utilizando el mejor modelo\nmejor_modelo = gs.best_estimator_\nmejor_modelo.fit(x_train, y_train)\nprint('Precisi\u00f3n: {0:.3f}'.format(mejor_modelo.score(x_eval, y_eval)))\n</pre> # utilizando el mejor modelo mejor_modelo = gs.best_estimator_ mejor_modelo.fit(x_train, y_train) print('Precisi\u00f3n: {0:.3f}'.format(mejor_modelo.score(x_eval, y_eval))) <pre>Precisi\u00f3n: 0.939\n</pre> <p>En este ejemplo, primero utilizamos el objeto <code>GridSearchCV</code> que nos permite realizar grid search junto con validaci\u00f3n cruzada, luego comenzamos a ajustar el modelo con las diferentes combinaciones de los valores de los par\u00e1metros <code>criterion</code> y <code>max_depth</code>. Finalmente imprimimos el mejor resultado de precisi\u00f3n y los valores de los par\u00e1metros que utilizamos para obtenerlos; por \u00faltimo utilizamos este mejor modelo para realizar las predicciones con los datos de evaluaci\u00f3n.</p> <p>Podemos ver que la precisi\u00f3n que obtuvimos con los datos de evaluaci\u00f3n es casi id\u00e9ntica a la que nos indic\u00f3 grid search, lo que indica que el modelo generaliza muy bien.</p> <p>Algoritmos para selecci\u00f3n de atributos</p> <p>Podemos encontrar dos clases generales de algoritmos de selecci\u00f3n de atributos: los m\u00e9todos de filtrado, y los m\u00e9todos empaquetados.</p> <ul> <li><p>M\u00e9todos de filtrado:  Estos m\u00e9todos aplican una medida estad\u00edstica para asignar una puntuaci\u00f3n a cada atributo. Los atributos luego son clasificados de acuerdo a su puntuaci\u00f3n y son, o bien seleccionados para su conservaci\u00f3n o eliminados del conjunto de datos. Los m\u00e9todos de filtrado son a menudo univariantes y consideran a cada atributo en forma independiente, o con respecto a la variable dependiente.</p> <ul> <li>Ejemplos : prueba de Chi cuadrado, prueba F de Fisher, ratio de ganancia de informaci\u00f3n y los coeficientes de correlaci\u00f3n.</li> </ul> </li> <li><p>M\u00e9todos empaquetados: Estos m\u00e9todos consideran la selecci\u00f3n de un conjunto de atributos como un problema de b\u00fasqueda, en donde las diferentes combinaciones son evaluadas y comparadas. Para hacer estas evaluaciones se utiliza un modelo predictivo y luego se asigna una puntuaci\u00f3n a cada combinaci\u00f3n basada en la precisi\u00f3n del modelo.</p> <ul> <li>Un ejemplo de este m\u00e9todo es el algoritmo de eliminaci\u00f3n recursiva de atributos.</li> </ul> </li> </ul> <p>Un m\u00e9todo popular en sklearn es el m\u00e9todo SelectKBest, el cual selecciona las  caracter\u00edsticas de acuerdo con las $k$ puntuaciones m\u00e1s altas (de acuerdo al criterio escogido).</p> <p>Para entender este conceptos, transformemos el conjunto de datos anterior a formato pandas DataFrame.</p> In\u00a0[13]: Copied! <pre># Datos\nX, y = make_classification(10000, 100, n_informative=3, n_classes=2,\n                          random_state=1982)\n\ndf = pd.DataFrame(X)\ndf.columns = [f'V{k}' for k in range(1,X.shape[1]+1)]\ndf['y']=y\ndf.head()\n</pre> # Datos X, y = make_classification(10000, 100, n_informative=3, n_classes=2,                           random_state=1982)  df = pd.DataFrame(X) df.columns = [f'V{k}' for k in range(1,X.shape[1]+1)] df['y']=y df.head() Out[13]: V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 ... V92 V93 V94 V95 V96 V97 V98 V99 V100 y 0 0.949283 -1.075706 -0.105733 -0.000047 -0.278974 0.510083 -0.778030 -1.976158 -1.201534 -1.047384 ... -0.630209 -0.331225 -0.202422 -1.786323 1.540031 1.119424 0.507775 -0.848286 -0.027485 1 1 0.183904 0.524554 -1.561357 -1.950628 1.077846 -0.598287 0.153160 -1.206113 0.673170 -0.843770 ... -1.015067 0.319214 0.240570 -2.205400 -0.430933 -0.313175 0.752012 -0.070265 1.390394 0 2 0.499151 -0.625950 2.977037 0.612030 -0.102034 2.076814 1.661343 1.310895 -1.115465 -0.544276 ... 0.311830 -1.130865 0.247865 -0.499241 -1.595737 -0.496805 -0.917257 0.976909 -1.518979 0 3 -0.172063 -0.599516 0.154253 -0.593797 0.931374 0.939714 1.107241 0.146723 -0.446275 0.095896 ... -1.641808 -1.170021 0.815094 -0.722564 -0.263476 -0.715898 1.962313 1.076288 -2.259682 0 4 -0.396408 0.876210 -0.791795 0.999677 0.046859 -0.166211 -0.549437 0.344644 0.349981 -0.207106 ... 1.307020 0.876912 0.882497 -0.704791 -0.743942 -0.075060 0.622693 0.751576 0.907325 0 <p>5 rows \u00d7 101 columns</p> <p>Comencemos con un simple algoritmo univariante que aplica el m\u00e9todo de filtrado. Para esto vamos a utilizar los objetos <code>SelectKBest</code> y <code>f_classif</code> del paquete <code>sklearn.feature_selection</code>.</p> <p>Este algoritmo selecciona a los mejores atributos bas\u00e1ndose en una prueba estad\u00edstica univariante. Al objeto <code>SelectKBest</code> le pasamos la prueba estad\u00edstica que vamos a a aplicar, en este caso una prueba F definida por el objeto <code>f_classif</code>, junto con el n\u00famero de atributos a seleccionar. El algoritmo va a aplicar la prueba a todos los atributos y va a seleccionar los que mejor resultado obtuvieron.</p> In\u00a0[14]: Copied! <pre>from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n</pre> from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import f_classif In\u00a0[15]: Copied! <pre># Separamos las columnas objetivo\nx_training = df.drop(['y',], axis=1)\ny_training = df['y']\n\n# Aplicando el algoritmo univariante de prueba F.\nk = 15  # n\u00famero de atributos a seleccionar\ncolumnas = list(x_training.columns.values)\nseleccionadas = SelectKBest(f_classif, k=k).fit(x_training, y_training)\n</pre> # Separamos las columnas objetivo x_training = df.drop(['y',], axis=1) y_training = df['y']  # Aplicando el algoritmo univariante de prueba F. k = 15  # n\u00famero de atributos a seleccionar columnas = list(x_training.columns.values) seleccionadas = SelectKBest(f_classif, k=k).fit(x_training, y_training) In\u00a0[16]: Copied! <pre>catrib = seleccionadas.get_support()\natributos = [columnas[i] for i in list(catrib.nonzero()[0])]\natributos\n</pre> catrib = seleccionadas.get_support() atributos = [columnas[i] for i in list(catrib.nonzero()[0])] atributos Out[16]: <pre>['V1',\n 'V42',\n 'V46',\n 'V49',\n 'V62',\n 'V64',\n 'V66',\n 'V68',\n 'V69',\n 'V75',\n 'V82',\n 'V86',\n 'V89',\n 'V98',\n 'V100']</pre> <p>Como podemos ver, el algoritmo nos seleccion\u00f3 la cantidad de atributos que le indicamos; en este ejemplo decidimos seleccionar solo 15; obviamente, cuando armemos nuestro modelo final vamos a tomar un n\u00famero mayor de atributos. Ahora se proceder\u00e1 a comparar los resultados de entrenar un modelo en particular con todas las variables y el subconjunto de variables seleccionadas.</p> In\u00a0[23]: Copied! <pre>import time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n</pre> import time from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression In\u00a0[18]: Copied! <pre>from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# Evaluar las m\u00e9tricas\ndef classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: dataframe con las columnas: ['y', 'yhat']\n    :return: dataframe con las m\u00e9tricas especificadas\n    \"\"\"\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    accuracy = round(accuracy_score(y_true, y_pred), 4)\n    recall = round(recall_score(y_true, y_pred, average='macro'), 4)\n    precision = round(precision_score(y_true, y_pred, average='macro'), 4)\n    fscore = round(f1_score(y_true, y_pred, average='macro'), 4)\n\n    df_result = pd.DataFrame({'accuracy': [accuracy],\n                              'recall': [recall],\n                              'precision': [precision],\n                              'fscore': [fscore]})\n\n    return df_result\n</pre> from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score  # Evaluar las m\u00e9tricas def classification_metrics(df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: dataframe con las columnas: ['y', 'yhat']     :return: dataframe con las m\u00e9tricas especificadas     \"\"\"     y_true = df['y']     y_pred = df['yhat']      accuracy = round(accuracy_score(y_true, y_pred), 4)     recall = round(recall_score(y_true, y_pred, average='macro'), 4)     precision = round(precision_score(y_true, y_pred, average='macro'), 4)     fscore = round(f1_score(y_true, y_pred, average='macro'), 4)      df_result = pd.DataFrame({'accuracy': [accuracy],                               'recall': [recall],                               'precision': [precision],                               'fscore': [fscore]})      return df_result In\u00a0[24]: Copied! <pre># Record start time\nstart_time = time.time()\n\n# Entrenamiento con todas las variables \nX = df.drop('y', axis=1)\nY = df['y']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n\n# Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n\npredicciones = rlog.predict(X_test)\n\ndf_pred = pd.DataFrame({\n    'y': Y_test,\n    'yhat': predicciones\n})\n\ndf_s1 = classification_metrics(df_pred).assign(name='Todas las variables')\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\nprint(\"Elapsed time:\", elapsed_time, \"seconds\")\n</pre> # Record start time start_time = time.time()  # Entrenamiento con todas las variables  X = df.drop('y', axis=1) Y = df['y']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)  # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo  predicciones = rlog.predict(X_test)  df_pred = pd.DataFrame({     'y': Y_test,     'yhat': predicciones })  df_s1 = classification_metrics(df_pred).assign(name='Todas las variables')  # Calculate the elapsed time elapsed_time = time.time() - start_time print(\"Elapsed time:\", elapsed_time, \"seconds\") <pre>Elapsed time: 0.027977705001831055 seconds\n</pre> In\u00a0[25]: Copied! <pre># Record start time\nstart_time = time.time()\n\n# Entrenamiento con las variables seleccionadas\nX = df[atributos]\nY = df['y']\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n\n# Creando el modelo\nrlog = LogisticRegression()\nrlog.fit(X_train, Y_train) # ajustando el modelo\n\npredicciones = rlog.predict(X_test)\n\ndf_pred = pd.DataFrame({\n    'y': Y_test,\n    'yhat': predicciones\n})\n\ndf_s2 = classification_metrics(df_pred).assign(name='Variables Seleccionadas')\n\n# Calculate the elapsed time\nelapsed_time = time.time() - start_time\nprint(\"Elapsed time:\", elapsed_time, \"seconds\")\n</pre> # Record start time start_time = time.time()  # Entrenamiento con las variables seleccionadas X = df[atributos] Y = df['y']  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)  # Creando el modelo rlog = LogisticRegression() rlog.fit(X_train, Y_train) # ajustando el modelo  predicciones = rlog.predict(X_test)  df_pred = pd.DataFrame({     'y': Y_test,     'yhat': predicciones })  df_s2 = classification_metrics(df_pred).assign(name='Variables Seleccionadas')  # Calculate the elapsed time elapsed_time = time.time() - start_time print(\"Elapsed time:\", elapsed_time, \"seconds\")  <pre>Elapsed time: 0.025048255920410156 seconds\n</pre> <p>Juntando ambos resultados:</p> In\u00a0[26]: Copied! <pre># juntar resultados en formato dataframe\npd.concat([df_s1,df_s2])\n</pre> # juntar resultados en formato dataframe pd.concat([df_s1,df_s2]) Out[26]: accuracy recall precision fscore name 0 0.8905 0.8906 0.8907 0.8905 Todas las variables 0 0.8985 0.8986 0.8986 0.8985 Variables Seleccionadas <p>Las m\u00e9tricas para ambos casos son parecidas y el tiempo de ejecuci\u00f3n del modelo con menos variable resulta ser menor (lo cual era algo esperable). Lo cual nos muestra que trabajando con menos variables, se puede captar las caracter\u00edsticas m\u00e1s relevante del problema, y en la medida que se trabaje con m\u00e1s datos, las mejoras a nivel de capacidad de c\u00f3mputo tendr\u00e1n un mejor desempe\u00f1o.</p>"},{"location":"machine_learning/over_02/#overfitting-ii","title":"Overfitting II\u00b6","text":"<p>Algunas de las t\u00e9cnicas que podemos utilizar para reducir el overfitting, son:</p> <ul> <li>Recolectar m\u00e1s datos.</li> <li>Introducir una penalizaci\u00f3n a la complejidad con alguna t\u00e9cnica de regularizaci\u00f3n.</li> <li>Utilizar modelos ensamblados.</li> <li>Utilizar validaci\u00f3n cruzada.</li> <li>Optimizar los par\u00e1metros del modelo con grid search.</li> <li>Reducir la dimensi\u00f3n de los datos.</li> <li>Aplicar t\u00e9cnicas de selecci\u00f3n de atributos.</li> </ul> <p>Veremos ejemplos de algunos m\u00e9todos para reducir el sobreajuste (overfitting).</p>"},{"location":"machine_learning/over_02/#validacion-cruzada","title":"Validaci\u00f3n cruzada\u00b6","text":"<p>La validaci\u00f3n cruzada se inicia mediante el fraccionamiento de un conjunto de datos en un n\u00famero $k$ de particiones (generalmente entre 5 y 10) llamadas pliegues.</p> <p>La validaci\u00f3n cruzada luego itera entre los datos de evaluaci\u00f3n y entrenamiento $k$ veces, de un modo particular. En cada iteraci\u00f3n de la validaci\u00f3n cruzada, un pliegue diferente se elige como los datos de evaluaci\u00f3n. En esta iteraci\u00f3n, los otros pliegues $k-1$ se combinan para formar los datos de entrenamiento. Por lo tanto, en cada iteraci\u00f3n tenemos $(k-1) / k$ de los datos utilizados para el entrenamiento y $1 / k$ utilizado para la evaluaci\u00f3n.</p> <p>Cada iteraci\u00f3n produce un modelo, y por lo tanto una estimaci\u00f3n del rendimiento de la generalizaci\u00f3n, por ejemplo, una estimaci\u00f3n de la precisi\u00f3n. Una vez finalizada la validaci\u00f3n cruzada, todos los ejemplos se han utilizado s\u00f3lo una vez para evaluar pero $k -1$ veces para entrenar. En este punto tenemos estimaciones de rendimiento de todos los pliegues y podemos calcular la media y la desviaci\u00f3n est\u00e1ndar de la precisi\u00f3n del modelo.</p> <p></p>"},{"location":"machine_learning/over_02/#mas-datos-y-curvas-de-aprendizaje","title":"M\u00e1s datos y curvas de aprendizaje\u00b6","text":"<ul> <li>Muchas veces, reducir el Sobreajuste es tan f\u00e1cil como conseguir m\u00e1s datos, dame m\u00e1s datos y te predecir\u00e9 el futuro!.</li> <li>En la vida real nunca es una tarea tan sencilla conseguir m\u00e1s datos.</li> <li>Una t\u00e9cnica para reducir el sobreajuste son las curvas de aprendizaje, las cuales grafican la precisi\u00f3n en funci\u00f3n del tama\u00f1o de los datos de entrenamiento.</li> </ul>"},{"location":"machine_learning/over_02/#optimizacion-de-parametros-con-grid-search","title":"Optimizaci\u00f3n de par\u00e1metros con Grid Search\u00b6","text":"<p>La mayor\u00eda de los modelos de Machine Learning cuentan con varios par\u00e1metros para ajustar su comportamiento, por lo tanto, otra alternativa que tenemos para reducir el Sobreajuste es optimizar estos par\u00e1metros por medio de un proceso conocido como grid search e intentar encontrar la combinaci\u00f3n ideal que nos proporcione mayor precisi\u00f3n.</p> <p>El enfoque que utiliza grid search es bastante simple, se trata de una b\u00fasqueda exhaustiva por el paradigma de fuerza bruta en el que se especifica una lista de valores para diferentes par\u00e1metros, y la computadora eval\u00faa el rendimiento del modelo para cada combinaci\u00f3n de \u00e9stos par\u00e1metros para obtener el conjunto \u00f3ptimo que nos brinda el mayor rendimiento.</p> <p></p>"},{"location":"machine_learning/over_02/#reduccion-de-dimensionalidad","title":"Reducci\u00f3n de dimensionalidad\u00b6","text":"<p>La reducci\u00f3n de dimensiones es frecuentemente usada como una etapa de preproceso en el entrenamiento de sistemas, y consiste en escoger un subconjunto de variables, de tal manera, que el espacio de caracter\u00edsticas quede \u00f3ptimamente reducido de acuerdo a un criterio de evaluaci\u00f3n, cuyo fin es distinguir el subconjunto que representa mejor el espacio inicial de entrenamiento.</p> <p>Como cada caracter\u00edstica que se incluye en el an\u00e1lisis, puede incrementar el costo y el tiempo de proceso de los sistemas, hay una fuerte motivaci\u00f3n para dise\u00f1ar e implementar sistemas con peque\u00f1os conjuntos de caracter\u00edsticas. Sin dejar de lado, que al mismo tiempo, hay una opuesta necesidad de incluir un conjunto suficiente de caracter\u00edsticas para lograr un alto rendimiento.</p> <p>La reducci\u00f3n de dimensionalidad se puede separar en dos tipos: Extracci\u00f3n de atributos y  Selecci\u00f3n de aributos.</p>"},{"location":"machine_learning/over_02/#extraccion-de-atributos","title":"Extracci\u00f3n de atributos\u00b6","text":"<p>La extracci\u00f3n de atributos comienza a partir de un conjunto inicial de datos medidos y crea valores derivados (caracter\u00edsticas) destinados a ser informativos y no redundantes, lo que facilita los pasos de aprendizaje y generalizaci\u00f3n posteriores, y en algunos casos conduce a a mejores interpretaciones humanas.</p> <p>Cuando los datos de entrada a un algoritmo son demasiado grandes para ser procesados y se sospecha que son redundantes (por ejemplo, la misma medici\u00f3n en pies y metros, o la repetitividad de las im\u00e1genes presentadas como p\u00edxeles), entonces se puede transformar en un conjunto reducido de caracter\u00edsticas (tambi\u00e9n denominado un vector de caracter\u00edsticas).</p> <p>Estos algoritmos fueron analizados con profundidad en la secci\u00f3n de An\u00e1lisis no supervisados - Reducci\u00f3n de la dimensionalidad.</p>"},{"location":"machine_learning/over_02/#seleccion-de-atributos","title":"Selecci\u00f3n de atributos\u00b6","text":"<p>Proceso por el cual seleccionamos un subconjunto de atributos (representados por cada una de las columnas en un datasetde forma tabular) que son m\u00e1s relevantes para la construcci\u00f3n del modelo predictivo sobre el que estamos trabajando.</p> <p>El objetivo de la selecci\u00f3n de atributos es :</p> <ul> <li>mejorar la capacidad predictiva de nuestro modelo,</li> <li>proporcionando modelos predictivos m\u00e1s r\u00e1pidos y eficientes,</li> <li>proporcionar una mejor comprensi\u00f3n del proceso subyacente que gener\u00f3 los datos.</li> </ul> <p>Los m\u00e9todos de selecci\u00f3n de atributos se pueden utilizar para identificar y eliminar los atributos innecesarios, irrelevantes y redundantes que no contribuyen a la exactitud del modelo predictivo o incluso puedan disminuir su precisi\u00f3n.</p>"},{"location":"machine_learning/over_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>K-Fold Cross Validation</li> <li>Cross Validation and Grid Search for Model Selection in Python</li> <li>Feature selection for supervised models using SelectKBest</li> </ol>"},{"location":"machine_learning/reg_01/","title":"Regresi\u00f3n I","text":"<p>Existen algunas situaciones donde los modelos lineales no son apropiados:</p> <ul> <li>El rango de valores de $Y$ est\u00e1 restringido (ejemplo: datos binarios o de conteos).</li> <li>La varianza de $Y$ depende de la media.</li> </ul> <p>La metodolog\u00eda para encontrar los par\u00e1metros $\\beta$ para el caso de la regresi\u00f3n lineal multiple se extienden de manera natural del modelo de regresi\u00f3n lineal multiple, cuya soluci\u00f3n viene dada por:</p> <p>$$\\beta = (XX^{\\top})^{-1}X^{\\top}y$$</p> <ol> <li><p>M\u00e9tricas absolutas: Las m\u00e9tricas absolutas o no escalada miden el error sin escalar los valores. Las m\u00e9trica absolutas m\u00e1s ocupadas son:</p> <ul> <li>Mean Absolute Error (MAE)</li> </ul> <p>$$\\textrm{MAE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | y_{t}-\\hat{y}_{t}\\right |$$</p> <ul> <li>Mean squared error (MSE):</li> </ul> <p>$$\\textrm{MSE}(y,\\hat{y}) =\\dfrac{1}{n}\\sum_{t=1}^{n}\\left ( y_{t}-\\hat{y}_{t}\\right )^2$$</p> </li> </ol> <ol> <li><p>M\u00e9tricas Porcentuales: Las m\u00e9tricas porcentuales o escaladas miden el error de manera escalada, es decir, se busca acotar el error entre valores de 0 a 1, donde 0 significa que el ajuste es perfecto, mientras que 1 ser\u00eda un mal ajuste. Cabe destacar que muchas veces las m\u00e9tricas porcentuales puden tener valores mayores a 1.Las m\u00e9trica Porcentuales m\u00e1s ocupadas son:</p> <ul> <li>Mean absolute percentage error (MAPE):</li> </ul> <p>$$\\textrm{MAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n}\\left | \\frac{y_{t}-\\hat{y}_{t}}{y_{t}} \\right |$$</p> <ul> <li>Symmetric mean absolute percentage error (sMAPE):</li> </ul> <p>$$\\textrm{sMAPE}(y,\\hat{y}) = \\dfrac{1}{n}\\sum_{t=1}^{n} \\frac{\\left |y_{t}-\\hat{y}_{t}\\right |}{(\\left | y_{t} \\right |^2+\\left | \\hat{y}_{t} \\right |^2)/2}$$</p> </li> </ol> <p>IMPORTANTE:</p> <ul> <li><p>Cabe destacar que el coeficiente $r^2$ funciona bien en el contexto del mundo de las regresiones lineales. Para el an\u00e1lisis de modelos no lineales, esto coeficiente pierde su interpretaci\u00f3n.</p> </li> <li><p>Se deja la siguiente refrerencia para comprender conceptos claves de test de hip\u00f3tesis, intervalos de confianza, p-valor. Estos t\u00e9rminos son escenciales para comprender la significancia del ajuste realizado.</p> </li> <li><p>Existen muchas m\u00e1s m\u00e9tricas, pero estas son las m\u00e1s usulaes de encontrar. En el archivo metrics.py se definen las distintas m\u00e9tricas presentadas, las cuales serpan de utilidad m\u00e1s adelante.</p> </li> </ul> In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># ejemplo sencillo\n\nn = 100 \nnp.random.seed(n)\n\nbeta = np.array([1,1]) # coeficientes\nx =  np.random.rand(n) # variable independiente\n\nmu, sigma = 0, 0.1 # media y desviacion estandar\nepsilon = np.random.normal(mu, sigma, n) # ruido blanco\n\ny = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes\n\n# generar dataframe\ndf = pd.DataFrame({\n    'x':x,\n    'y':y\n})\ndf.head()\n</pre> # ejemplo sencillo  n = 100  np.random.seed(n)  beta = np.array([1,1]) # coeficientes x =  np.random.rand(n) # variable independiente  mu, sigma = 0, 0.1 # media y desviacion estandar epsilon = np.random.normal(mu, sigma, n) # ruido blanco  y = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes  # generar dataframe df = pd.DataFrame({     'x':x,     'y':y }) df.head() Out[2]: x y 0 0.543405 1.612417 1 0.278369 1.347058 2 0.424518 1.267849 3 0.844776 1.935274 4 0.004719 1.082601 <p>Grafiquemos los puntos en el plano cartesiano.</p> In\u00a0[3]: Copied! <pre># grafico de puntos\nsns.set(rc={'figure.figsize':(10,8)})\nsns.scatterplot(\n    x='x',\n    y='y',\n    data=df,\n)  \nplt.show()\n</pre> # grafico de puntos sns.set(rc={'figure.figsize':(10,8)}) sns.scatterplot(     x='x',     y='y',     data=df, )   plt.show() <p>Lo primero que debemos hacer es separar nuestro datos en los conjuntos de training set y test set. Concepto de  Train set y Test set</p> <p>Al momento de entrenar los modelos de machine leraning, se debe tener un conjunto para poder entrenar el modelo y otro conjunto para poder evaluar el modelo. Es por esto que el conjunto de datos se separ\u00e1 en dos conjuntos:</p> <ul> <li><p>Train set: Conjunto de entrenamiento con el cual se entrenar\u00e1n los algoritmos de machine learning.</p> </li> <li><p>Test set: Conjunto de testeo para averiguar la confiabilidad del modelo, es decir, cuan bueno es el ajuste del modelo.</p> </li> </ul> <p></p> <p>Tama\u00f1o ideal de cada conjunto</p> <p>La respuesta depende fuertemente del tama\u00f1o del conjunto de datos. A modo de regla emp\u00edrica, se considerar\u00e1 el tama\u00f1o \u00f3ptimo basado en la siguiente tabla:</p> n\u00famero de filas train set test set entre 100-1000 67% 33% entre 1.000- 100.000 80% 20% mayor a 100.000 99% 1% In\u00a0[4]: Copied! <pre>from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\n# import some data to play with\n\nX = df[['x']] # we only take the first two features.\ny = df['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# print rows train and test sets\nprint('Separando informacion:\\n')\nprint('numero de filas data original : ',len(X))\nprint('numero de filas train set     : ',len(X_train))\nprint('numero de filas test set      : ',len(X_test))\n</pre> from sklearn import datasets from sklearn.model_selection import train_test_split  # import some data to play with  X = df[['x']] # we only take the first two features. y = df['y']  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # print rows train and test sets print('Separando informacion:\\n') print('numero de filas data original : ',len(X)) print('numero de filas train set     : ',len(X_train)) print('numero de filas test set      : ',len(X_test)) <pre>Separando informacion:\n\nnumero de filas data original :  100\nnumero de filas train set     :  80\nnumero de filas test set      :  20\n</pre> <p>Existen varias librer\u00edas para poder aplicar modelos de regresi\u00f3n, de los cuales la atenci\u00f3n estar\u00e1 enfocada en las librer\u00edas de <code>statsmodels</code> y <code>sklearn</code>.</p> In\u00a0[5]: Copied! <pre>import statsmodels.api as sm\n\nmodel = sm.OLS(y_train, sm.add_constant(X_train))\nresults = model.fit()\n</pre> import statsmodels.api as sm  model = sm.OLS(y_train, sm.add_constant(X_train)) results = model.fit() <p>En <code>statsmodel</code> existe un comando para ver informaci\u00f3n del modelo en estudio mediante el comando <code>summary</code></p> In\u00a0[6]: Copied! <pre># resultados del modelo\nprint(results.summary())\n</pre> # resultados del modelo print(results.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.894\nModel:                            OLS   Adj. R-squared:                  0.893\nMethod:                 Least Squares   F-statistic:                     658.4\nDate:                Sat, 22 Jul 2023   Prob (F-statistic):           8.98e-40\nTime:                        18:28:07   Log-Likelihood:                 69.472\nNo. Observations:                  80   AIC:                            -134.9\nDf Residuals:                      78   BIC:                            -130.2\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.9805      0.021     46.338      0.000       0.938       1.023\nx              1.0099      0.039     25.659      0.000       0.932       1.088\n==============================================================================\nOmnibus:                        0.424   Durbin-Watson:                   1.753\nProb(Omnibus):                  0.809   Jarque-Bera (JB):                0.587\nSkew:                           0.102   Prob(JB):                        0.746\nKurtosis:                       2.633   Cond. No.                         4.17\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>A continuaci\u00f3n se dara una interpretaci\u00f3n de esta tabla:</p> <p>Descripci\u00f3n del Modelo</p> <p>Estos son estad\u00edsticas relacionadas a la ejecuci\u00f3n del modelo.</p> Variable Descripi\u00f3n Dep. Variable Nombre de la variables dependiente Model Nombre del modelo ocupado Method M\u00e9todo para encontrar los par\u00e1metros \u00f3ptimos Date Fecha de ejecuci\u00f3n No. Observations N\u00famero de observaciones Df Residuals Grados de libertas de los residuos Df Model Grados de libertad del modelo Covariance Type Tipo de covarianza <p>Ajustes del Modelo</p> <p>Estos son estad\u00edsticas relacionadas con la verosimilitud y la confiabilidad del modelo.</p> Variable Descripi\u00f3n R-squared Valor del R-cuadrado Adj. R-squared Valor del R-cuadrado ajustado F-statistic Test para ver si todos los par\u00e1metros son iguales a cero Prob (F-statistic) Probabilidad Asociada al test Log-Likelihood Logaritmo de la funci\u00f3n de verosimilitud AIC Valor del estad\u00edstico AIC BIC Valor del estad\u00edstico BIC <p>En este caso, tanto el r-cuadrado como el r-cuadrado ajustado est\u00e1n cerca del 0.9, se tiene un buen ajuste lineal de los datos. Adem\u00e1s, el test F nos da una probabilidad menor al 0.05, se rechaza la hip\u00f3tess nula que los coeficientes son iguales de cero.</p> <p>Par\u00e1metros del modelo</p> <p>La tabla muestra los valores asociados a los par\u00e1metros del modelo</p> coef std err t P&gt;|t| [0.025 0.975] const 0.9805 0.021 46.338 0.000 0.938 1.023 x 1.0099 0.039 25.659 0.000 0.932 1.088 <p>Ac\u00e1 se tiene:</p> <ul> <li>Variables: Las variables en estudio son <code>const</code> (intercepto) y <code>x</code>.</li> <li>coef: Valor estimado del coeficiente.</li> <li>std err: Desviaci\u00f3n estandar del estimador.</li> <li>t: t = estimate/std error.</li> <li>P&gt;|t|:p-valor individual para cada par\u00e1metro para aceptar o rechazar hip\u00f3tesis nula (par\u00e1metros significativamente distinto de cero).</li> <li>[0.025 | 0.975]: Intervalo de confianza de los par\u00e1metros</li> </ul> <p>En este caso, los valores estimados son cercanos a 1 (algo esperable debido a la simulaci\u00f3n realizadas), adem\u00e1s, se observa que cada uno de los par\u00e1metros es significativamente distinto de cero.</p> <p>Estad\u00edsticos interesantes del modelo</p> Variable Descripci\u00f3n Omnibus Prueba de la asimetr\u00eda y curtosis de los residuos Prob(Omnibus) Probabilidad de que los residuos se distribuyan normalmente Skew Medida de simetr\u00eda de los datos Kurtosis Medida de curvatura de los datos Durbin-Watson Pruebas de homocedasticidad Jarque-Bera (JB) Como la prueba Omnibus, prueba tanto el sesgo como la curtosis. Prob(JB) Probabilidad de que los residuos se distribuyan normalmente Cond. No. N\u00famero de condici\u00f3n. Mide la sensibilidad de la salida de una funci\u00f3n en comparaci\u00f3n con su entrada <p>En este caso:</p> <ul> <li><p>Tanto el test de Omnibus como el test  Jarque-Bera nos arroja una probabilidad cercana a uno, lo cual confirma la hip\u00f3tesis que los residuos se distribuyen de manera normal.</p> </li> <li><p>Para el test de Durbin-Watson, basados en la tablas de valores(tama\u00f1o de la muestra 80 y n\u00famero de variables 2), se tiene que los l\u00edmites para asumir que no existe correlaci\u00f3n en los residuos es de: $[d_u,4-d_u]=[1.66,2.34]$, dado que el valor obtenido (1.753) se encuentra dentro de este rango, se concluye que no hay autocorrelaci\u00f3n de los residuos.</p> </li> <li><p>El n\u00famero de condici\u00f3n es peque\u00f1o (podemos asumir que menor a 30 es un buen resultado) por lo que podemos asumir que no hay colinealidad de los datos.</p> </li> </ul> <p>Ahora, para convencernos de manera visual de los resultados, realicemos un gr\u00e1fico con el ajuste lineal:</p> In\u00a0[7]: Copied! <pre># grafico de puntos\nsns.lmplot(\n    x='x',\n    y='y',\n    data=df,\n    height = 8,\n)  \nplt.show()\n</pre> # grafico de puntos sns.lmplot(     x='x',     y='y',     data=df,     height = 8, )   plt.show() In\u00a0[8]: Copied! <pre># predicciones\ny_pred = results.predict(sm.add_constant(X_test))\n</pre> # predicciones y_pred = results.predict(sm.add_constant(X_test)) <p>Ahora, analizaremos las m\u00e9tricas de error asociado a las predicciones del modelo:</p> In\u00a0[9]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[10]: Copied! <pre>from sklearn.metrics import r2_score\n\n# ejemplo \ndf_temp = pd.DataFrame(\n    {\n        'y':y_test,\n        'yhat': y_pred\n        }\n)\n\nprint('\\nMetricas para el regresor consumo_litros_milla:\\n')\nregression_metrics(df_temp)\n</pre> from sklearn.metrics import r2_score  # ejemplo  df_temp = pd.DataFrame(     {         'y':y_test,         'yhat': y_pred         } )  print('\\nMetricas para el regresor consumo_litros_milla:\\n') regression_metrics(df_temp) <pre>\nMetricas para el regresor consumo_litros_milla:\n\n</pre> Out[10]: mae mse rmse mape smape 0 0.1028 0.0171 0.1309 6.7733 0.1269 <p>Funci\u00f3n de Autocorrelaci\u00f3n</p> <p>La funci\u00f3n de autocorrelaci\u00f3n muestra que los residuos se encuentra dentro de la banda de valores cr\u00edticos $(-0.2,0.2)$, concluyendo que no existe correlaci\u00f3n entre los residuos.</p> In\u00a0[11]: Copied! <pre>from statsmodels.graphics.tsaplots import plot_acf\nsns.set(rc={'figure.figsize':(12,8)})\n\n# funcion de autocorrelation\nplot_acf(results.resid)\nplt.show()\n</pre> from statsmodels.graphics.tsaplots import plot_acf sns.set(rc={'figure.figsize':(12,8)})  # funcion de autocorrelation plot_acf(results.resid) plt.show() <p>QQ-plot</p> <p>La gr\u00e1fica de qq-plot nos muestra una comparaci\u00f3n en las distribuci\u00f3n de los residuos respecto a una poblaci\u00f3n con una distribuci\u00f3n normal. En este caso, los puntos (que representan la distribuci\u00f3n de los errores) se encuentran cercana a la recta (distribuci\u00f3n normal), concluyendo que la distribuci\u00f3n de los residuos sigue una distribuci\u00f3n normal.</p> In\u00a0[12]: Copied! <pre>import scipy.stats as stats\nfig = sm.qqplot(results.resid, stats.t, fit=True, line=\"45\")\nplt.show()\n</pre> import scipy.stats as stats fig = sm.qqplot(results.resid, stats.t, fit=True, line=\"45\") plt.show() <p>Histograma</p> <p>Esta es una comparaci\u00f3n directa enntre la distribuci\u00f3n de los residuos versus la distribuci\u00f3n de una variable normal mediante un histograma.</p> In\u00a0[13]: Copied! <pre>df_hist = pd.DataFrame({'error':results.resid})\nsns.histplot(\n    x='error',\n    data=df_hist,\n    kde=True,\n     bins=15\n)  \nplt.show()\n</pre> df_hist = pd.DataFrame({'error':results.resid}) sns.histplot(     x='error',     data=df_hist,     kde=True,      bins=15 )   plt.show() <p>A modo de conclusi\u00f3n, es correcto asumir que los errores siguen la distribuci\u00f3n de un ruido blanco, cumpliendo correctamente con los supuestos de la regresi\u00f3n lineal.</p> In\u00a0[14]: Copied! <pre># ejemplo sencillo\n\nn = 100 \nnp.random.seed(n)\n\nbeta = np.array([1,1]) # coeficientes\nx =  np.random.rand(n) # variable independiente\n</pre> # ejemplo sencillo  n = 100  np.random.seed(n)  beta = np.array([1,1]) # coeficientes x =  np.random.rand(n) # variable independiente  In\u00a0[15]: Copied! <pre>mu, sigma = 0, 0.1 # media y desviacion estandar\nepsilon = np.random.normal(mu, sigma, n) # ruido blanco\n\ny = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes\n\ny[:10] = 3.1 # contaminacion\nx[10] = x[10]-1\ny[10]= y[10]-1\nx[11] = x[11] +1\ny[11] = y[11]+1\n\n# etiqueta\noutlier = np.zeros(n)\noutlier[:10] = 1\noutlier[10:12] = 2\n\n# generar dataframe\ndf = pd.DataFrame({\n    'x':x,\n    'y':y,\n    'outlier':outlier\n})\n</pre> mu, sigma = 0, 0.1 # media y desviacion estandar epsilon = np.random.normal(mu, sigma, n) # ruido blanco  y = np.dot(np.c_[ np.ones(n),x]  , beta) + epsilon # variables dependientes  y[:10] = 3.1 # contaminacion x[10] = x[10]-1 y[10]= y[10]-1 x[11] = x[11] +1 y[11] = y[11]+1  # etiqueta outlier = np.zeros(n) outlier[:10] = 1 outlier[10:12] = 2  # generar dataframe df = pd.DataFrame({     'x':x,     'y':y,     'outlier':outlier }) In\u00a0[16]: Copied! <pre># grafico de puntos\nsns.set(rc={'figure.figsize':(10,8)})\nsns.scatterplot(\n    x='x',\n    y='y',\n    hue='outlier',\n    data=df,\n    palette = ['blue','red','black']\n)  \nplt.show()\nplt.show()\n</pre>  # grafico de puntos sns.set(rc={'figure.figsize':(10,8)}) sns.scatterplot(     x='x',     y='y',     hue='outlier',     data=df,     palette = ['blue','red','black'] )   plt.show() plt.show() <p>En este caso, se tiene dos tipos de outliers en este caso:</p> <ul> <li>Significativos: Aquellos outliers que afectan la regresi\u00f3n cambiando la tendencia a este grupo de outliers (puntos rojos).</li> <li>No significativo: Si bien son datos at\u00edpicos  puesto que se encuentran fuera de la nube de puntos, el ajuste de la regresi\u00f3n lineal no se ve afectado (puntos negros).</li> </ul> <p>Veamos el ajuste lineal.</p> In\u00a0[17]: Copied! <pre># grafico de puntos\nsns.lmplot(\n    x='x',\n    y='y',\n    data=df,\n    height = 8,\n)  \nplt.show()\n</pre> # grafico de puntos sns.lmplot(     x='x',     y='y',     data=df,     height = 8, )   plt.show() <p>Otro gr\u00e1fico de inter\u00e9s, es el gr\u00e1fico de influencia, que analiza la distancia de Cook de los residuos.</p> In\u00a0[18]: Copied! <pre># modelos de influencia\nX = df[['x']] # we only take the first two features.\ny = df['y']\nmodel = sm.OLS(y, sm.add_constant(X))\nresults = model.fit()\nsm.graphics.influence_plot(results)\nplt.show()\n</pre> # modelos de influencia X = df[['x']] # we only take the first two features. y = df['y'] model = sm.OLS(y, sm.add_constant(X)) results = model.fit() sm.graphics.influence_plot(results) plt.show() <p>Los puntos grandes se interpretan como puntos que tienen una alta influencia sobre la regresi\u00f3n lineal, mientras aquellos puntos peque\u00f1os tienen una influencia menor.</p> <p>En este caso, la recta se ve fuertemente afectadas por estos valores. Para estos casos se pueden hacer varias cosas:</p> <ul> <li><p>Eliminaci\u00f3n de los outliers: Una vez identificado los outliers (algo que no es tan trivial de identificar para datos multivariables), se puden eliminar y seguir con el paso de modelado.</p> <ul> <li>Ventajas: F\u00e1cil de trabajar la data para los modelos que dependen fuertemente de la media de los datos.</li> <li>Desventajas: Para el caso multivariables no es t\u00e1n trivial encontrar outliers.</li> </ul> </li> <li><p>Modelos m\u00e1s robustos a outliers: Se pueden aplicar otros modelos de regresi\u00f3n cuya estimaci\u00f3n de los par\u00e1metros, no se vea afectado por los valores de outliers.</p> <ul> <li>Ventajas: El an\u00e1lisis se vuelve independiente de los datos.</li> <li>Desventajas: Modelos m\u00e1s costoso computacionalmente y/o m\u00e1s complejos de implementar.</li> </ul> </li> </ul>"},{"location":"machine_learning/reg_01/#regresion-i","title":"Regresi\u00f3n I\u00b6","text":""},{"location":"machine_learning/reg_01/#regression-lineal","title":"Regressi\u00f3n Lineal\u00b6","text":"<p>El modelo de regresio\u0301n lineal general o modelo de regresi\u00f3n multiple,  supone que, $\\boldsymbol{Y} =  \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon},$ donde:</p> <ul> <li>$\\boldsymbol{X} = (x_1,...,x_n)^{T}$: variable explicativa</li> <li>$\\boldsymbol{Y} = (y_1,...,y_n)^{T}$: variable respuesta</li> <li>$\\boldsymbol{\\epsilon} = (\\epsilon_1,...,\\epsilon_n)^{T}$: error se asume un ruido blanco, es decir, $\\epsilon \\sim \\mathcal{N}( \\boldsymbol{0},\\sigma^2I)$</li> <li>$\\boldsymbol{\\beta} = (\\beta_1,...,\\beta_n)^{T}$: coeficientes de regresio\u0301n.</li> </ul> <p>La idea es tratar de establecer la relaci\u00f3n entre las variables independientes y dependientes por medio de ajustar el mejor hyper plano con respecto a los puntos.</p> <p>Por ejemplo, para el caso de la regresi\u00f3n lineal simple, se tiene la siguiente estructura: $y_i=\\beta_0+\\beta_1x_i+\\epsilon_i.$ En este caso, la regresi\u00f3n lineal corresponder\u00e1 a la recta que mejor pasa por los puntos observados.</p>"},{"location":"machine_learning/reg_01/#mejores-paremetros-metodo-de-minimos-cudrados","title":"Mejores par\u00e9metros: M\u00e9todo de minimos cudrados\u00b6","text":"<p>El m\u00e9todo de m\u00ednimos cudrados es un m\u00e9todo de optimizaci\u00f3n que busca encontrar la mejor aproximaci\u00f3n mediante la minimizaci\u00f3n de los residuos al cuadrado, es decir, se buscar encontrar:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</p> <p>Para el caso de la regresi\u00f3n lineal simple, se busca una funci\u00f3n $$f(x;\\beta) = \\beta_{0} + \\beta_{1}x,$$</p> <p>por lo tanto el problema que se debe resolver es el siguiente:</p> <p>$$(P)\\ \\min \\sum_{i=1}^n e_{i}^2 =\\dfrac{1}{n}\\sum_{i=1}^{n}\\left ( y_{i}-(\\beta_{0} + \\beta_{1}x_{i})\\right )^2$$</p> <p>Lo que significa, que para este problema, se debe encontrar $\\beta = (\\beta_{0},\\beta_{1})$ que minimicen el problema de optimizaci\u00f3n. En este caso la soluci\u00f3n viene dada por:</p> <p>$$\\hat{\\beta}_{1} = \\dfrac{\\sum(x-\\bar{x})(y-\\bar{y})}{\\sum(x-\\bar{x})^2} = \\rho (x,y)\\ ; \\  \\hat{\\beta}_{0} = \\bar{y}-\\hat{\\beta}_{1} \\bar{x} $$</p>"},{"location":"machine_learning/reg_01/#seleccion-de-modelos","title":"Selecci\u00f3n de modelos\u00b6","text":""},{"location":"machine_learning/reg_01/#criterio-de-informacion-de-akaike-aic","title":"Criterio de informaci\u00f3n de Akaike (AIC)\u00b6","text":"<p>El criterio de informaci\u00f3n de Akaike (AIC) es una medida de la calidad relativa de un modelo estad\u00edstico, para un conjunto dado de datos. Como tal, el AIC proporciona un medio para la selecci\u00f3n del modelo.</p> <p>AIC maneja un trade-off entre la bondad de ajuste del modelo y la complejidad del modelo. Se basa en la entrop\u00eda de informaci\u00f3n: se ofrece una estimaci\u00f3n relativa de la informaci\u00f3n perdida cuando se utiliza un modelo determinado para representar el proceso que genera los datos.</p> <p>AIC no proporciona una prueba de un modelo en el sentido de probar una hip\u00f3tesis nula, es decir AIC no puede decir nada acerca de la calidad del modelo en un sentido absoluto. Si todos los modelos candidatos encajan mal, AIC no dar\u00e1 ning\u00fan aviso de ello.</p> <p>En el caso general, el AIC es</p> <p>$$AIC = 2k-2\\ln(L)$$</p> <p>donde $k$ es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico , y $L$ es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.</p>"},{"location":"machine_learning/reg_01/#criterio-de-informacion-bayesiano-bic","title":"Criterio de informaci\u00f3n bayesiano (BIC)\u00b6","text":"<p>En estad\u00edstica, el criterio de informaci\u00f3n bayesiano (BIC) o el m\u00e1s general criterio de Schwarz (SBC tambi\u00e9n, SBIC) es un criterio para la selecci\u00f3n de modelos entre un conjunto finito de modelos. Se basa, en parte, de la funci\u00f3n de probabilidad y que est\u00e1 estrechamente relacionado con el Criterio de Informaci\u00f3n de Akaike (AIC).</p> <p>Cuando el ajuste de modelos, es posible aumentar la probabilidad mediante la adici\u00f3n de par\u00e1metros, pero si lo hace puede resultar en sobreajuste. Tanto el BIC y AIC resuelven este problema mediante la introducci\u00f3n de un t\u00e9rmino de penalizaci\u00f3n para el n\u00famero de par\u00e1metros en el modelo, el t\u00e9rmino de penalizaci\u00f3n es mayor en el BIC que en el AIC.</p> <p>El BIC fue desarrollado por Gideon E. Schwarz, quien dio un argumento bayesiano a favor de su adopci\u00f3n.1\u200b Akaike tambi\u00e9n desarroll\u00f3 su propio formalismo Bayesiano, que ahora se conoce como la ABIC por Criterio de Informaci\u00f3n Bayesiano de Akaike \"</p> <p>En el caso general, el BIC es</p> <p>$$BIC =k\\ln(n)-2\\ln(L)$$</p> <p>donde $k$ es el n\u00famero de par\u00e1metros en el modelo estad\u00edstico, $n$ es la cantidad de datos disponibles y $L$ es el m\u00e1ximo valor de la funci\u00f3n de verosimilitud para el modelo estimado.</p>"},{"location":"machine_learning/reg_01/#r-cuadrado","title":"R-cuadrado\u00b6","text":"<p>El coeficiente de determinaci\u00f3n o R-cuadrado ($r^2$ ) , es un estad\u00edstico usado en el contexto de un modelo estad\u00edstico cuyo principal prop\u00f3sito es predecir futuros resultados o probar una hip\u00f3tesis. El coeficiente determina la calidad del modelo para replicar los resultados, y la proporci\u00f3n de variaci\u00f3n de los resultados que puede explicarse por el modelo.</p> <p>El valor del $r^2$ habitualmente entre 0 y 1, donde 0 significa una mala calidad de ajuste en el modelo y 1 corresponde a un ajuste lineal perfecto. A menudo, este estad\u00edstico es ocupado para modelos lineales.</p> <p>Se define por la f\u00f3rmula:</p> <p>$$r^2 = \\dfrac{SS_{reg}}{SS_{tot}} = 1 - \\dfrac{SS_{res}}{SS_{tot}},$$</p> <p>donde:</p> <ul> <li><p>$SS_{reg}$ ( suma explicada de cuadrados (ESS)): $\\sum_{i}(\\hat{y}-\\bar{y})^2$</p> </li> <li><p>$SS_{res}$: ( suma residual de cuadrados (RSS)): $\\sum_{i}(y_{i}-\\hat{y})^2 = \\sum_{i}e_{i}^2$</p> </li> <li><p>$SS_{tot}$: ( varianza): $\\sum_{i}(y_{i}-\\bar{y})$, donde: $SS_{tot}=SS_{reg}+SS_{res}$</p> </li> </ul> <p>En una forma general, se puede ver que $r^2$ est\u00e1 relacionado con la fracci\u00f3n de varianza inexplicada (FVU), ya que el segundo t\u00e9rmino compara la varianza inexplicada (varianza de los errores del modelo) con la varianza total (de los datos).</p> <p></p> <ul> <li><p>Las \u00e1reas de los cuadrados azules representan los residuos cuadrados con respecto a la regresi\u00f3n lineal ($SS_{tot}$).</p> </li> <li><p>Las \u00e1reas de los cuadrados rojos representan los residuos al cuadrado con respecto al valor promedio ($SS_{res}$).</p> </li> </ul> <p>Por otro lado, a medida que m\u00e1s variables explicativas se agregan al modelo, el $r^2$ aumenta de forma autom\u00e1tica, es decir, entre m\u00e1s variables explicativas se agreguen, mejor ser\u00e1 la calidad ser\u00e1 el ajuste (un falso argumento).</p> <p>Es por ello que se define el R cuadrado ajustado, que viene a ser  una modificaci\u00f3n del $r^2$, ajustando por el n\u00famero de variables explicativas en un modelo ($p$) en relaci\u00f3n con el n\u00famero de puntos de datos ($n$).</p> <p>$$r^2_{ajustado} = 1-(1-r^2)\\dfrac{n-1}{n-p-1} ,$$</p>"},{"location":"machine_learning/reg_01/#error-de-un-modelo","title":"Error de un modelo\u00b6","text":""},{"location":"machine_learning/reg_01/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>El error corresponde a la diferencia entre el valor original y el valor predicho,es decir:</p> <p>$$e_{i}=y_{i}-\\hat{y}_{i} $$</p> <p></p>"},{"location":"machine_learning/reg_01/#formas-de-medir-el-error-de-un-modelo","title":"Formas de medir el error de un modelo\u00b6","text":"<p>Para medir el ajuste de un modelo se ocupan las denominadas funciones de distancias o m\u00e9tricas. Existen varias m\u00e9tricas, dentro de las cuales encontramos:</p>"},{"location":"machine_learning/reg_01/#otros-estadisticos-interesantes-del-modelo","title":"Otros estad\u00edsticos interesantes del modelo\u00b6","text":""},{"location":"machine_learning/reg_01/#test-f","title":"Test F\u00b6","text":"<p>EL test F para regresi\u00f3n lineal prueba si alguna de las variables independientes en un modelo de regresi\u00f3n lineal m\u00faltiple es significativa.</p> <p>En t\u00e9rminos de test de hip\u00f3tesis, se quiere contrastar lo siguiente:</p> <ul> <li>$H_0: \\beta_1 = \\beta_2 = ... = \\beta_{p-1} = 0$</li> <li>$H_1: \\beta_j \u2260 0$, para al menos un valor de $j$</li> </ul>"},{"location":"machine_learning/reg_01/#test-omnibus","title":"Test Omnibus\u00b6","text":"<p>EL test Omnibusesta relacionado con la simetr\u00eda y curtosis del resido. Se espera ver un valor cercano a cero que indicar\u00eda normalidad. El Prob (Omnibus) realiza una prueba estad\u00edstica que indica la probabilidad de que los residuos se distribuyan normalmente.</p>"},{"location":"machine_learning/reg_01/#test-durbin-watson","title":"Test Durbin-Watson\u00b6","text":"<p>El Test Durbin-Watson es un test de homocedasticidad. Para ver los l\u00edmites relacionados de este test, se puede consultar la siguiente tablas de valores.</p>"},{"location":"machine_learning/reg_01/#test-jarque-bera","title":"Test Jarque-Bera\u00b6","text":"<p>Como el test Omnibus en que prueba tanto el sesgo como la curtosis. Esperamos ver en esta prueba una confirmaci\u00f3n de la prueba Omnibus.</p>"},{"location":"machine_learning/reg_01/#aplicacion-con-python","title":"Aplicaci\u00f3n con python\u00b6","text":""},{"location":"machine_learning/reg_01/#ejemplo-sencillo","title":"Ejemplo sencillo\u00b6","text":"<p>Para comprender los modelos de regresi\u00f3n lineal, mostraremos un caso sencillo de uso. Para ello realizaremos un simulaci\u00f3n de una recta, en el cual le agregaremos un ruido blanco.</p>"},{"location":"machine_learning/reg_01/#ejemplo-con-statsmodel","title":"Ejemplo con Statsmodel\u00b6","text":"<p>Para trabajar los modelos de <code>statsmodel</code>, basta con instanciar el comando <code>OLS</code>. El modelo no considera intercepto, por lo tanto, para agregar el intercepto, a las variables independientes se le debe agregar un vector de unos (tanto para el conjunto de entranamiento como de testeo).</p>"},{"location":"machine_learning/reg_01/#analisis-del-error","title":"An\u00e1lisis del error\u00b6","text":""},{"location":"machine_learning/reg_01/#predicciones","title":"Predicciones\u00b6","text":"<p>Ahora que ya se tiene el modelo entrenado y se ha analizado sus principales caracter\u00edsticas, se pueden realizar predicciones de los valores que se desconocen, de la siguiente manera:</p>"},{"location":"machine_learning/reg_01/#normalidad-de-los-residuos","title":"Normalidad de los residuos\u00b6","text":"<p>Basados en los distintos test (Durbin-Watson,Omnibus,Jarque-Bera ) se concluye que los residuos del modelo son un ruido blanco. Para convencernos de esto de manera gr\u00e1fica, se realizan los siguientes gr\u00e1ficos de inter\u00e9s.</p>"},{"location":"machine_learning/reg_01/#outliers","title":"Outliers\u00b6","text":"<p>Un outlier (o valor at\u00edpico) una observaci\u00f3n que es num\u00e9ricamente distante del resto de los datos. Las estad\u00edsticas derivadas de los conjuntos de datos que incluyen valores at\u00edpicos ser\u00e1n frecuentemente enga\u00f1osas. Estos valores pueden afectar fuertemente al modelo de regresi\u00f3n log\u00edstica. Veamos un ejemplo:</p>"},{"location":"machine_learning/reg_01/#que-hacer-ante-la-presencia-de-outliers","title":"\u00bf Qu\u00e9 hacer ante la presencia de outliers?\u00b6","text":""},{"location":"machine_learning/reg_01/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<ul> <li>Los modelos de regresi\u00f3n lineal son una gran herramienta para realizar predicciones.</li> <li>Los outliers afectan considerablemente a la regresi\u00f3n lineal, por lo que se debn buscar estrategias para abordar esta problem\u00e1tica.</li> <li>En esta oportunidad se hizo un detalle t\u00e9cnico de disntintos est\u00e1disticos asociados a la regresi\u00f3n l\u00edneal (apuntando a un an\u00e1lisis inferencial ), no obstante, en los pr\u00f3ximos modelos, se estar\u00e1 interesado en analizar las predicciones del modelo y los errores asociados a ella, por lo cual los aspectos t\u00e9cnico quedar\u00e1n como lecturas complementarias.</li> <li>Existen varios casos donde los modelos de regresi\u00f3n l\u00edneal no realizan un correcto ajuste de los datos, pero es una gran herramienta para comenzar.</li> </ul>"},{"location":"machine_learning/reg_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Linear Regression in Python</li> </ol>"},{"location":"machine_learning/reg_02/","title":"Regressi\u00f3n II","text":"In\u00a0[1]: Copied! <pre># librerias\n \nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \npd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes\n\n# Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab\n%matplotlib inline\n</pre> # librerias   import os import numpy as np import pandas as pd  import matplotlib.pyplot as plt import seaborn as sns  pd.set_option('display.max_columns', 500)  # Ver m\u00e1s columnas de los dataframes  # Ver gr\u00e1ficos de matplotlib en jupyter notebook/lab %matplotlib inline In\u00a0[2]: Copied! <pre># sklearn models\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_openml\n</pre> # sklearn models from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.datasets import fetch_openml In\u00a0[3]: Copied! <pre># Load the Boston Housing dataset using fetch_openml\ndata = fetch_openml(data_id=531)\n\n# Create a DataFrame from the data\nboston_df = pd.DataFrame(data.data, columns=data.feature_names)\nboston_df['TARGET'] = data.target\nboston_df = boston_df.astype(float)\nboston_df.head()\n</pre> # Load the Boston Housing dataset using fetch_openml data = fetch_openml(data_id=531)  # Create a DataFrame from the data boston_df = pd.DataFrame(data.data, columns=data.feature_names) boston_df['TARGET'] = data.target boston_df = boston_df.astype(float) boston_df.head() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\sklearn\\datasets\\_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n</pre> Out[3]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 In\u00a0[4]: Copied! <pre># descripcion del conjunto de datos\nboston_df.describe()\n</pre> # descripcion del conjunto de datos boston_df.describe() Out[4]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT TARGET count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 22.532806 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 9.197104 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 5.000000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 17.025000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 21.200000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 25.000000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 50.000000 In\u00a0[5]: Copied! <pre>boston_df.info()\n</pre> boston_df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     506 non-null    float64\n 1   ZN       506 non-null    float64\n 2   INDUS    506 non-null    float64\n 3   CHAS     506 non-null    float64\n 4   NOX      506 non-null    float64\n 5   RM       506 non-null    float64\n 6   AGE      506 non-null    float64\n 7   DIS      506 non-null    float64\n 8   RAD      506 non-null    float64\n 9   TAX      506 non-null    float64\n 10  PTRATIO  506 non-null    float64\n 11  B        506 non-null    float64\n 12  LSTAT    506 non-null    float64\n 13  TARGET   506 non-null    float64\ndtypes: float64(14)\nmemory usage: 55.5 KB\n</pre> In\u00a0[6]: Copied! <pre>#matriz de correlacion\ncorr_mat=boston_df.corr(method='pearson')\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix',fmt='.2f')\nplt.show()\n</pre> #matriz de correlacion corr_mat=boston_df.corr(method='pearson') plt.figure(figsize=(20,10)) sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix',fmt='.2f') plt.show() <p>Apliquemos el modelo de regresi\u00f3n lineal multiple con sklearn</p> In\u00a0[7]: Copied! <pre># datos para la regresion lineal simple\nX = boston_df.drop(\"TARGET\",axis=1) \nY = boston_df[\"TARGET\"]\n\n# split dataset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2) \n\n# ajustar el modelo\nmodel_rl = LinearRegression() # Creando el modelo.\nmodel_rl.fit(X_train, Y_train) # ajustando el modelo\n\n# prediciones\nY_predict = model_rl.predict(X_test)\n</pre> # datos para la regresion lineal simple X = boston_df.drop(\"TARGET\",axis=1)  Y = boston_df[\"TARGET\"]  # split dataset X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 2)   # ajustar el modelo model_rl = LinearRegression() # Creando el modelo. model_rl.fit(X_train, Y_train) # ajustando el modelo  # prediciones Y_predict = model_rl.predict(X_test) In\u00a0[8]: Copied! <pre>from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[9]: Copied! <pre>from sklearn.metrics import r2_score\n\n# ejemplo: boston df\ndf_temp = pd.DataFrame(\n    {\n        'y':Y_test,\n        'yhat': model_rl.predict(X_test)\n        }\n)\n\ndf_metrics = regression_metrics(df_temp)\ndf_metrics['r2'] =  round(r2_score(Y_test, model_rl.predict(X_test)),4)\n\nprint('\\nMetricas para el regresor CRIM:')\ndf_metrics\n</pre> from sklearn.metrics import r2_score  # ejemplo: boston df df_temp = pd.DataFrame(     {         'y':Y_test,         'yhat': model_rl.predict(X_test)         } )  df_metrics = regression_metrics(df_temp) df_metrics['r2'] =  round(r2_score(Y_test, model_rl.predict(X_test)),4)  print('\\nMetricas para el regresor CRIM:') df_metrics <pre>\nMetricas para el regresor CRIM:\n</pre> Out[9]: mae mse rmse mape smape r2 0 3.113 18.4954 4.3006 16.036 0.2764 0.7789 <p>Cuando se aplica el modelo de regresi\u00f3n lineal con todas las variables regresoras, las m\u00e9tricas disminuyen considerablemente, lo implica una mejora en el modelo</p> <p>Un problema que se tiene, a diferencia de la regresi\u00f3n lineal simple,es que no se puede ver gr\u00e1ficamente la calidad del ajuste, por lo que solo se puede confiar en las m\u00e9tricas calculadas. Adem\u00e1s, se dejan las siguientes preguntas:</p> <ul> <li>\u00bf Entre m\u00e1s regresores, mejor ser\u00e1 el modelo de regresi\u00f3n lineal?</li> <li>\u00bf Qu\u00e9 se debe tener en cuenta antes de agregar otro variable regresora al modelo de regresi\u00f3n lineal ?</li> <li>\u00bf Qu\u00e9 sucede si se tienen outliers ?</li> <li>\u00bf Existen otros modelos mejor que la regresi\u00f3n lineal ?</li> </ul> <p>Ya se han discutido algunos de estos puntos, por lo que la atenci\u00f3n estar\u00e1 en abordar otros modelos.</p> In\u00a0[10]: Copied! <pre>from sklearn import linear_model\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn import neighbors\n</pre> from sklearn import linear_model from sklearn import tree from sklearn import svm from sklearn import neighbors In\u00a0[11]: Copied! <pre>class SklearnRegressionModels:\n    def __init__(self,model,name_model):\n\n        self.model = model\n        self.name_model = name_model\n        \n    @staticmethod\n    def test_train_model(X,y,n_size):\n        X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)\n        return X_train, X_test, y_train, y_test\n    \n    def fit_model(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        return self.model.fit(X_train, y_train) \n    \n    def df_testig(self,X,y,test_size):\n        X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )\n        model_fit = self.model.fit(X_train, y_train)\n        preds = model_fit.predict(X_test)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test,\n                'yhat': model_fit.predict(X_test)\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,X,y,test_size):\n        df_temp = self.df_testig(X,y,test_size)\n        df_metrics = regression_metrics(df_temp)\n        df_metrics['r2'] =  round(r2_score(df_temp['y'],df_temp['yhat']),4)\n\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n\n    def parameters(self,X,y,test_size):\n        model_fit = self.fit_model(X,y,test_size)\n        \n        list_betas = [\n             ('beta_0',model_fit.intercept_)\n                ]\n            \n        betas = model_fit.coef_\n        \n        for num, beta in enumerate(betas):\n            name_beta = f'beta_{num+1}'\n            list_betas.append((name_beta,round(beta,2)))\n\n        result = pd.DataFrame(\n            columns = ['coef','value'],\n            data = list_betas\n        )\n        \n        result['model'] = self.name_model\n        return result\n</pre> class SklearnRegressionModels:     def __init__(self,model,name_model):          self.model = model         self.name_model = name_model              @staticmethod     def test_train_model(X,y,n_size):         X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=n_size , random_state=42)         return X_train, X_test, y_train, y_test          def fit_model(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         return self.model.fit(X_train, y_train)           def df_testig(self,X,y,test_size):         X_train, X_test, y_train, y_test = self.test_train_model(X,y,test_size )         model_fit = self.model.fit(X_train, y_train)         preds = model_fit.predict(X_test)         df_temp = pd.DataFrame(             {                 'y':y_test,                 'yhat': model_fit.predict(X_test)             }         )                  return df_temp          def metrics(self,X,y,test_size):         df_temp = self.df_testig(X,y,test_size)         df_metrics = regression_metrics(df_temp)         df_metrics['r2'] =  round(r2_score(df_temp['y'],df_temp['yhat']),4)          df_metrics['model'] = self.name_model                  return df_metrics      def parameters(self,X,y,test_size):         model_fit = self.fit_model(X,y,test_size)                  list_betas = [              ('beta_0',model_fit.intercept_)                 ]                      betas = model_fit.coef_                  for num, beta in enumerate(betas):             name_beta = f'beta_{num+1}'             list_betas.append((name_beta,round(beta,2)))          result = pd.DataFrame(             columns = ['coef','value'],             data = list_betas         )                  result['model'] = self.name_model         return result  In\u00a0[12]: Copied! <pre># boston dataframe\nX = boston_df.drop(\"TARGET\",axis=1) \nY = boston_df[\"TARGET\"]\n</pre> # boston dataframe X = boston_df.drop(\"TARGET\",axis=1)  Y = boston_df[\"TARGET\"] In\u00a0[13]: Copied! <pre># models\nreg_lineal = linear_model.LinearRegression()\nreg_ridge = linear_model.Ridge(alpha=.5)\nreg_lasso = linear_model.Lasso(alpha=0.1)\n\nreg_knn = neighbors.KNeighborsRegressor(5,weights='uniform')\nreg_bayesian = linear_model.BayesianRidge()\nreg_tree = tree.DecisionTreeRegressor(max_depth=5)\nreg_svm = svm.SVR(kernel='linear')\n\n\nlist_models =[\n    [reg_lineal,'lineal'],\n    [reg_ridge,'ridge'],\n    [reg_lasso,'lasso'],\n    [reg_knn,'knn'],\n    [reg_bayesian,'bayesian'],\n    [reg_tree,'decision_tree'],\n    [reg_svm,'svm'],\n]\n</pre> # models reg_lineal = linear_model.LinearRegression() reg_ridge = linear_model.Ridge(alpha=.5) reg_lasso = linear_model.Lasso(alpha=0.1)  reg_knn = neighbors.KNeighborsRegressor(5,weights='uniform') reg_bayesian = linear_model.BayesianRidge() reg_tree = tree.DecisionTreeRegressor(max_depth=5) reg_svm = svm.SVR(kernel='linear')   list_models =[     [reg_lineal,'lineal'],     [reg_ridge,'ridge'],     [reg_lasso,'lasso'],     [reg_knn,'knn'],     [reg_bayesian,'bayesian'],     [reg_tree,'decision_tree'],     [reg_svm,'svm'], ] In\u00a0[14]: Copied! <pre>frames_metrics = []\nframes_coef = []\n\nfor model,name_models in list_models:\n    fit_model =  SklearnRegressionModels( model,name_models)\n    frames_metrics.append(fit_model.metrics(X,Y,0.2))\n    if name_models in ['lineal','ridge','lasso']:\n        frames_coef.append(fit_model.parameters(X,Y,0.2))\n</pre> frames_metrics = [] frames_coef = []  for model,name_models in list_models:     fit_model =  SklearnRegressionModels( model,name_models)     frames_metrics.append(fit_model.metrics(X,Y,0.2))     if name_models in ['lineal','ridge','lasso']:         frames_coef.append(fit_model.parameters(X,Y,0.2)) In\u00a0[15]: Copied! <pre># juntar resultados: metricas\npd.concat(frames_metrics).sort_values('rmse')\n</pre> # juntar resultados: metricas pd.concat(frames_metrics).sort_values('rmse') Out[15]: mae mse rmse mape smape r2 model 0 2.6062 20.3563 4.5118 15.0760 0.2620 0.7224 decision_tree 0 3.1891 24.2911 4.9286 16.8664 0.2886 0.6688 lineal 0 3.1493 24.3776 4.9374 16.6837 0.2860 0.6676 ridge 0 3.1251 24.6471 4.9646 16.5449 0.2839 0.6639 bayesian 0 3.1452 25.1556 5.0155 16.7519 0.2870 0.6570 lasso 0 3.6639 25.8601 5.0853 18.8859 0.3177 0.6474 knn 0 3.1404 29.4359 5.4255 16.7713 0.2873 0.5986 svm <p>Basados en los distintos estad\u00edsticos, el mejor modelo corresponde al modelo de decision_tree. Por otro lado, podemos analizar los coeficientes de los modelos l\u00edneales ordinarios,Ridge y Lasso.</p> In\u00a0[16]: Copied! <pre># juntar resultados: coeficientes\npd.concat(frames_coef)\n</pre> # juntar resultados: coeficientes pd.concat(frames_coef) Out[16]: coef value model 0 beta_0 30.246751 lineal 1 beta_1 -0.110000 lineal 2 beta_2 0.030000 lineal 3 beta_3 0.040000 lineal 4 beta_4 2.780000 lineal 5 beta_5 -17.200000 lineal 6 beta_6 4.440000 lineal 7 beta_7 -0.010000 lineal 8 beta_8 -1.450000 lineal 9 beta_9 0.260000 lineal 10 beta_10 -0.010000 lineal 11 beta_11 -0.920000 lineal 12 beta_12 0.010000 lineal 13 beta_13 -0.510000 lineal 0 beta_0 26.891132 ridge 1 beta_1 -0.110000 ridge 2 beta_2 0.030000 ridge 3 beta_3 0.020000 ridge 4 beta_4 2.640000 ridge 5 beta_5 -12.270000 ridge 6 beta_6 4.460000 ridge 7 beta_7 -0.010000 ridge 8 beta_8 -1.380000 ridge 9 beta_9 0.250000 ridge 10 beta_10 -0.010000 ridge 11 beta_11 -0.860000 ridge 12 beta_12 0.010000 ridge 13 beta_13 -0.520000 ridge 0 beta_0 19.859769 lasso 1 beta_1 -0.100000 lasso 2 beta_2 0.030000 lasso 3 beta_3 -0.020000 lasso 4 beta_4 0.920000 lasso 5 beta_5 -0.000000 lasso 6 beta_6 4.310000 lasso 7 beta_7 -0.020000 lasso 8 beta_8 -1.150000 lasso 9 beta_9 0.240000 lasso 10 beta_10 -0.010000 lasso 11 beta_11 -0.730000 lasso 12 beta_12 0.010000 lasso 13 beta_13 -0.560000 lasso <p>Al comparar los resultados entre ambos modelos, se observa que hay coeficientes en la regresi\u00f3n Lasso que se van a cero directamente, pudiendo eliminar estas variables del modelo. Por otro lado, queda como tarea para el lector, hacer una eliminaci\u00f3n de outliers del modelo y probar estos modelos lineales para ver si existe alg\u00fan tipo de diferencia.</p>"},{"location":"machine_learning/reg_02/#regression-ii","title":"Regressi\u00f3n II\u00b6","text":""},{"location":"machine_learning/reg_02/#regression-multiple","title":"Regressi\u00f3n M\u00faltiple\u00b6","text":"<p>Los modelos de regresi\u00f3n multiple son los m\u00e1s utilizados en el mundo de machine learning, puestos que se dispone de varios caracter\u00edstica de la poblaci\u00f3n objetivo. A menudo, se estar\u00e1 abordando estos modelos de la perspectiva de los modelos lineales, por lo cual se debetener en mente algunos supuestos antes de comenzar:</p> <ul> <li>No colinialidad o multicolinialidad: En los modelos lineales m\u00faltiples los predictores deben ser independientes, no debe de haber colinialidad entre ellos</li> <li>Parsimonia: Este t\u00e9rmino hace referencia a que el mejor modelo es aquel capaz de explicar con mayor precisi\u00f3n la variabilidad observada en la variable respuesta empleando el menor n\u00famero de predictores, por lo tanto, con menos asunciones.</li> <li>Homocedasticidad:La varianza de los residuos debe de ser constante en todo el rango de observaciones.</li> <li>Otros Factores:<ul> <li>Distribuci\u00f3n normal de los residuos</li> <li>No autocorrelaci\u00f3n (Independencia)</li> <li>Valores at\u00edpicos, con alto leverage o influyentes</li> <li>Tama\u00f1o de la muestra</li> </ul> </li> </ul> <p>Por otro lado, existen otros tipos de modelos de regresi\u00f3n, en los cuales se  necesitan menos supuestos que los modelos de regresi\u00f3n lineal, a cambio se pierde un poco de interpretabilidad en sus par\u00e1metros y centran su atenci\u00f3n en los resultados obtenidos de las predicciones.</p>"},{"location":"machine_learning/reg_02/#aplicacion-con-python","title":"Aplicaci\u00f3n con python\u00b6","text":""},{"location":"machine_learning/reg_02/#dataset-boston-house-prices","title":"Dataset  Boston house prices\u00b6","text":"<p>En este ejemplo se va utilizar el dataset Boston que ya viene junto con sklearn y es ideal para practicar con Regresiones Lineales; el mismo contiene precios de casas de varias \u00e1reas de la ciudad de Boston.</p>"},{"location":"machine_learning/reg_02/#otros-modelos-de-regresion","title":"Otros modelos de Regresi\u00f3n\u00b6","text":"<p>Existen varios modelos de regresi\u00f3n, sin embargo, la intepretaci\u00f3n de sus par\u00e1metros y el an\u00e1lisis de confiabilidad no es tan directo como los modelos de regresi\u00f3n lineal. Por este motivo, la atenci\u00f3n estar\u00e1 centrada en la predicci\u00f3n m\u00e1s que en la confiabilidad como tal del modelo.</p>"},{"location":"machine_learning/reg_02/#modelos-lineales","title":"Modelos lineales\u00b6","text":"<p>Existen varios modelos lineales que podemos trabajar en sklearn (ver referencia), los cualeas podemos utilizar e ir comparando unos con otros.</p> <p>De lo modelos lineales, destacamos los siguientes:</p> <ul> <li>regresi\u00f3n lineal cl\u00e1sica: regresi\u00f3n cl\u00e1sica por m\u00ednimos cudrados. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2   $$</li> <li>lasso): se ocupa cuando tenemos un gran n\u00famero de regresores y queremos que disminuya el problema de colinealidad (es decir, estimar como cero los par\u00e1metros poco relevantes). $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2 + \\lambda \\sum_{i=1}^n |\\beta_{i}| $$</li> <li>ridge: tambi\u00e9n sirve para disminuir el problema de colinealidad, y adem\u00e1s trata de que los coeficientes sean m\u00e1s rocuesto bajo outliers. $$(P)\\ \\min \\sum_{i=1}^n (y_{i}-f_{i}(x;\\beta))^2  + \\lambda \\sum_{i=1}^n \\beta_{i}^2 $$</li> </ul> <p>Dado que en sklearn, la forma de entrenar, estimar y predecir modelos de regresi\u00f3n siguen una misma estructura, para fectos pr\u00e1cticos, definimos una rutina para estimar las distintas m\u00e9tricas de la siguiente manera:</p>"},{"location":"machine_learning/reg_02/#bayesian-regression","title":"Bayesian Regression\u00b6","text":"<p>En estad\u00edstica, la regresi\u00f3n lineal bayesiana es un enfoque de regresi\u00f3n lineal en el que el an\u00e1lisis estad\u00edstico se realiza dentro del contexto de la inferencia bayesiana. Cuando el modelo de regresi\u00f3n tiene errores que tienen una distribuci\u00f3n normal, y si se asume una forma particular de distribuci\u00f3n previa, los resultados expl\u00edcitos est\u00e1n disponibles para las distribuciones de probabilidad posteriores de los par\u00e1metros del modelo.</p> <p></p>"},{"location":"machine_learning/reg_02/#k-vecinos-mas-cercanos-knn","title":"k-vecinos m\u00e1s cercanos  Knn\u00b6","text":"<p>El m\u00e9todo de los $k$ vecinos m\u00e1s cercanos (en ingl\u00e9s, k-nearest neighbors, abreviado $knn$) es un m\u00e9todo de clasificaci\u00f3n supervisada (Aprendizaje, estimaci\u00f3n basada en un conjunto de entrenamiento y prototipos) que sirve para estimar la funci\u00f3n de densidad $F(x/C_j)$ de las predictoras $x$ por cada clase  $C_{j}$.</p> <p>Este es un m\u00e9todo de clasificaci\u00f3n no param\u00e9trico, que estima el valor de la funci\u00f3n de densidad de probabilidad o directamente la probabilidad a posteriori de que un elemento $x$ pertenezca a la clase $C_j$ a partir de la informaci\u00f3n proporcionada por el conjunto de prototipos. En el proceso de aprendizaje no se hace ninguna suposici\u00f3n acerca de la distribuci\u00f3n de las variables predictoras.</p> <p>En el reconocimiento de patrones, el algoritmo $knn$ es usado como m\u00e9todo de clasificaci\u00f3n de objetos (elementos) basado en un entrenamiento mediante ejemplos cercanos en el espacio de los elementos. $knn$ es un tipo de aprendizaje vago (lazy learning), donde la funci\u00f3n se aproxima solo localmente y todo el c\u00f3mputo es diferido a la clasificaci\u00f3n. La normalizaci\u00f3n de datos puede mejorar considerablemente la exactitud del algoritmo $knn$.</p> <p></p>"},{"location":"machine_learning/reg_02/#decision-tree-regressor","title":"Decision Tree Regressor\u00b6","text":"<p>Un \u00e1rbol de decisi\u00f3n es un modelo de predicci\u00f3n utilizado en diversos \u00e1mbitos que van desde la inteligencia artificial hasta la Econom\u00eda. Dado un conjunto de datos se fabrican diagramas de construcciones l\u00f3gicas, muy similares a los sistemas de predicci\u00f3n basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva, para la resoluci\u00f3n de un problema.</p> <p></p> <p>Vamos a explicar c\u00f3mo se construye un \u00e1rbol de decisi\u00f3n. Para ello, vamos a hacer hincapi\u00e9 en varios aspectos</p>"},{"location":"machine_learning/reg_02/#elementos","title":"Elementos\u00b6","text":"<p>Los \u00e1rboles de decisi\u00f3n est\u00e1n formados por nodos, vectores de n\u00fameros, flechas y etiquetas.</p> <ul> <li>Cada nodo se puede definir como el momento en el que se ha de tomar una decisi\u00f3n de entre varias posibles, lo que va haciendo que a medida que aumenta el n\u00famero de nodos aumente el n\u00famero de posibles finales a los que puede llegar el individuo. Esto hace que un \u00e1rbol con muchos nodos sea complicado de dibujar a mano y de analizar debido a la existencia de numerosos caminos que se pueden seguir.</li> <li>Los vectores de n\u00fameros ser\u00edan la soluci\u00f3n final a la que se llega en funci\u00f3n de las diversas posibilidades que se tienen, dan las utilidades en esa soluci\u00f3n.</li> <li>Las flechas son las uniones entre un nodo y otro y representan cada acci\u00f3n distinta.</li> <li>Las etiquetas se encuentran en cada nodo y cada flecha y dan nombre a cada acci\u00f3n.</li> </ul>"},{"location":"machine_learning/reg_02/#conceptos","title":"Conceptos\u00b6","text":"<p>Cuando tratemos en el desarrollo de \u00e1rboles utilizaremos frecuentemente estos conceptos:</p> <ul> <li>Costo. Se refiere a dos conceptos diferentes: el costo de medici\u00f3n para determinar el valor de una determinada propiedad (atributo) exhibida por el objeto y el costo de clasificaci\u00f3n err\u00f3nea al decidir que el objeto pertenece a la clase $X$ cuando su clase real es $Y$.</li> <li>Sobreajuste (Overfitting). Se produce cuando los datos de entrenamiento son pocos o contienen incoherencias. Al tomar un espacio de hip\u00f3tesis $H$, se dice que una hip\u00f3tesis $h \u2208 H$ sobreajusta un conjunto de entrenamiento $C$ si existe alguna hip\u00f3tesis alternativa $h' \u2208 H$ tal que $h$ clasifica mejor que $h'$ los elementos del conjunto de entrenamiento, pero $h'$ clasifica mejor que h el conjunto completo de posibles instancias.</li> <li>Poda (Prunning). La poda consiste en eliminar una rama de un nodo transform\u00e1ndolo en una hoja (terminal), asign\u00e1ndole la clasificaci\u00f3n m\u00e1s com\u00fan de los ejemplos de entrenamiento considerados en ese nodo.</li> <li>La validaci\u00f3n cruzada. Es el proceso de construir un \u00e1rbol con la mayor\u00eda de los datos y luego usar la parte restante de los datos para probar la precisi\u00f3n del \u00e1rbol.</li> </ul>"},{"location":"machine_learning/reg_02/#reglas","title":"Reglas\u00b6","text":"<p>En los \u00e1rboles de decisi\u00f3n se tiene que cumplir una serie de reglas.</p> <ol> <li>Al comienzo del juego se da un nodo inicial que no es apuntado por ninguna flecha, es el \u00fanico del juego con esta caracter\u00edstica.</li> <li>El resto de los nodos del juego son apuntados por una \u00fanica flecha.</li> <li>De esto se deduce que hay un \u00fanico camino para llegar del nodo inicial a cada uno de los nodos del juego. No hay varias formas de llegar a la misma soluci\u00f3n final, las decisiones son excluyentes.</li> </ol> <p>En los \u00e1rboles de decisiones las decisiones que se eligen son lineales, a medida que vas seleccionando entre varias opciones se van cerrando otras, lo que implica normalmente que no hay marcha atr\u00e1s. En general se podr\u00eda decir que las normas siguen una forma condicional:</p> <p>$$\\textrm{Opci\u00f3n }1-&gt;\\textrm{opci\u00f3n }2-&gt;\\textrm{opci\u00f3n }3-&gt;\\textrm{Resultado Final }X$$</p> <p>Estas reglas suelen ir impl\u00edcitas en el conjunto de datos a ra\u00edz del cual se construye el \u00e1rbol de decisi\u00f3n.</p>"},{"location":"machine_learning/reg_02/#svm","title":"SVM\u00b6","text":"<p>Las m\u00e1quinas de vectores de soporte  (del ingl\u00e9s Support Vector Machines, SVM) son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT&amp;T.</p> <p>Estos m\u00e9todos est\u00e1n propiamente relacionados con problemas de clasificaci\u00f3n y regresi\u00f3n. Dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo m\u00e1s amplios posibles mediante un hiperplano de separaci\u00f3n definido como el vector entre los 2 puntos, de las 2 clases, m\u00e1s cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en funci\u00f3n de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase.</p> <p>M\u00e1s formalmente, una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso infinita) que puede ser utilizado en problemas de clasificaci\u00f3n o regresi\u00f3n. Una buena separaci\u00f3n entre las clases permitir\u00e1 una clasificaci\u00f3n correcta.</p> <p></p>"},{"location":"machine_learning/reg_02/#idea-basica","title":"Idea B\u00e1sica\u00b6","text":"<p>Dado un conjunto de puntos, subconjunto de un conjunto mayor (espacio), en el que cada uno de ellos pertenece a una de dos posibles categor\u00edas, un algoritmo basado en SVM construye un modelo capaz de predecir si un punto nuevo (cuya categor\u00eda desconocemos) pertenece a una categor\u00eda o a la otra.</p> <p>Como en la mayor\u00eda de los m\u00e9todos de clasificaci\u00f3n supervisada, los datos de entrada (los puntos) son vistos como un vector $p-dimensional$ (una lista ordenada de $p$ n\u00fameros).</p> <p>La SVM busca un hiperplano que separe de forma \u00f3ptima a los puntos de una clase de la de otra, que eventualmente han podido ser previamente proyectados a un espacio de dimensionalidad superior.</p> <p>En ese concepto de \"separaci\u00f3n \u00f3ptima\" es donde reside la caracter\u00edstica fundamental de las SVM: este tipo de algoritmos buscan el hiperplano que tenga la m\u00e1xima distancia (margen) con los puntos que est\u00e9n m\u00e1s cerca de \u00e9l mismo. Por eso tambi\u00e9n a veces se les conoce a las SVM como clasificadores de margen m\u00e1ximo. De esta forma, los puntos del vector que son etiquetados con una categor\u00eda estar\u00e1n a un lado del hiperplano y los casos que se encuentren en la otra categor\u00eda estar\u00e1n al otro lado.</p> <p>Los algoritmos SVM pertenecen a la familia de los clasificadores lineales. Tambi\u00e9n pueden ser considerados un caso especial de la regularizaci\u00f3n de Tikhonov.</p> <p>En la literatura de las SVM, se llama atributo a la variable predictora y caracter\u00edstica a un atributo transformado que es usado para definir el hiperplano. La elecci\u00f3n de la representaci\u00f3n m\u00e1s adecuada del universo estudiado, se realiza mediante un proceso denominado selecci\u00f3n de caracter\u00edsticas.</p> <p>Al vector formado por los puntos m\u00e1s cercanos al hiperplano se le llama vector de soporte.</p> <p>Los modelos basados en SVM est\u00e1n estrechamente relacionados con las redes neuronales. Usando una funci\u00f3n kernel, resultan un m\u00e9todo de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptr\u00f3n multicapa.</p> <p></p>"},{"location":"machine_learning/reg_02/#aplicando-varios-modelos-al-mismo-tiempo","title":"Aplicando varios modelos al mismo tiempo\u00b6","text":"<p>Veremos el performance de los distintos modelos estudiados.</p>"},{"location":"machine_learning/reg_02/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<ul> <li>Existen distintos modelos de regresi\u00f3n lineal: normal, Ridge y Lasso. Cada uno con sus respectivs ventajas y desventajas.</li> <li>Existen otros tipos de modelos de regresi\u00f3n (bayesiano, knn, arboles de decisi\u00f3n, svm, entre otros). Por ahora, nos interesa saber como funcionan, para poder configurar los hiperpar\u00e1metros de los modelos ocupados en python (principalmente de la librer\u00eda sklearn).</li> <li>En el mundo del machine learning se estar\u00e1 interesado m\u00e1s en predecir con el menor error posible (siempre tomando como referencia alguna de las m\u00e9tricas mencionadas) que hacer un an\u00e1lisis exhaustivo de la confiabilidad del modelo. Siendo este el caso y si la capacidad computacional lo permite, lo ideal es probar varios modelos al mismo tiempo y poder discriminar bajo un determinado criterio (a menudo el error cuadr\u00e1tico medio (rmse) o el mape).</li> </ul>"},{"location":"machine_learning/reg_02/#referencias","title":"Referencias\u00b6","text":"<ol> <li>Supervised learning</li> </ol>"},{"location":"machine_learning/ts_01/","title":"Series Temporales I","text":"<ul> <li><p>Una serie temporal o cronol\u00f3gica es una sucesi\u00f3n de datos medidos en determinados momentos y ordenados cronol\u00f3gicamente. Los datos pueden estar espaciados a intervalos iguales (como la temperatura en un observatorio meteorol\u00f3gico en d\u00edas sucesivos al mediod\u00eda) o desiguales (como el peso de una persona en sucesivas mediciones en el consultorio m\u00e9dico, la farmacia, etc.).</p> </li> <li><p>Uno de los usos m\u00e1s habituales de las series de datos temporales es su an\u00e1lisis para predicci\u00f3n y pron\u00f3stico (as\u00ed se hace por ejemplo con los datos clim\u00e1ticos, las acciones de bolsa, o las series de datos demogr\u00e1ficos). Resulta dif\u00edcil imaginar una rama de las ciencias en la que no aparezcan datos que puedan ser considerados como series temporales. Las series temporales se estudian en estad\u00edstica, procesamiento de se\u00f1ales, econometr\u00eda y muchas otras \u00e1reas.</p> </li> </ul> In\u00a0[1]: Copied! <pre># librerias \n\nimport os\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n# graficos incrustados\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\n# parametros esteticos de seaborn\nsns.set_palette(\"deep\", desat=.6)\nsns.set_context(rc={\"figure.figsize\": (12, 4)})\n</pre> # librerias   import os import pandas as pd import numpy as np  import matplotlib.pyplot as plt  import seaborn as sns   # graficos incrustados sns.set_style(\"whitegrid\") %matplotlib inline  # parametros esteticos de seaborn sns.set_palette(\"deep\", desat=.6) sns.set_context(rc={\"figure.figsize\": (12, 4)}) In\u00a0[2]: Copied! <pre># cargar datos\nurl='https://drive.google.com/file/d/1a9e0hoBLYof4mJfCeifOm_-H4J12pP-b/view?usp=drive_link'\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\n\ndf = pd.read_csv(url, sep=\",\")\ndf.columns = ['date','passengers']\ndf.head()\n</pre> # cargar datos url='https://drive.google.com/file/d/1a9e0hoBLYof4mJfCeifOm_-H4J12pP-b/view?usp=drive_link' url='https://drive.google.com/uc?id=' + url.split('/')[-2]  df = pd.read_csv(url, sep=\",\") df.columns = ['date','passengers'] df.head() Out[2]: date passengers 0 1949-01 112 1 1949-02 118 2 1949-03 132 3 1949-04 129 4 1949-05 121 In\u00a0[3]: Copied! <pre># resumen\ndf.describe()\n</pre> # resumen df.describe() Out[3]: passengers count 144.000000 mean 280.298611 std 119.966317 min 104.000000 25% 180.000000 50% 265.500000 75% 360.500000 max 622.000000 In\u00a0[4]: Copied! <pre># fechas\nprint('Fecha Inicio: {}\\nFecha Fin:    {}'.format(df.date.min(),df.date.max()))\n</pre> # fechas print('Fecha Inicio: {}\\nFecha Fin:    {}'.format(df.date.min(),df.date.max())) <pre>Fecha Inicio: 1949-01\nFecha Fin:    1960-12\n</pre> In\u00a0[5]: Copied! <pre># formato datetime de las fechas\ndf['date'] = pd.to_datetime(df['date'], format='%Y-%m')\n\n# dejar en formato ts\ny = df.set_index('date').resample('M').mean()\n\ny.head()\n</pre> # formato datetime de las fechas df['date'] = pd.to_datetime(df['date'], format='%Y-%m')  # dejar en formato ts y = df.set_index('date').resample('M').mean()  y.head() Out[5]: passengers date 1949-01-31 112.0 1949-02-28 118.0 1949-03-31 132.0 1949-04-30 129.0 1949-05-31 121.0 In\u00a0[6]: Copied! <pre># graficar datos\ny.plot(figsize=(15, 3),color = 'blue')\nplt.show()\n</pre> # graficar datos y.plot(figsize=(15, 3),color = 'blue') plt.show() <p>Basado en el gr\u00e1fico, uno podria decir que tiene un comportamiento estacionario en el tiempo, es decir, que se repita cada cierto instante de tiempo. Adem\u00e1s, la demanda ha tendido a incrementar a\u00f1o a a\u00f1o.</p> <p>Por otro lado, podemos agregar valor a nuestro entendimiento de nuestra serie temporal, realizando un diagrama de box-plot a los distintos a\u00f1os.</p> In\u00a0[7]: Copied! <pre># Create the boxplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.boxplot(x=y.index.year, y=y[\"passengers\"], data=y, ax=ax)\n\n# Set labels and title\nplt.xlabel(\"Year\")\nplt.ylabel(\"Passengers\")\nplt.title(\"Boxplot of Passengers by Year\")\n\n# Display the plot\nplt.show()\n</pre>  # Create the boxplot fig, ax = plt.subplots(figsize=(15, 6)) sns.boxplot(x=y.index.year, y=y[\"passengers\"], data=y, ax=ax)  # Set labels and title plt.xlabel(\"Year\") plt.ylabel(\"Passengers\") plt.title(\"Boxplot of Passengers by Year\")  # Display the plot plt.show() In\u00a0[8]: Copied! <pre>from pylab import rcParams\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\nfig = decomposition.plot()\nplt.show()\n</pre> from pylab import rcParams import statsmodels.api as sm import matplotlib.pyplot as plt  rcParams['figure.figsize'] = 18, 8 decomposition = sm.tsa.seasonal_decompose(y, model='multiplicative') fig = decomposition.plot() plt.show() <p>Analicemos cada uno de estos gr\u00e1ficos:</p> <ul> <li><p>gr\u00e1fico 01 (serie original): este gr\u00e1fico simplemente nos muestra la serie original graficada en el tiempo.</p> </li> <li><p>gr\u00e1fico 02 (tendencia): este gr\u00e1fico nos muestra la tendencia de la serie, para este caso, se tiene una tendencial lineal positiva.</p> </li> <li><p>gr\u00e1fico 03 (estacionariedad): este gr\u00e1fico nos muestra la estacionariedad de la serie, para este caso, se muestra una estacionariedad a\u00f1o a a\u00f1o, esta estacionariedad se puede ver como una curva invertida (funci\u00f3n cuadr\u00e1tica negativa), en donde a aumenta hasta hasta a mediados de a\u00f1os (o un poco m\u00e1s) y luego esta cae suavemente a final de a\u00f1o.</p> </li> <li><p>gr\u00e1fico 04 (error): este gr\u00e1fico nos muestra el error de la serie, para este caso, el error oscila entre 0 y 1. En general se busca que el error sea siempre lo m\u00e1s peque\u00f1o posible y que tenga el comportamiento de una distribuci\u00f3n normal. Cuando el error sigue una distribuci\u00f3n normal con media cero y varianza 1, se dice que el error es un ruido blanco.</p> </li> </ul> <p>\u00bf c\u00f3mo es un ruido blanco?, mostremos un ruido blanco en python y veamos como este luce:</p> In\u00a0[9]: Copied! <pre># grafico: lineplot \n\nnp.random.seed(42) # fijar semilla\n\nmean = 0\nstd = 1 \nnum_samples = 300\n\n\nsamples = np.random.normal(mean, std, size=num_samples)\n\nplt.plot(samples)\nplt.title(\"Ruido blanco: N(0,1)\")\nplt.show()\n</pre> # grafico: lineplot   np.random.seed(42) # fijar semilla  mean = 0 std = 1  num_samples = 300   samples = np.random.normal(mean, std, size=num_samples)  plt.plot(samples) plt.title(\"Ruido blanco: N(0,1)\") plt.show() <p>Observemos que el ruido blanco oscila sobre el valor 0 y tiene una varianza constante (igual a 1).</p> <p>Veamos su histograma:</p> In\u00a0[10]: Copied! <pre># grafico: histograma\nplt.hist(samples,bins = 10)\nplt.title(\"Ruido blanco: N(0,1)\")\nplt.show()\n</pre> # grafico: histograma plt.hist(samples,bins = 10) plt.title(\"Ruido blanco: N(0,1)\") plt.show() <p>EL histograma de una variable normal, se caracteriza por esa forma de campana sim\u00e9trica entorno a un valor, en este caso, entorno al valor 0.</p> <p></p> <p>La imagen A: Este es un excelente ejemplo de una serie estacionaria, a simple vista se puede ver que los valores se encuentran oscilando en un rango acotado (tendencia constante) y la variabilidad es constante.</p> <p>Las im\u00e1genes B, C y D no son estacionarias porque:</p> <ul> <li><p>imagen B: existe una una tendencia no contante, para este caso lineal (similar al caso que estamos analizando).</p> </li> <li><p>imagen C: existe varianza no constante, si bien varia dentro de valores acotados, la oscilaciones son err\u00e1ticas.</p> </li> <li><p>imagen D: existe codependencia entre los distintos instantes de tiempo.</p> </li> </ul> <p>\u00bf Por qu\u00e9 es importante este concepto ?</p> <ul> <li>Supuesto base de muchos modelos de series temporales (desde el punto de vista estad\u00edstico)</li> <li>No se requiere muchas complicaciones extras para que las predicciones sean \"buenas\".</li> </ul> <p>Autocorrelaci\u00f3n (ACF) y autocorrelaci\u00f3n parcial PACF</p> <p>Definamos a grandes rasgos estos conceptos:</p> <ul> <li><p>Funci\u00f3n de autocorrelaci\u00f3n (ACF). En el retardo $k$, es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a $k$ intervalos de distancia.</p> </li> <li><p>Funci\u00f3n de autocorrelaci\u00f3n parcial (PACF). En el retardo $k$, es la autocorrelaci\u00f3n entre los valores de las series que se encuentran a $k$ intervalos de distancia, teniendo en cuenta los valores de los intervalos intermedios.</p> </li> </ul> <p>Si la serie temporal es estacionaria, los gr\u00e1ficos ACF / PACF mostrar\u00e1n una r\u00e1pida disminuci\u00f3n de la correlaci\u00f3n despu\u00e9s de un peque\u00f1o retraso entre los puntos.</p> <p>Gr\u00e1fiquemos la acf y pacf de nuestra serie temporal ocupando los comandos plot_acf y plot_pacf, respectivamente.</p> In\u00a0[11]: Copied! <pre>from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom matplotlib import pyplot\npyplot.figure(figsize=(12,9))\n\n# acf\npyplot.subplot(211)\nplot_acf(y.passengers, ax=pyplot.gca(), lags = 30)\n\n#pacf\npyplot.subplot(212)\nplot_pacf(y.passengers, ax=pyplot.gca(), lags = 30)\npyplot.show()\n</pre> from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf from matplotlib import pyplot pyplot.figure(figsize=(12,9))  # acf pyplot.subplot(211) plot_acf(y.passengers, ax=pyplot.gca(), lags = 30)  #pacf pyplot.subplot(212) plot_pacf(y.passengers, ax=pyplot.gca(), lags = 30) pyplot.show() <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\statsmodels\\graphics\\tsaplots.py:348: FutureWarning: The default method 'yw' can produce PACF values outside of the [-1,1] interval. After 0.13, the default will change tounadjusted Yule-Walker ('ywm'). You can use this method now by setting method='ywm'.\n  warnings.warn(\n</pre> <p>Se observa de ambas imagenes, que estas no decaen r\u00e1pidamente a cero, por lo cual se puede decir que la serie en estudio no es estacionaria.</p> <p>Prueba  Dickey-Fuller</p> <p>En estad\u00edstica, la prueba Dickey-Fuller prueba la hip\u00f3tesis nula de que una ra\u00edz unitaria est\u00e1 presente en un modelo autorregresivo. La hip\u00f3tesis alternativa es diferente seg\u00fan la versi\u00f3n de la prueba utilizada, pero generalmente es estacionariedad o tendencia-estacionaria. Lleva el nombre de los estad\u00edsticos David Dickey y Wayne Fuller, quienes desarrollaron la prueba en 1979.</p> <p>Para efectos pr\u00e1ticos, para el caso de estacionariedad se puede definir el test como:</p> <ul> <li>Hip\u00f3tesis nula: la serie temporal no es estacionaria.</li> <li>Hip\u00f3tesis alternativa: la serie temporal es alternativa.</li> </ul> <p>Rechazar la hip\u00f3tesis nula (es decir, un valor p muy bajo) indicar\u00e1 estacionariedad</p> In\u00a0[12]: Copied! <pre>from statsmodels.tsa.stattools import adfuller\n\n#test Dickey-Fulle:\nprint ('Resultados del test de Dickey-Fuller:')\ndftest = adfuller(y.passengers, autolag='AIC')\ndfoutput = pd.Series(dftest[0:4], \n                     index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\nprint(dfoutput)\n</pre> from statsmodels.tsa.stattools import adfuller  #test Dickey-Fulle: print ('Resultados del test de Dickey-Fuller:') dftest = adfuller(y.passengers, autolag='AIC') dfoutput = pd.Series(dftest[0:4],                       index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])  print(dfoutput) <pre>Resultados del test de Dickey-Fuller:\nTest Statistic                   0.815369\np-value                          0.991880\n#Lags Used                      13.000000\nNumber of Observations Used    130.000000\ndtype: float64\n</pre> <p>Dado que el p-value es 0.991880, se concluye que la serie temporal no es estacionaria.</p> <p>\u00bf Qu\u00e9 se puede hacer si la serie no es etacionaria ?</p> <p>Nuestro caso en estudio resulto ser una serie no estacionaria, no obstante, se pueden realizar tranformaciones para solucionar este problema.</p> <p>Como es de esperar, estas tranformaciones deben cumplir ciertas porpiedades (inyectividad, diferenciables, etc.). A continuaci\u00f3n, se presentan algunas de las tranformaciones m\u00e1s ocupadas en el \u00e1mbito de series temporales:</p> <p>Sea $X_{t}$ una serie temporal, entonces uno puede definir una nueva serie temporal $Y_{t}$ de la siguiente manera:</p> <ul> <li><p>diferenciaci\u00f3n:  $Y_{t} =\\Delta X_{t}  =X_{t}-X_{t-1}$.</p> </li> <li><p>transformaciones de Box-Cox: $$Y_{t} = \\left\\{\\begin{matrix} \\dfrac{X_{t}^{\\lambda}-1}{\\lambda}, \\ \\  \\textrm{si }  \\lambda &gt; 0\\\\  \\textrm{log}(X_{t}), \\ \\  \\textrm{si }  \\lambda = 0 \\end{matrix}\\right.$$</p> </li> </ul> <p>Ayudemonos con python para ver el resultado de las distintas transformaciones.</p> In\u00a0[13]: Copied! <pre># diferenciacion\n\nY_diff = y.diff()\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_diff)\nplt.title(\"Serie diferenciada\")\nplt.show()\n</pre> # diferenciacion  Y_diff = y.diff()  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_diff) plt.title(\"Serie diferenciada\") plt.show() In\u00a0[14]: Copied! <pre>def box_transformations(y,param):\n    if param&gt;0:\n        return y.apply(lambda x: (x**(param)-1)/param)\n    elif param==0:\n        return np.log(y)\n    else:\n        print(\"lambda es negativo, se devulve la serie original\")\n        return y\n</pre> def box_transformations(y,param):     if param&gt;0:         return y.apply(lambda x: (x**(param)-1)/param)     elif param==0:         return np.log(y)     else:         print(\"lambda es negativo, se devulve la serie original\")         return y In\u00a0[15]: Copied! <pre># logaritmo\n\nY_log = box_transformations(y,0)\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_log)\nplt.title(\"Serie logaritmica\")\nplt.show()\n</pre> # logaritmo  Y_log = box_transformations(y,0)  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_log) plt.title(\"Serie logaritmica\") plt.show() In\u00a0[16]: Copied! <pre># cuadratica\n\nY_quad = box_transformations(y,2)\n\nrcParams['figure.figsize'] = 12, 4\nplt.plot(Y_log)\nplt.title(\"Serie cuadratica\")\nplt.show()\n</pre> # cuadratica  Y_quad = box_transformations(y,2)  rcParams['figure.figsize'] = 12, 4 plt.plot(Y_log) plt.title(\"Serie cuadratica\") plt.show() <p>Se deja como tarea al lector profundizar m\u00e1s en estos conceptos.</p> In\u00a0[17]: Copied! <pre>target_date =  '1958-01-01'\n\n# crear conjunto de entrenamiento y de testeo\nmask_ds = y.index &lt; target_date\n\ny_train = y[mask_ds]\ny_test = y[~mask_ds]\n\n#plotting the data\ny_train['passengers'].plot()\ny_test['passengers'].plot()\nplt.show()\n</pre> target_date =  '1958-01-01'  # crear conjunto de entrenamiento y de testeo mask_ds = y.index &lt; target_date  y_train = y[mask_ds] y_test = y[~mask_ds]  #plotting the data y_train['passengers'].plot() y_test['passengers'].plot() plt.show() <p>Una pregunta natural que surgue es: \u00bf por qu\u00e9 no se toman datos de manera aleatoria?.</p> <p>La respuesta es que como se trabaja el la variable tiempo, por lo tanto los datos siguen un orden natural de los sucesos, en cambio, en los problemas de regresi\u00f3n no existe orden en los sucesos, por cada par de punto es de cierta forma independiente uno con otros. Adem\u00e1s, si se sacar\u00e1n puntos de testeo de manera aleatoria, podr\u00eda romper con la tendencia y estacionariedad original de la serie.</p> <p>Veamos un ejemplo sensillo de este caso en python:</p> In\u00a0[18]: Copied! <pre>from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef regression_metrics(df):\n    \"\"\"\n    Aplicar las distintas m\u00e9tricas definidas\n    :param df: DataFrame con las columnas: ['y', 'yhat']\n    :return: DataFrame con las m\u00e9tricas especificadas\n    \"\"\"\n    df_result = pd.DataFrame()\n\n    y_true = df['y']\n    y_pred = df['yhat']\n\n    df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]\n    df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]\n    df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]\n    df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]\n    df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]\n\n    return df_result\n</pre> from statsmodels.tsa.statespace.sarimax import SARIMAX from sklearn.metrics import mean_absolute_error, mean_squared_error  def mean_absolute_percentage_error(y_true, y_pred):     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  def regression_metrics(df):     \"\"\"     Aplicar las distintas m\u00e9tricas definidas     :param df: DataFrame con las columnas: ['y', 'yhat']     :return: DataFrame con las m\u00e9tricas especificadas     \"\"\"     df_result = pd.DataFrame()      y_true = df['y']     y_pred = df['yhat']      df_result['mae'] = [round(mean_absolute_error(y_true, y_pred), 4)]     df_result['mse'] = [round(mean_squared_error(y_true, y_pred), 4)]     df_result['rmse'] = [round(np.sqrt(mean_squared_error(y_true, y_pred)), 4)]     df_result['mape'] = [round(mean_absolute_percentage_error(y_true, y_pred), 4)]     df_result['smape'] = [round(2 * mean_absolute_percentage_error(y_true, y_pred) / (mean_absolute_percentage_error(y_true, y_pred) + 100), 4)]      return df_result In\u00a0[19]: Copied! <pre># parametros\nparam = [(1,0,0),(0,0,0,12)]\n\n# modelo\nmodel = SARIMAX(y_train,\n                        order=param[0],\n                        seasonal_order=param[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n# ajustar modelo\nmodel_fit = model.fit(disp=0)\n\n# fecha de las predicciones        \nstart_index = y_test.index.min()\nend_index = y_test.index.max()\n\npreds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)\ndf_temp = pd.DataFrame(\n            {\n                'y':y_test['passengers'],\n                'yhat': preds.predicted_mean\n            }\n        )\n\n# resultados del ajuste\ndf_temp.head()\n</pre> # parametros param = [(1,0,0),(0,0,0,12)]  # modelo model = SARIMAX(y_train,                         order=param[0],                         seasonal_order=param[1],                         enforce_stationarity=False,                         enforce_invertibility=False) # ajustar modelo model_fit = model.fit(disp=0)  # fecha de las predicciones         start_index = y_test.index.min() end_index = y_test.index.max()  preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False) df_temp = pd.DataFrame(             {                 'y':y_test['passengers'],                 'yhat': preds.predicted_mean             }         )  # resultados del ajuste df_temp.head() Out[19]: y yhat 1958-01-31 340.0 336.756041 1958-02-28 318.0 337.513783 1958-03-31 362.0 338.273230 1958-04-30 348.0 339.034386 1958-05-31 363.0 339.797254 In\u00a0[20]: Copied! <pre># resultados de las m\u00e9tricas\ndf_metrics = regression_metrics(df_temp)\ndf_metrics['model'] = f\"SARIMA_{param[0]}X{param[1]}\".replace(' ','')\ndf_metrics\n</pre> # resultados de las m\u00e9tricas df_metrics = regression_metrics(df_temp) df_metrics['model'] = f\"SARIMA_{param[0]}X{param[1]}\".replace(' ','') df_metrics Out[20]: mae mse rmse mape smape model 0 81.8529 11619.4305 107.7935 17.0068 0.2907 SARIMA_(1,0,0)X(0,0,0,12) In\u00a0[21]: Copied! <pre># graficamos resultados\n\npreds = df_temp['yhat']\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> # graficamos resultados  preds = df_temp['yhat'] ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7)) ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show() <p>Observamos a simple vista que el ajuste no es tan bueno que digamos (analizando las m\u00e9tricas y el gr\u00e1fico).</p> <p>Entonces, \u00bf qu\u00e9 se puede hacer?. La respuesta es simple \u00a1 probar varios modelos sarima !.</p> <p>Ahora, \u00bf c\u00f3mo lo hacemos?. Lo primero es definir una clase llamada <code>SarimaModels</code> que automatice el proceso anterior, y nos quedamos con aquel modelo que minimice alguna de las m\u00e9tricas propuestas, por ejemplo, minimizar las m\u00e9tricas de mae y mape</p> In\u00a0[22]: Copied! <pre># creando clase SarimaModels\n\nclass SarimaModels:\n    def __init__(self,params):\n\n        self.params = params\n        \n        \n    @property\n    def name_model(self):\n        return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ','')\n    \n    @staticmethod\n    def test_train_model(y,date):\n        mask_ds = y.index &lt; date\n\n        y_train = y[mask_ds]\n        y_test = y[~mask_ds]        \n        \n        return y_train, y_test\n    \n    def fit_model(self,y,date):\n        y_train, y_test = self.test_train_model(y,date )\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        \n        model_fit = model.fit(disp=0)\n\n        return model_fit\n    \n    def df_testig(self,y,date):\n        y_train, y_test = self.test_train_model(y,date )\n        model = SARIMAX(y_train,\n                        order=self.params[0],\n                        seasonal_order=self.params[1],\n                        enforce_stationarity=False,\n                        enforce_invertibility=False)\n        \n        model_fit = model.fit(disp=0)\n        \n        start_index = y_test.index.min()\n        end_index = y_test.index.max()\n\n        preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)\n        df_temp = pd.DataFrame(\n            {\n                'y':y_test['passengers'],\n                'yhat': preds.predicted_mean\n            }\n        )\n        \n        return df_temp\n    \n    def metrics(self,y,date):\n        df_temp = self.df_testig(y,date)\n        df_metrics = regression_metrics(df_temp)\n        df_metrics['model'] = self.name_model\n        \n        return df_metrics\n</pre> # creando clase SarimaModels  class SarimaModels:     def __init__(self,params):          self.params = params                       @property     def name_model(self):         return f\"SARIMA_{self.params[0]}X{self.params[1]}\".replace(' ','')          @staticmethod     def test_train_model(y,date):         mask_ds = y.index &lt; date          y_train = y[mask_ds]         y_test = y[~mask_ds]                          return y_train, y_test          def fit_model(self,y,date):         y_train, y_test = self.test_train_model(y,date )         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)                  model_fit = model.fit(disp=0)          return model_fit          def df_testig(self,y,date):         y_train, y_test = self.test_train_model(y,date )         model = SARIMAX(y_train,                         order=self.params[0],                         seasonal_order=self.params[1],                         enforce_stationarity=False,                         enforce_invertibility=False)                  model_fit = model.fit(disp=0)                  start_index = y_test.index.min()         end_index = y_test.index.max()          preds = model_fit.get_prediction(start=start_index,end=end_index, dynamic=False)         df_temp = pd.DataFrame(             {                 'y':y_test['passengers'],                 'yhat': preds.predicted_mean             }         )                  return df_temp          def metrics(self,y,date):         df_temp = self.df_testig(y,date)         df_metrics = regression_metrics(df_temp)         df_metrics['model'] = self.name_model                  return df_metrics In\u00a0[23]: Copied! <pre># definir parametros \n\nimport itertools\n\np = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\nparams = list(itertools.product(pdq,seasonal_pdq))\ntarget_date = '1958-01-01'\n</pre> # definir parametros   import itertools  p = d = q = range(0, 2) pdq = list(itertools.product(p, d, q)) seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]  params = list(itertools.product(pdq,seasonal_pdq)) target_date = '1958-01-01' In\u00a0[24]: Copied! <pre># iterar para los distintos escenarios\n\nframes = []\nfor param in params:\n    try:\n        sarima_model = SarimaModels(param)\n        df_metrics = sarima_model.metrics(y,target_date)\n        frames.append(df_metrics)\n    except:\n        pass\n</pre> # iterar para los distintos escenarios  frames = [] for param in params:     try:         sarima_model = SarimaModels(param)         df_metrics = sarima_model.metrics(y,target_date)         frames.append(df_metrics)     except:         pass In\u00a0[25]: Copied! <pre># juntar resultados de las m\u00e9tricas y comparar\ndf_metrics_result = pd.concat(frames)\ndf_metrics_result.sort_values(['mae','mape'])\n</pre> # juntar resultados de las m\u00e9tricas y comparar df_metrics_result = pd.concat(frames) df_metrics_result.sort_values(['mae','mape']) Out[25]: mae mse rmse mape smape model 0 16.4272 406.5114 20.1621 4.1002 0.0788 SARIMA_(0,1,0)X(1,0,0,12) 0 17.7173 469.8340 21.6757 4.2001 0.0806 SARIMA_(0,1,1)X(1,1,1,12) 0 17.7204 480.9764 21.9312 4.1431 0.0796 SARIMA_(1,1,0)X(1,1,1,12) 0 17.8053 501.0603 22.3844 4.1164 0.0791 SARIMA_(1,1,1)X(0,1,0,12) 0 17.8056 505.4167 22.4815 4.1061 0.0789 SARIMA_(0,1,0)X(0,1,0,12) ... ... ... ... ... ... ... 0 94.9444 14674.5556 121.1386 19.8867 0.3318 SARIMA_(0,1,0)X(0,0,0,12) 0 360.7115 150709.0664 388.2127 82.0329 0.9013 SARIMA_(0,0,1)X(0,0,1,12) 0 366.5303 153175.7993 391.3768 83.7030 0.9113 SARIMA_(0,0,0)X(0,0,1,12) 0 422.9626 187068.9748 432.5147 98.3713 0.9918 SARIMA_(0,0,1)X(0,0,0,12) 0 428.5000 189730.5556 435.5807 100.0000 1.0000 SARIMA_(0,0,0)X(0,0,0,12) <p>64 rows \u00d7 6 columns</p> <p>En este caso el mejor modelo resulta ser el modelo $SARIMA(0,1,0)X(1,0,0,12)$. Veamos gr\u00e1ficamente que tal el ajuste de este modelo.</p> In\u00a0[26]: Copied! <pre># ajustar mejor modelo\n\nparam = [(0,1,0),(1,0,0,12)]\nsarima_model =  SarimaModels(param)\nmodel_fit = sarima_model.fit_model(y,target_date)\nbest_model = sarima_model.df_testig(y,target_date)\nbest_model.head()\n</pre> # ajustar mejor modelo  param = [(0,1,0),(1,0,0,12)] sarima_model =  SarimaModels(param) model_fit = sarima_model.fit_model(y,target_date) best_model = sarima_model.df_testig(y,target_date) best_model.head() Out[26]: y yhat 1958-01-31 340.0 345.765806 1958-02-28 318.0 330.574552 1958-03-31 362.0 390.254475 1958-04-30 348.0 381.573759 1958-05-31 363.0 389.169386 In\u00a0[27]: Copied! <pre># graficar mejor modelo\n\npreds = best_model['yhat']\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> # graficar mejor modelo  preds = best_model['yhat'] ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7)) ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show() <p>Para este caso, el mejor modelo encontrado se ajusta bastante bien a los datos.</p> <p>Finalmente, vemos algunos resultados del error asosciado al modelo. Para esto ocupamos la herramienta plot_diagnostics, el cual nos arroja cuatro gr\u00e1ficos analizando el error de diferentes manera.</p> In\u00a0[28]: Copied! <pre># resultados del error \nmodel_fit.plot_diagnostics(figsize=(16, 8))\nplt.show()\n</pre> # resultados del error  model_fit.plot_diagnostics(figsize=(16, 8)) plt.show() <ul> <li><p>gr\u00e1fico 01 (standarized residual): Este gr\u00e1fico nos muestra el error estandarizado en el tiempo. En este caso se observa que esta nueva serie de tiempo corresponde a una serie estacionaria que oscila entorno al cero, es decir, un ruido blanco.</p> </li> <li><p>gr\u00e1fico 02 (histogram plus estimated density): Este gr\u00e1fico nos muestra el histograma del error. En este caso, el histograma es muy similar al histograma de una variable $\\mathcal{N}(0,1)$ (ruido blanco).</p> </li> <li><p>gr\u00e1fico 03 (normal QQ):  el gr\u00e1fico Q-Q (\"Q\" viene de cuantil) es un m\u00e9todo gr\u00e1fico para el diagn\u00f3stico de diferencias entre la distribuci\u00f3n de probabilidad de una poblaci\u00f3n de la que se ha extra\u00eddo una muestra aleatoria y una distribuci\u00f3n usada para la comparaci\u00f3n. En este caso se comparar la distribuci\u00f3n del error versus una distribuci\u00f3n normal. Cuando mejor es el ajuste lineal sobre los puntos, m\u00e1s parecida es la distribuci\u00f3n entre la muestra obtenida y la distribuci\u00f3n de prueba (distribuci\u00f3n normal).</p> </li> <li><p>gr\u00e1fico 04 (correlogram): Este gr\u00e1fico nos muestra el gr\u00e1fico de autocorrelaci\u00f3n entre las variables del error, se observa que no hay correlaci\u00f3n entre ninguna de las variables, por lo que se puedan dar indicios de independencia entre las variables.</p> </li> </ul> <p>En conclusi\u00f3n, el error asociado al modelo en estudio corresponde a un ruido blanco.</p> In\u00a0[29]: Copied! <pre>from prophet import Prophet\n</pre> from prophet import Prophet <pre>C:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\db-connectors-RsEieBu8-py3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[30]: Copied! <pre># rename \ny_train_prophet = y_train.reset_index()\ny_train_prophet.columns = [\"ds\",\"y\"]\n\ny_test_prophet = y_test.reset_index()\ny_test_prophet.columns = [\"ds\",\"y\"]\n</pre> # rename  y_train_prophet = y_train.reset_index() y_train_prophet.columns = [\"ds\",\"y\"]  y_test_prophet = y_test.reset_index() y_test_prophet.columns = [\"ds\",\"y\"] In\u00a0[31]: Copied! <pre># model\nm = Prophet()\nm.fit(y_train_prophet)\n</pre> # model m = Prophet() m.fit(y_train_prophet) <pre>21:45:53 - cmdstanpy - INFO - Chain [1] start processing\n21:45:53 - cmdstanpy - INFO - Chain [1] done processing\n</pre> Out[31]: <pre>&lt;prophet.forecaster.Prophet at 0x20c23ff6d60&gt;</pre> In\u00a0[32]: Copied! <pre># forecast\nfuture = m.make_future_dataframe(periods=365*4)\nforecast = m.predict(future)[['ds', 'yhat']]\nforecast.tail()\n</pre> # forecast future = m.make_future_dataframe(periods=365*4) forecast = m.predict(future)[['ds', 'yhat']] forecast.tail() Out[32]: ds yhat 1563 1961-12-26 535.962819 1564 1961-12-27 534.927951 1565 1961-12-28 533.244132 1566 1961-12-29 530.943853 1567 1961-12-30 528.076593 In\u00a0[33]: Copied! <pre># metrics\nresult = y_test_prophet.merge(forecast,on = 'ds',how='inner')\nregression_metrics(result)\n</pre> # metrics result = y_test_prophet.merge(forecast,on = 'ds',how='inner') regression_metrics(result) Out[33]: mae mse rmse mape smape 0 39.9557 2013.1356 44.868 9.8525 0.1794 In\u00a0[34]: Copied! <pre>preds = result[['ds','yhat']].set_index(\"ds\")\nax = y['1949':].plot(label='observed')\npreds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))\n\nax.set_xlabel('Date')\nax.set_ylabel('Passengers')\nplt.legend()\nplt.show()\n</pre> preds = result[['ds','yhat']].set_index(\"ds\") ax = y['1949':].plot(label='observed') preds.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))  ax.set_xlabel('Date') ax.set_ylabel('Passengers') plt.legend() plt.show()"},{"location":"machine_learning/ts_01/#series-temporales-i","title":"Series Temporales I\u00b6","text":""},{"location":"machine_learning/ts_01/#introduccion","title":"Introducci\u00f3n\u00b6","text":""},{"location":"machine_learning/ts_01/#analisis-de-series-temporales","title":"An\u00e1lisis de series temporales\u00b6","text":"<p>Para entender mejor el comportamiento de una serie temporal, nos iremos ayudando con python.</p>"},{"location":"machine_learning/ts_01/#descripcion-del-problema","title":"Descripci\u00f3n del problema\u00b6","text":"<p>El conjunto de datos se llama <code>AirPassengers.csv</code>, el cual contiene la informaci\u00f3n del total de pasajeros (a nivel de mes) entre los a\u00f1o 1949 y 1960.</p> <p>En terminos  estad\u00edsticos, el problema puede ser presentado por la serie temporal $\\left \\{ X_t:  t \\in T \\right \\}$ , donde:</p> <ul> <li>$X_{t}$: corresponde al total de pasajeros en el tiempo t</li> <li>$t$: tiempo medido a nivel de mes.</li> </ul> <p>Comparando la serie temporal con un dataframe, este ser\u00eda un dataframe de una sola columna, en cuyo \u00edndice se encuentra las distintas fechas.</p> <p>El objetivo es poder desarrollar un modelo predictivo que me indique el n\u00famero de pasajeros para los pr\u00f3ximos dos a\u00f1os. Antes de ajustar el modelo, se debe entender el comportamiento de la serie de tiempo en estudio y con esta informaci\u00f3n, encontrar el modelo que mejor se puede ajustar (en caso que exista).</p> <p>Como son muchos los conceptos que se presentar\u00e1n, es necesario ir apoyandose con alguna herramienta de programaci\u00f3n, en nuestro caso python. Dentro de python, la librer\u00eda statsmodels es ideal para hacer este tipo de an\u00e1lisis.</p> <p>Lo primero es cargar, transformar y visualizar el conjunto de datos.</p>"},{"location":"machine_learning/ts_01/#descomposicion-stl","title":"Descomposici\u00f3n STL\u00b6","text":"<p>Una serie temporal la podemos descomponer en tres componentes:</p> <ul> <li>tendencia ($T$): trayectoria de los datos en el tiempo (direcci\u00f3n positiva o negativa).</li> <li>estacionalidad($S$):  fluctuaciones regulares y predecibles en un periodo determinado (anual, semestral,etc.)</li> <li>ruido($e$): error intr\u00ednsico al tomar una serie temporal (instrumenos, medici\u00f3n humana, etc.)</li> </ul> <p>Cuando un descompone la serie temporla en sus tres componenctes (tendencia, estacionalidad, ruido) se habla de descompocisi\u00f3n STL. En muchas ocasiones no es posible descomponer adecuadamente la serie temporal, puesto que la muestra obtenida no presenta un comportamiento ciclico o repetitivo en el periodo de tiempo analizado.</p> <p>Por otro lado, esta descomposici\u00f3n se puede realizar de dos maneras diferentes:</p> <ul> <li>Aditiva: $$X_{t} = T_{t} + S_{t} + e_{t}$$</li> <li>Multiplicativa: $$X_{t} = T_{t} * S_{t} * e_{t}$$</li> </ul> <p>Por suepuesto esta no es la \u00fanica forma de descomponer una serie, pero sirve como punto de partida para comprender nuestra serie en estudio.</p> <p>Realizaremos un descomposici\u00f3n del tipo multiplicativa, ocupando el comando de statsmodels seasonal_decompose.</p>"},{"location":"machine_learning/ts_01/#series-estacionarias","title":"Series Estacionarias\u00b6","text":"<p>Un concepto importante para el \u00e1nalisis de series temporales, es el concepto de estacionariedad.</p>"},{"location":"machine_learning/ts_01/#definicion","title":"Definici\u00f3n\u00b6","text":"<p>Sea $\\left \\{ X_t: t \\in T \\right \\}$ una serie temporal. Se dice que una serie temporal es d\u00e9bilmente estacionaria si:</p> <ul> <li>$\\mathbb{V}(X_t) &lt; \\infty$, para todo $t \\in T$.</li> <li>$\\mu_{X}(t)= \\mu$, para todo $t \\in T$.</li> <li>$\\gamma_{X}(r,s)= \\gamma_{X}(r+h,s+h)=\\gamma_{X}(h)  $, para todo $r,s,h\\in T$.</li> </ul> <p>En palabras simple, una serie temporal es d\u00e9bilmente estacionaria si var\u00eda poco respecto a su propia media.</p> <p>Veamos las siguientes im\u00e1genes:</p>"},{"location":"machine_learning/ts_01/#formas-de-detectar-la-estacionariedad","title":"Formas de detectar la estacionariedad\u00b6","text":"<p>La manera m\u00e1s simple es gr\u00e1ficarla e inferir el comportamiento de esta. La ventaja que este m\u00e9todo es r\u00e1pido, sin embargo, se encuentra sesgado por el criterio del ojo humano.</p> <p>Por otro lado existen algunas alternativas que aqu\u00ed presentamos:</p>"},{"location":"machine_learning/ts_01/#modelos-de-forecast-pronostico","title":"Modelos de forecast  (Pron\u00f3stico)\u00b6","text":"<p>Para realizar el pron\u00f3stico de series, existen varios modelos cl\u00e1sicos para analizar series temporales:</p> <p>Modelos con variabilidad (varianza) constante</p> <ul> <li><p>Modelos de media m\u00f3vil (MA(d)): el modelo queda en funci\u00f3n de los ruidos $e_{1},e_{2},...,e_{d}$</p> </li> <li><p>Modelos autorregresivos (AR(q)): el modelo queda en funci\u00f3n de los ruidos $X_{1},X_{2},...,X_{q}$</p> </li> <li><p>Modelos ARMA  (ARMA(p,q)): Mezcla de los modelos $MA(d)$ y $AR(q)$</p> </li> <li><p>Modelos ARIMA (ARIMA(p,d,q)):: Mezcla de los modelos $MA(d)$ y $AR(q)$ sobre la serie diferenciada $d$ veces.</p> </li> <li><p>Modelos SARIMA (SARIMA(p,d,q)x(P,D,Q,S)): Mezcla de los modelos ARIMA(p,d,q) agregando componentes de estacionariedad ($S$).</p> </li> </ul> <p>Dentro de estos modelos, se tiene que uno son un caso particular de otro m\u00e1s general:</p> <p>$$MA(d),AR(q) \\subset ARMA(p,q) \\subset ARIMA(p,d,q)  \\subset SARIMA(p,d,q)x(P,D,Q,S)  $$</p> <p>Modelos de volatibilidad</p> <ul> <li><p>Arch</p> </li> <li><p>Garch</p> </li> <li><p>Modelos de espacio estado</p> </li> </ul>"},{"location":"machine_learning/ts_01/#realizar-pronostico-con-statsmodels","title":"Realizar pron\u00f3stico con statsmodels\u00b6","text":"<p>El pron\u00f3stico lo realizaremos ocupando los modelos disponible en statsmodels, particularmen los modelos $SARIMA(p,d,q)x(P,D,Q,S)$.</p> <p>Como todo buen proceso de machine learning es necesario separar nuestro conjunto de datos en dos (entrenamiento y testeo). \u00bf C\u00f3mo se realiza esto con series temporales ?.</p> <p>El camino correcto para considerar una fecha objetivo (target_date), el cual separ\u00e9 en dos conjuntos, de la siguiente manera:</p> <ul> <li>y_train: todos los datos hasta la fecha target_date</li> <li>y_test: todos los datos despu\u00e9s la fecha target_date</li> </ul>"},{"location":"machine_learning/ts_01/#prophet","title":"Prophet\u00b6","text":"<p>Prophet es un procedimiento para pronosticar datos de series temporales basado en un modelo aditivo en el que las tendencias no lineales se ajustan a la estacionalidad anual, semanal y diaria, adem\u00e1s de los efectos de las vacaciones. Funciona mejor con series temporales que tienen fuertes efectos estacionales y varias temporadas de datos hist\u00f3ricos. Prophet es resistente a los datos faltantes y los cambios en la tendencia, y por lo general maneja bien los valores at\u00edpicos.</p> <p>Prophet es un software de c\u00f3digo abierto lanzado por el equipo Core Data Science de Facebook. Est\u00e1 disponible para descargar en CRAN y PyPI.</p> <p>Nota: Para entender mejor este algoritmo, puede leer el siguiente paper.</p>"},{"location":"machine_learning/ts_01/#conclusion","title":"Conclusi\u00f3n\u00b6","text":"<p>Este fue una introducci\u00f3n amigable a los conceptos claves de series temporales, a medida que m\u00e1s se profundice en la teor\u00eda, mejor ser\u00e1n las t\u00e9cnicas empleadas sobre la serie temporal en fin de obtener el mejor pron\u00f3stico posible.</p> <p>En esta secci\u00f3n nos limitamos a algunos modelos y algunos criterios de verificaci\u00f3n de estacionariedad. En la literatura existen muchas m\u00e1s, pero con los mostrados de momento y un poco de expertice en el tema, se pueden abordar casi todos los problemas de series temporales.</p>"},{"location":"machine_learning/ts_01/#referencias","title":"Referencias\u00b6","text":"<ol> <li>A comprehensive beginner\u2019s guide to create a Time Series Forecast (with Codes in Python and R)</li> <li>A Gentle Introduction to SARIMA for Time Series Forecasting in Python</li> <li>An Introductory Study on Time Series Modeling and Forecasting </li> </ol>"}]}